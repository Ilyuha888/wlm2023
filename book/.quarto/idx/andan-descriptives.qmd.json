{"title":"Описательные статистики","markdown":{"headingText":"Описательные статистики","headingAttr":{"id":"andan-descriptives","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"::: {.lab-chapter .lab-junior}\n:::\n\n```{r andan-desc-pkgs, echo=FALSE, message=FALSE, warning=FALSE}\nlibrary(tidyverse)\ntheme_set(theme_bw())\nlibrary(latex2exp)\n```\n\n{{< include _symbols.qmd >}}\n\n## Виды статистики {#andan-descriptives-kinds-of-stats}\n::: {.lab-chapter .lab-junior}\n:::\n\nНапомним себе, что статистика [как набор методов и инструментов] делится на два вида --- описательная статистика и статистика вывода.\n\n- **Описательная статистика (descriptive statistics[^desc-stats-1])** занимается обработкой статистических данных, их наглядным представлением, и собственно описанием через некоторые характеристики.\n  - Эти характеристики, количественно описывающие особенности имеющихся данных, называются **описательными статистиками (descriptive statistics[^desc-stats-2])**.\n  - *Задача описательной статистики* --- ёмко описать имеющиеся данные и составить на основе этих описаний общее представление о них, а также обнаружить особенности, которые могут повлиять на дальнейший анализ.\n- **Статистика вывода (inferential statistics)** занимается поиском ответов на содержательные вопросы, которые мы задаем данным в ходе их анализа в рамках научных и практических исследований.\n  - Состоит из двух компонентов --- *тестирования статистических гипотез* и *статистических методов*.\n\n[^desc-stats-1]: Mass (uncountable) noun\n[^desc-stats-2]: Countable noun, plural in this case\n\n::: {.callout-note title=\"Замечание о машинном обучении\"}\n\nВ названии книги упомянуто «машинное обучение». Иногда его причисляют к статистике, иногда рассматривают отдельно. На самом же деле, статистические методы лежат где-то между статистикой вывода и машинным обучением.\n\nПочему?\n\nДело в том, что на статистические методы можно смотреть по-разному.\n\n- Если нашей задачей является поиск ответов на **исследовательские** вопросы о закономерностях, о связи каких-либо факторов или влиянии переменных друг на друга, то мы будем смотреть на статистические модели с точки зрения статистики вывода. Это позволит нам находить ответы на интересующие нас вопросы --- причем не важно, говорим мы о научных исследованиях или об исследованиях в индустрии.\n- Если перед нами стоит задача хорошо **предсказывать** одни переменные на основании значений других --- например, выдавать рекомендации на Яндекс.Музыке или в Яндекс.Лавке --- то мы будем смотреть на те же статистические модели с точки зрения машинного обучения.\n\nТо есть, модели в анализе данных и машинном обучении одни и те же, но то, какую модель мы назовем хорошей и как мы эту «хорошесть» определим, будет отличаться в зависимости от задачи --- *исследовательская* или *предиктивная* --- которая перед нами стоит.\n\n:::\n\n\n\n## Меры центральной тенденции {#andan-descriptives-central-tendency}\n::: {.lab-chapter .lab-junior}\n:::\n\nИтак, мы хотим описать наши данные. Точнее, распределения переменных, которые у нас в данных есть. Хотим сделать это просто и ёмко. Насколько просто и ёмко? Ну, допустим максимально — одним числом. Для этого неплохо подойдет значение переменной, которое лежит *в центре* распределения.\n\nКак мы будем искать, что там в центре распределения? Зависит от [шкалы]() [@stevens46], в которой измерена конкретная переменная (@tbl-scales-cental-tendencies).\n\n|    **Шкала**   |        **Мера центральной тенденции**        |\n|:---------------|:---------------------------------------------|\n| _Номинальная_  | Мода                                         |\n| _Порядковая_   | Медиана                                      |\n| _Интервальная_ | Среднее арифметическое                       |\n| _Абсолютная_   | Среднее арифметическое, геометрическое и др. |\n\n: Шкалы и меры центральной тенденции {#tbl-scales-cental-tendencies tbl-colwidths=\"[25,75]\"}\n\nОднако есть некоторые нюансы.\n\n\n### Мода {#andan-descriptives-mode}\n::: {.lab-chapter .lab-junior}\n:::\n\nСамый простой вариант найти центральную тенденцию --- это определить наиболее часто встречающееся значение переменной. Это значение называется *модой (mode)*.\n\n::: {#def-mode-discrete}\n**Мода** [дискретной переменной] --- наиболее часто встречающееся значение данной переменной.\n:::\n\nНапример, у нас есть следующий ряд наблюдений по какой-то переменной:\n\n$$\n\\begin{bmatrix}\n1 & 3 & 4 & 6 & 3 & 2 & 3 & 3 & 2 & 4 & 1\n\\end{bmatrix}\n$$\n\nЕсли мы посчитаем, сколько раз встретилась каждое значение переменной и составим таблицу частот, то получим следующее:\n\n$$\n\\begin{matrix}\n\\text{Значение} & 1 & 2 & 3 & 4 & 6 \\\\\n\\text{Частота}  & 2 & 2 & 4 & 2 & 1\n\\end{matrix}\n$$\n\nОчевидно, что $3$ встречается чаще других значений --- это и есть мода.\n\nПонятно, что если на нашей шкале нет чисел, а есть текстовые лейблы, это ничего не меняет. Пусть у нас есть переменная с кодами аэропортов:\n\n$$\n\\begin{bmatrix}\n\\text{DME} & \\text{LED} & \\text{IST} & \\text{AER} & \\text{IST} &\\text{SVO} & \\text{LED} & \\text{VKO} & \\text{LED} & \\text{IST} & \\text{IST} & \\text{VKO} & \\text{AER} & \\text{DME}\n\\end{bmatrix}\n$$\n\n$$\n\\begin{matrix}\n\\text{Значение} & \\text{DME} & \\text{LED} & \\text{IST} & \\text{AER} & \\text{SVO} & \\text{VKO}\\\\\n\\text{Частота}  & 2 & 3 & 4 & 2 & 1 & 2\n\\end{matrix}\n$$\n\nМода --- $\\text{IST}$ (Международный аэропорт Стамбула, İstanbul Havalimanı).\n\nТак мы действуем в случае с эмпирическим распределением. Если нам известна [функция вероятности переменной (probability mass function, PMF)](), то мы можем определить моду, основываясь на ней:\n\n::: {#def-mode-discrete-pmf}\n**Мода** [дискретной переменной] --- это значение переменной, при котором её функция вероятности принимает своё максимальное значение.\n:::\n\n$$\n\\text{mode}(X) = \\arg \\max(\\text{PMF}(X)) = \\arg \\max_{x_i}(\\prob (X = x_i)),\n$$ {#eq-mode-pmf}\n\nгде $X$ --- дискретная случайная величина, $x_i$ --- значение этой случайной величины.\n\n```{r mode-pmf, echo=FALSE}\n#| label: fig-mode-pmf\n#| fig-cap: \"Определение моды с помощью функции вероятности\"\n\n\ntibble(x = 1:10,\n       y = c(.01, .03, .07, .1, .1, .15, .2, .1, .09, .15)) |> \n  ggplot(aes(x, y)) +\n  annotate(geom = \"point\", x = 7, y = 0.2, size = 7, shape = 21, color = \"darkred\", fill = \"red\", alpha = .5) +\n  geom_point() +\n  geom_vline(xintercept = 7, color = \"darkred\", linetype = \"dashed\") +\n  annotate(geom = \"text\", label = \"это максимум функции\", \n           x = 8.5, y = 0.2, color = \"darkred\") +\n  annotate(geom = \"text\", label = \"это мода\", \n           x = 7, y = 0, color = \"darkred\") +\n  scale_x_continuous(breaks = 1:10) +\n  labs(x = \"Value\", y = \"Probability\")\n```\n\nОкей, мы видим, что *мода отлично считается на дискретных переменных*. А как же быть с непрерывными?\n\n[Напомним себе](), что вероятность того, что непрерывная случайная величина примет своё конкретное значение, равна нулю. Из этого следует, что все значения непрерывной случайное величины уникальны --- каждое повторяется только один раз. Получается, что строить частотную таблицу бессмысленно...\n\nПо этой причине **для непрерывных переменных моду не считают**.\n\n\n#### Мода для непрерывной переменной {#andan-descriptives-mode-contunious}\n::: {.lab-chapter .lab-middle}\n:::\n\nДа, это так. Действительно, посчитать моду для непрерывной переменной способом, аналогичным тому, что мы увидели выше, не получится. Однако математиков это не остановило.\n\nЕсли мы посмотрим на [график плотности вероятности]() (probability density function, PDF), который является аналогом PMF для дискретных переменных, мы увидим, что какие-то значения встречаются чаще, а какие-то реже. Что в общем-то логично. Напомним себе, [как это выглядит](), например, для любимого [стандартного] [нормального распределения]():\n\n```{r mode-continuous-data, echo=FALSE}\nmode_cont_data <- tibble(x = seq(-4, 4, by = .01), y = dnorm(x))\nmode_high_freq <- mode_cont_data |> filter(x > -.5 & x < .5)\nmode_low_freq <- mode_cont_data |> filter(x > -2.5 & x < -1.5)\n```\n\n```{r mode-continuous-freqs, echo=FALSE}\n#| label: fig-continuous-freqs\n#| fig-cap: \"Частоты интервалов значений непрерывной случайной величины на функции плотности распределения\"\n\n\nggplot(mode_cont_data,\n       aes(x, y)) +\n  geom_line() +\n  geom_polygon(data = tibble(y = c(0, 0), x = c(.5, -.5)) |> \n                 bind_rows(mode_high_freq),\n               fill = \"seagreen\", alpha = .5) +\n  geom_polygon(data = tibble(y = c(0, 0), x = c(-1.5, -2.5)) |>\n                 bind_rows(mode_low_freq),\n               fill = \"royalblue\", alpha = .5) +\n  annotate(geom = \"text\", label = \"эти значения встречаются часто\",\n           x = 0, y = .2, angle = 90, color = \"darkgreen\") +\n  annotate(geom = \"text\", label = \"эти значения\\nвстречаются реже\",\n           x = -2.2, y = .15, angle = 90, color = \"darkblue\") +\n  labs(x = \"Value\", y = \"Density\")\n```\n\nТо есть, самые часто встречающиеся значения --- это **пик распределения**. Там и должна быть мода. Визуально это выглядит достаточно справедливо.\n\nМатематики так и решили:\n\n::: {#def-mode-continuous}\n**Мода** [непрерывной переменной] --- это значение переменной, при котором её функция плотности вероятности достигает локального[^local-max-mode] максимума.\n:::\n\n[^local-max-mode]: Здесь в примере локальный максимум функции плотности вероятности на интервале $(-4, \\, 4)$ совпадает с глобальным максимумом --- мы об этом знаем, потому что форма распределения нам известна. В случае эмпрического распределения корректнее говорить именно о локальном максимуме, так как глобальный максимум нам не доступен ввиду того, что мы работаем с выборкой.\n\n$$\n\\text{mode}(X) = \\arg \\max(\\text{PDF}(X)) = \\arg \\max_{x \\in S}f(x),\n$$ {#eq-mode-pdf}\n\nгдe $X$ --- непрерывная случайная величина, $x$ --- значение этой случайной величины, $S$ --- имеющаяся выборка значений переменной.\n\n```{r mode-continuous-mode, echo=FALSE}\n#| label: fig-continuous-mode\n#| fig-cap: \"Положение моды на функции плотности [стандартного] нормального распределения\"\n\n\nggplot(mode_cont_data,\n       aes(x, y)) +\n  geom_line() +\n  geom_polygon(data = tibble(y = c(0, 0), x = c(.5, -.5)) |> \n                 bind_rows(mode_high_freq),\n               fill = \"seagreen\", alpha = .5) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"darkgreen\") +\n  geom_point(aes(x = 0, y = .4), color = \"darkgreen\", size = 2) +\n  annotate(geom = \"text\", label = \"мода тут\",\n           x = 0, y = 0, color = \"darkgreen\") +\n  annotate(geom = \"text\", label = \"локальный максимум тут\",\n           x = 1.5, y = 0.4, color = \"darkgreen\") +\n  labs(x = \"Value\", y = \"Density\")\n```\n\nХотя моду для непрерывной переменной вычислить можно, обычно этого не делают, так как достаточно других мер центральной тенденции для описания распределения.\n\n***\n\n::: {.callout-warning title=\"Take-home: мода\"}\n- мода --- это значение переменной, которое встречается в выборке чаще всего\n- на практике она рассчитывается через построение частотной таблицы\n- используется с дискретными (номинальными и порядковыми) переменными\n- для непрерывных переменных её рассчитать можно, но обычного этого не делают\n:::\n\n\n\n### Унимодальные и полимодальные распределения {#andan-descriptives-unimodal-bimodal}\n::: {.lab-chapter .lab-junior}\n:::\n\nНормальное распределение, как и ряд других --- биномиальное, отрицательное биномиальное, пуассоновское --- относятся к *унимодальным*. Такие распределения имеют только *одну моду* (см. @fig-norm-mode, @fig-binom-mode, @fig-poiss-mode).\n\n```{r mode-norm-mode, echo=FALSE}\n#| label: fig-norm-mode\n#| fig-cap: \"Нормальное распределение (μ = 2$, σ = 0.5). Пунктирной линией обозначено положение моды.\"\n\n\ntibble(x = seq(0, 4, by = .01),\n       y = dnorm(x, mean = 2, sd = 0.5)) |> \n  ggplot(aes(x, y)) +\n  geom_line() +\n  geom_vline(xintercept = 2, linetype = \"dashed\") +\n  labs(x = \"Value\", y = \"Density\")\n```\n\n```{r mode-binom-mode, echo=FALSE}\n#| label: fig-binom-mode\n#| fig-cap: \"Биномиальное распределение (n = 50, p = 0.3). Пунктирной линией обозначено положение моды.\"\n\n\ntibble(x = seq(0, 20, by = 1),\n       y = dbinom(x, prob = 0.3, size = 50)) |> \n  ggplot(aes(x, y)) +\n  geom_point() +\n  geom_vline(xintercept = 15, linetype = \"dashed\") +\n  labs(x = \"Value\", y = \"Probability\")\n```\n\n```{r mode-poiss-mode, echo=FALSE}\n#| label: fig-poiss-mode\n#| fig-cap: \"Распределение Пуассона (λ = 5.5). Пунктирной линией обозначено положение моды.\"\n\n\ntibble(x = seq(0, 20, by = 1),\n       y = dpois(x, lambda = 5.5)) |> \n  ggplot(aes(x, y)) +\n  geom_point() +\n  geom_vline(xintercept = 5, linetype = \"dashed\") +\n  labs(x = \"Value\", y = \"Probability\")\n```\n\nЭто теоретические распределения. С эмпирическими распределениями дело обстоит так же, хотя они обычно менее гладенькие и красивые (см. @fig-mode-norm-sample и @fig-mode-nbinom-sample).\n\n```{r mode-norm-sample, echo=FALSE}\n#| label: fig-mode-norm-sample\n#| fig-cap: \"Эмпирическое распределение, сгенерированное из нормального распределения (μ = 8, σ = 4, n = 100). `set.seed(314)`. Пунктирной линией обозначено положение моды.\"\n\nset.seed(314)\ntibble(x = rnorm(100, mean = 8, sd = 4)) |> \n  ggplot(aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 fill = \"gray90\", binwidth = .5) +\n  geom_density() +\n  geom_vline(xintercept = 8.5, linetype = \"dashed\") +\n  labs(x = \"Value\", y = \"Density\")\n```\n\n```{r mode-norm-sample, echo=FALSE}\n#| label: fig-mode-nbinom-sample\n#| fig-cap: \"Эмпирическое распределение, сгенерированное из логнормального распределения (μ = 1.1, σ = 1.39, n = 30). `set.seed(314)`. Пунктирной линией обозначено положение моды.\"\n\nset.seed(314)\ntibble(x = rlnorm(n = 30, meanlog = 1.1, sdlog = 1.39)) |> \n  ggplot(aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 fill = \"gray90\", binwidth = .5) +\n  geom_density() +\n  geom_vline(xintercept = 1.35, linetype = \"dashed\") +\n  labs(x = \"Value\", y = \"Density\")\n```\n\nОднако на практике возможны и другие ситуации. Например, такие (@fig-bimodal, @fig-polymodal):\n\n```{r mode-bimodal, echo=FALSE}\n#| label: fig-bimodal\n#| fig-cap: Бимодальное распределение. Сгенерировано из двух нормальных распределений (μ<sub>1</sub> = 1.5, σ<sub>1</sub> = 0.4, n<sub>1</sub> = 80; μ<sub>2</sub> = 4, σ<sub>2</sub> = 0.5, n<sub>2</sub> = 40). `set.seed(65)`. Пунктирными линиями обозначены положения мод.\n\nset.seed(65)\ntibble(x = c(rnorm(80, 1.5, 0.4), rnorm(40, 4, 0.5))) |> \n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 fill = \"gray90\", binwidth = .4) +\n  geom_density() +\n  geom_vline(xintercept = 1.5, linetype = \"dashed\") +\n  geom_vline(xintercept = 4.1, linetype = \"dashed\") +\n  labs(x = \"Value\", y = \"Density\")\n```\n\n```{r mode-polymodal, echo=FALSE}\n#| label: fig-polymodal\n#| fig-cap: Полимодальное распределение. Сгенерировано из двух нормальных распределений (μ<sub>1</sub> = 1.5, σ<sub>1</sub> = 0.3, n<sub>1</sub> = 80; μ<sub>2</sub> = 3.4, σ<sub>2</sub> = 0.5, n<sub>2</sub> = 40) и бета-распределения (α = 2, β = 4, n = 50). `set.seed(65)`. Пунктирными линиями обозначены положения мод.\n\nset.seed(65)\ntibble(x = c(rnorm(80, 1.5, 0.3), rnorm(40, 3.4, 0.5), rbeta(50, 2, 4))) |> \n  ggplot(aes(x)) +\n  geom_histogram(aes(y = after_stat(density)), \n                 fill = \"gray90\", binwidth = .4) +\n  geom_density() +\n  geom_vline(xintercept = 0.4, linetype = \"dashed\") +\n  geom_vline(xintercept = 1.45, linetype = \"dashed\") +\n  geom_vline(xintercept = 3.5, linetype = \"dashed\") +\n  labs(x = \"Value\", y = \"Density\")\n```\n\nВ первом случае (@fig-bimodal) мы видим *два локальных максимума* функции плотности вероятности --- такое распределение называется **бимодальным**. Во втором случае (@fig-polymodal) функция плотности вероятности имеет *три локальных максимума* --- такое распределение называется **полимодальным**. Бимональное распределение является частным случаем полимодального распределения.\n\nВ прицнипе, пиков может быть и больше, однако при работе с реальными данными чаще всего мы сталкиваемся с бимодальными распределениями.\n\n**Что это значит и что с этим делать?**\n\nБимодальное распределение сигнализирует нам о **гетерогенности выборки**. Если мы видим два выделяющихся пика, стоит подумать о том, что наша выборка неоднородна и в ней выделяются две подвыборки. Посмотрим на структуру выборки из примера выше (@fig-bimodal-struct):\n\n```{r mode-bimodal-struct, echo=FALSE}\n#| label: fig-bimodal-struct\n#| fig-cap: \"Структура бимодального распределения из @fig-bimodal. Для удобства сопоставления графиков плотностей вероятности по оси ординат отложены частоты.\"\n\nset.seed(65)\ntibble(x = c(rnorm(80, 1.5, 0.4), rnorm(40, 4, 0.5)),\n       d = c(rep(\"A\", times = 80), rep(\"B\", times = 40))) -> bimodal_struct_data\ngridExtra::grid.arrange(\n  bimodal_struct_data |> \n    ggplot(aes(x)) +\n    geom_histogram(fill = \"gray90\", binwidth = .4) +\n    geom_density(aes(y = after_stat(count))) +\n    geom_vline(xintercept = 1.5, linetype = \"dashed\") +\n    geom_vline(xintercept = 4.1, linetype = \"dashed\") +\n    labs(x = \"Value\", y = \"Count\"),\n  \n  bimodal_struct_data |> \n    ggplot(aes(x)) +\n    geom_histogram(aes(fill = d), \n                   alpha = .3, binwidth = .4) +\n    geom_density(aes(y = after_stat(count), color = d)) +\n    geom_vline(xintercept = 1.5, linetype = \"dashed\") +\n    geom_vline(xintercept = 4.1, linetype = \"dashed\") +\n    guides(color = \"none\", fill = \"none\") +\n    labs(x = \"Value\", y = \"Count\")\n)\n```\n\nДействительно, наше распределение состоит из двух других распределений, у каждого из которого есть своя мода --- поэтому итоговое распределение получается бимодальным. Конечно, сейчас нам это очень удобно показать, потому что мы знаем, как это распределение генерировалось. Когда же у нас есть реальные данные и мы там наблюдаем такого «верблюда», бывает достаточно сложно сказать, что «пошло не так».\n\nСамо по себе распределение не даст нам ответ на вопрос, почему оно бимодальное --- чтобы выяснить причины такого поведения переменной нам потребуются другие данные. Обычно у вас в данных есть «соцдем» --- пол, возраст, сфера и место работы, уровень обрвазования и др. Попробуйте построить распределение с разбиением исследуемой бимодальной переменной по переменным «соцдема». Это, к сожалению, не является рецептом успеха, поскольку причина гетерогенности выборки может и не содержаться в ваших данных, но такое изучение данных станет хорошим показателем того, что вы не просто «забили» на странное распределение своей переменной, а поисследователи возможные его причины.\n\nЕсли вам удалось найти причины гетерогенности выборки --- допустим, у вас выделяются подвыборки «бакалавры» и «магистры» --- стоит подумать о том, как обойтись с этой переменной в планируемом анализе, так как игнорировать её, по-видимому, нельзя, поскольку она влияет на вариатиность данных.\n\n::: {.callout-tip title=\"Соцдем лишним не бывает\"}\nНа этапе планирования исследования подумайте о том, чем могут отличаться ваши респонденты или испытуемые между собой, помимо индивидуальных различий.\n\n- Если в эксперименте используете задачу мысленного вращения (mental rotation, [@shepard71]), вполне возможно, испытуемые, работающие в сфере 3D-моделирования или дизайна интерьеров, могут сформировать подвыборку.\n- В случае HR-исследования, где фиксируется доход респондента, необходимо записать город, в котором он проживает и/или работает.\n- При изучении удовлетворенности городским пространством важными пунктами станут беременность, наличие/отсутствие детей, наличие/отсутствие автомобиля и др.\n\nИ так далее. Примеров для каждого случая можно подобрать много.\n\nСтоит ли, скажем, в первом случае сразу исключить из выборки 3D-моделлеров? Зависит. От количества времени и денег на проведение исследования. Однако *как минимум эту информацию надо зафиксировать в данных*. А решить, исключать ли этих респондентов из выборки или нет, можно и позже. Главно об этом написать в отчете/статье, когда будете описывать предобработку данных.\n:::\n\n***\n\n::: {.callout-warning title=\"Take-home: бимодальное распределение\"}\n- бимодальное распределение намекает на неоднородность данных --- скорее всего, в выборке есть две подвыборки\n- необходимо поискать в данных причины этой неодноросности, например, в социально-демографических переменных\n- если удалось найти переменную, объясняющую бимодальность, стоит подумать о том, как её учитывать в планируемом анализе\n:::\n\n\n\n### Медиана {#andan-descriptives-median}\n::: {.lab-chapter .lab-junior}\n:::\n\nДля номинальной шкалы мода --- это единственно возможная мера центральной тенденции, потому что на этой шкале отсутствует порядок элементов. На других шкалах наблюдения уже можно сортировать по возрастнию или убыванию, поскольку начиная с ранговой (порядковой) шкалы на всех них определена операция сравнения на «больше-меньше».\n\nВозьмем тот же ряд наблюдений, что и в предыдущем разделе:\n\n$$\n\\begin{bmatrix}\n1 & 3 & 4 & 6 & 3 & 2 & 3 & 3 & 2 & 4 & 1\n\\end{bmatrix}\n$$\n\nОтсортируем наблюдения по возрастанию:\n\n$$\n\\begin{bmatrix}\n1  & 1 & 2 & 2 & 3 & 3 & 3 & 3 & 4 & 4 & 6\n\\end{bmatrix}\n$$\n\nНаша задача --- определить центральную тенденцию. Давайте посмотрим, что оказалось в середине отсортированного ряда:\n\n\n$$\n\\begin{bmatrix}\n1 & 1 & 2 & 2 & 3 & \\mathbf{3} & 3 & 3 & 4 & 4 & 6\n\\end{bmatrix}\n$$\n\nЭто медиана. В данном случае она равна $3$.\n\n::: {#def-median}\n**Медиана (median)** --- это значение, которое располагается на середине отсортированного ряда значений переменной. \n:::\n\nМедиана делит все наблюдения переменной ровно пополам и половина наблюдений оказывается по одну сторону от медианы, а половина --- по другую.\n\nЕсли число наблюдений нечётное, то всё ясно --- в середине отсортированного ряда будет какое-то значение. А если число наблюдений чётное? Тогда мы попадаем между значениями. \n\nВозьмем для примера такой вектор наблюдений:\n\n$$\n\\begin{bmatrix}\n14 & 10 & 9 & 16 & 30 & 3 & 25 & 8 & 18 & 7\n\\end{bmatrix}\n$$\n\nОтсортируем:\n\n$$\n\\begin{bmatrix}\n3 & 7 & 8 & 9 & 10 & 14 & 16 & 18 & 25 & 30\n\\end{bmatrix}\n$$\n\nНайдем середину:\n\n$$\n\\begin{bmatrix}\n3 & 7 & 8 & 9 & 10 & | & 14 & 16 & 18 & 25 & 30\n\\end{bmatrix}\n$$\n\nВ таком случае в качестве медианы берется среднее между двумя срединными значениями:\n\n$$\n\\text{median} = \\frac{10 + 14}{2} = 12\n$$\n\nИтого, формализовать вычисление медианы можно следующим образом:\n\n$$\n\\text{median}(X) = X(a) =\n\\cases{\nX\\left(\\frac{n+1}{2}\\right), & if  2 | n \\\\\n\\dfrac{X(\\frac{n}{2}) + X(\\frac{n}{2} + 1)}{2}, & otherwise\n}\n$$ {#eq-median-formula}\n\nгде $X$ --- ряд наблюдений случайной величины, $n$ --- число наблюдений, $X(a)$ --- наблюдение с индексом $a$ в отсортированном векторе $X$.\n\nЕсли мы будем смотреть на медиану с позиции описания распределения, то она будет той самой линией, которая разделит площадь под графиком функции плотности вероятности пополам:\n\n```{r median-norm, echo=FALSE}\n#| label: fig-median-norm\n#| fig-cap: \"Медиана нормального распределения\"\n\ntibble(x = seq(-4, 4, by = .01),\n       y = dnorm(x)) -> median_norm_data \nmedian_norm_data |> filter(x >= 0) -> median_norm_upper\nmedian_norm_data |> filter(x <= 0) -> median_norm_lower\nmedian_norm_data |> \n  ggplot(aes(x, y)) +\n  geom_line() +\n  geom_polygon(data = tibble(y = c(0, 0), x = c(4, 0)) |> \n                 bind_rows(median_norm_upper),\n               fill = \"seagreen\", alpha = 0.5) +\n  geom_polygon(data = tibble(y = c(0, 0), x = c(0, -4)) |> \n                 bind_rows(median_norm_lower),\n               fill = \"royalblue\", alpha = 0.5) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  annotate(geom = \"text\", label = \"эти значения\\nбольше медианы \\n\\n 50%\", \n           color = \"darkgreen\", x = 1, y = 0.05) +\n  annotate(geom = \"text\", label = \"эти значения\\nменьше медианы \\n\\n 50%\", \n           color = \"darkblue\", x = -1, y = 0.05) +\n  annotate(geom = \"text\", label = \"это медиана\", \n           x = -0.01, y = 0)\n```\n\nПри этом форма распределения не имеет значения --- площадь под графиком всегда будет делиться пополам:\n\n```{r median-left-skew, echo=FALSE}\n#| label: fig-median-left-skew\n#| fig-cap: Медиана распределения с отрицательной асимметрией.\n\nset.seed(115)\ntibble(x = seq(0.3, 1, by = .001),\n       y = dbeta(x, 7, 1.5)) -> median_leftskew_data\n\nmedian_leftskew_data |> \n  filter(x >= .845) %>%\n  bind_rows(\n    tibble(y = c(0, 0),\n           x = c(max(median_leftskew_data$x), .845)),\n    .) -> median_leftskew_data_upper\n\nmedian_leftskew_data |> \n  filter(x <= .845) %>%\n  bind_rows(\n    tibble(y = c(0, 0),\n           x = c(.845, min(median_leftskew_data$x))),\n    .) -> median_leftskew_data_lower\n\n# DescTools::AUC(median_leftskew_data_upper$x,\n#                median_leftskew_data_upper$y)\n\nmedian_leftskew_data |> \n  ggplot(aes(x, y)) +\n  geom_line() +\n  geom_polygon(data = median_leftskew_data_lower, \n               aes(x = x, y = y), fill = \"royalblue\", alpha = .5) +\n  geom_polygon(data = median_leftskew_data_upper, \n               aes(x = x, y = y), fill = \"seagreen\", alpha = .5) +\n  geom_vline(xintercept = 0.845, linetype = \"dashed\") +\n  annotate(geom = \"text\", label = \"50%\", \n           color = \"darkgreen\", x = .9, y = 1) +\n  annotate(geom = \"text\", label = \"50%\", \n           color = \"darkblue\", x = .8, y = 1) +\n  labs(x = \"Value\", y = \"Density\")\n```\n\n```{r median-right-skew, echo=FALSE}\n#| label: fig-median-right-skew\n#| fig-cap: Медиана распределения с положительной асимметрией.\n\nset.seed(115)\ntibble(x = seq(0, 5, by = .001),\n       y = dgamma(x, 2, 2)) -> median_rightskew_data\n\nmedian_rightskew_data |>\n  filter(x >= .85) %>%\n  bind_rows(\n    tibble(y = c(0, 0),\n           x = c(max(median_rightskew_data$x), .85)),\n    .) -> median_rightskew_data_upper\n\nmedian_rightskew_data |>\n  filter(x <= .85) %>%\n  bind_rows(\n    tibble(y = c(0, 0),\n           x = c(.85, min(median_rightskew_data$x))),\n    .) -> median_rightskew_data_lower\n\n# DescTools::AUC(median_rightskew_data_lower$x,\n#                median_rightskew_data_lower$y)\n\nmedian_rightskew_data |> \n  ggplot(aes(x, y)) +\n  geom_line() +\n  geom_polygon(data = median_rightskew_data_lower, \n               aes(x = x, y = y), fill = \"royalblue\", alpha = .5) +\n  geom_polygon(data = median_rightskew_data_upper, \n               aes(x = x, y = y), fill = \"seagreen\", alpha = .5) +\n  annotate(geom = \"text\", label = \"50%\", \n           color = \"darkgreen\", x = 1.25, y = .2) +\n  annotate(geom = \"text\", label = \"50%\", \n           color = \"darkblue\", x = .5, y = .2) +\n  geom_vline(xintercept = 0.85, linetype = \"dashed\") +\n  labs(x = \"Value\", y = \"Density\")\n```\n\n```{r median-bimodal, echo=FALSE}\n#| label: fig-median-bimodal\n#| fig-cap: Медиана бимодального распределения.\n\nset.seed(115)\ntibble(x = c(rnorm(80, 1.5, 0.4), rnorm(40, 4, 0.5))) -> median_bimodal_data\n\nmedian_bimodal_data |> \n  ggplot(aes(x)) +\n  geom_density() -> g\nggplot_build(g) -> b\nb$data[[1]] -> median_bimodal_data_build\n\nmedian_bimodal_data$x |> median() -> m\n# median_bimodal_data_build$x |> median() -> m\n\nmedian_bimodal_data_build |> \n  select(y, x) |> \n  filter(x >= m) %>% \n  bind_rows(\n    tibble(y = c(0, 0),\n           x = c(max(median_bimodal_data_build$x), m)),\n    .) -> median_bimodal_data_build_upper\n\nmedian_bimodal_data_build |>\n  select(y, x) |> \n  filter(x <= m) %>%\n  bind_rows(\n    tibble(y = c(0, 0),\n           x = c(m , min(median_bimodal_data_build$x))),\n    .) -> median_bimodal_data_build_lower\n\n# DescTools::AUC(x = median_bimodal_data_build_upper$x,\n#                y = median_bimodal_data_build_upper$y)\n# DescTools::AUC(x = median_bimodal_data_build_lower$x,\n#                y = median_bimodal_data_build_lower$y)\n\nmedian_bimodal_data_build |> \n  ggplot(aes(x, y)) +\n  geom_histogram(data = median_bimodal_data,\n                 aes(x = x, y = after_stat(density)), \n                 fill = \"gray90\", binwidth = .4) +\n  geom_line() +\n  geom_polygon(data = median_bimodal_data_build_lower, \n               aes(x = x, y = y), fill = \"royalblue\", alpha = .5) +\n  geom_polygon(data = median_bimodal_data_build_upper, \n               aes(x = x, y = y), fill = \"seagreen\", alpha = .5) +\n  geom_vline(xintercept = m, linetype = \"dashed\") +\n  annotate(geom = \"text\", label = \"50%\", \n           color = \"darkgreen\", x = 2.25, y = .1) +\n  annotate(geom = \"text\", label = \"50%\", \n           color = \"darkblue\", x = 1.25, y = .1) +\n  labs(x = \"Value\", y = \"Density\")\n```\n\n***\n\n::: {.callout-warning title=\"Take-home: медиана\"}\n- медиану можно расчитать только на шкалах, где задан порядок (ранговая, интервальная, абсолютная)\n- медиана делит выборку наблюдений на две равные части\n- линия медианы раздели площадь под графиком функции плотности вероятности пополам\n:::\n\n\n\n### Среднее {#andan-descriptives-mean}\n::: {.lab-chapter .lab-junior}\n:::\n\nЕсли наша переменная измерена в самых мощных шкалах --- интервальной или абсолютной --- то нам доступна ещё одна мера центральной тенденции.\n\n\n#### Арифметическое среднее {#andan-descriptives-arithmetic-mean}\n::: {.lab-chapter .lab-junior}\n:::\n\nС этим существом все знакомы еще со школы. **Арифметическое среднее (arithmetic mean, mean, average)** считается так:\n\n$$\nM_{X} = \\bar x = \\dfrac{\\sum_{i=1}^{n}x_i}{n},\n$$\n\nгде $\\bar X$ --- среднее арифметическое, $x_i$ --- наблюдение в векторе $X$, $n$ --- количество наблюдений.\n\nНу, то есть всё сложить и поделить на количество того, чего сложили. Изи.\n\n##### Свойства среднего арифметического {#andan-descriptives-arithmetic-mean-features}\n\n* **Если к каждому значению распределения прибавить некоторое число (константу), то среднее увеличится на это же константу.**\n\n\n$$\nM_{X+c} = M_X + c\n$$\n\nВот почему:\n\n$$\nM_{X+c} = \\frac{\\sum_{i=1}^n (x_i + c)}{n} = \\frac{\\sum_{i=1}^n x_i + nc}{n} = \\frac{\\sum_{i=1}^n x_i}{n} + c = M_X + c\n$$\n\n\nИначе говоря, распределение просто сдвинется. Например, если к каждому значению синего распределения прибавить $2$, получится красное:\n\n```{r creating_tibble_for_feature_vis_1, include=FALSE}\nsmpl1 <- tibble(x1 = seq(-3, 3, by = .001),\n               y1 = dnorm(x1),\n               x2 = x1 + 2,\n               y2 = dnorm(x2, mean = 2))\n```\n\n```{r mean_feature_1, echo=FALSE}\nsmpl1 %>% \n  ggplot() +\n  geom_line(aes(x1, y1), color = \"blue4\") +\n  geom_line(aes(x2, y2), color = \"red4\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"blue4\") +\n  geom_vline(xintercept = 2, linetype = \"dashed\", color = \"red4\") +\n  labs(x = \"Value\", y = \"Density\")\n```\n\n\n* **Если каждое значение распределение умножить на некоторое число (константу), то среднее увеличится во столько же раз.**\n\n$$\nM_{X \\times c} = M_X \\times c\n$$\n\nВот почему:\n\n$$\nM_{X \\times c} = \\frac{\\sum_{i=1}^n (x_i \\times c)}{n} = \\frac{c \\times \\sum_{i=1}^n x_i}{n} = \\frac{\\sum_{i=1}^n x_i}{n} \\times c = M_X \\times c\n$$\n\nНапример, здесь каждое значение синего распределения умножили на $3$ и получили красное:\n\n```{r creating_tibble_for_feature_vis_2, include=FALSE}\nsmpl2 <- tibble(x1 = seq(-2, 4, by = .001),\n               y1 = dnorm(x1, mean = 1),\n               x2 = x1 * 3,\n               y2 = dnorm(x2, mean = 3, sd = 3))\n```\n\n```{r mean_feature_2, echo=FALSE}\nsmpl2 %>% \n  ggplot() +\n  geom_line(aes(x1, y1), color = \"blue4\") +\n  geom_line(aes(x2, y2), color = \"red4\") +\n  geom_vline(xintercept = 1, linetype = \"dashed\", color = \"blue4\") +\n  geom_vline(xintercept = 3, linetype = \"dashed\", color = \"red4\") +\n  labs(x = \"Value\", y = \"Density\")\n```\n\nТут, правда, явно [что-то ещё](#var_features) произошло, но мы пока этого не знаем. Однако, отметит этот факт.\n\n\n* **Сумма отклонений от среднего значения равна нулю.**\n\n$$\n\\sum_{i=1}^n(x_i - M_X) = 0\n$$\n\nЭлегантное доказательство:\n\n$$\n\\sum_{i=1}^n(x_i - M_X) = \\sum_{i=1}^n x_i - \\sum_{i=1}^n M_X = \\sum_{i=1}^n x_i - nM_X = \\\\\n= \\sum_{i=1}^n x_i - n \\times \\frac{1}{n} \\sum_{i=1}^n x_i = \\sum_{i=1}^n x_i - \\sum_{i=1}^n x_i = 0\n$$\n\nНо можно это осмыслить и более просто графически.\n\n**Отклонение** --- это разность между средним и конкретным значением переменной. И, действительно, так как среднее находится в центре распределения, то часть значений лежит справа, а часть слева. Значит, будут как положительные, так и отрицательные отклонения --- и их сумма в итоге будет равна нулю.\n\n```{r df_polygons, include=FALSE}\npoly_left <- smpl1 %>% \n  select(x1, y1) %>% \n  filter(x1 < 0) %>% \n  bind_rows(tibble(x1 = c(0, -3), y1 = c(0, 0)))\npoly_right <- smpl1 %>% \n  select(x1, y1) %>% \n  filter(x1 > 0) %>% \n  bind_rows(tibble(x1 = c(3, 0), y1 = c(0, 0)))\n```\n\n```{r zero_deviation_sum, echo=FALSE}\nsmpl1 %>% \n  ggplot() +\n  geom_line(aes(x1, y1)) +\n  geom_polygon(data = poly_left, aes(x=x1, y=y1), fill=\"red4\", alpha = .5) +\n  geom_polygon(data = poly_right, aes(x=x1, y=y1), fill=\"green4\", alpha = .5) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  annotate(geom=\"text\", x = -1, y = .05, label =\"отрицательные\\nотклонения\") +\n  annotate(geom=\"text\", x = 1, y = .05, label =\"положительные\\nотклонения\") +\n  labs(x = \"Value\", y = \"Density\")\n```\n\n\n\n\n#### Усеченное среднее {#andan-descriptives-trimmed-mean}\n::: {.lab-chapter .lab-junior}\n:::\n\nПРО УСЕЧЕННОЕ СРЕДНЕЕ\n\n\nСреднее арифметическое не одиноко --- есть и другие. Встретяться они вам примерно нигде --- то есть о-о-о-очень редко и, скорее всего, в каком-то изощрённом виде. Но упомянуть их, пожалуй, стоит.\n\n#### Геометрическое среднее {#andan-descriptives-geometric-mean}\n::: {.lab-chapter .lab-middle}\n:::\n\nРедко встречается в научных работах, но заради общего представления пусть будет. Поскольку оно считается через умножение, то может быть рассчитано только на абсолютной шкале.\n\n$$\nG_{X} = \\sqrt[n]{\\prod_{i=1}^n x_i} = \\Big(\\prod_{i=1}^n x_i\\Big)^{\\tfrac{1}{n}}\n$$\n\n#### Квадратичное среднее {#andan-descriptives-quandratic-mean}\n::: {.lab-chapter .lab-middle}\n:::\n\n> А вот это уже более полезная история. Мы с ним столкнёмся далее, правда под разными масками.\n\n**Квадратичное среднее (quadratic mean, root mean square, RMS)** --- это квадратный корень из среднего квадрата наблюдений. Ничего не понятно, поэтому по порядку.\n\n* есть наблюдение $x_i$\n* значит есть и его квадрат $x_i^2$\n* мы умеем считать обычно среднее арифметическое, но ведь $x_i^2$ --- это тоже наблюдение, просто в квадрате, так?\n* значит можем посчитать среднее арифметическое квадратов наблюдений --- *средний квадрат*\n\n$$\n\\frac{\\sum_{i=1}^n x_i^2}{n}\n$$\n\n* норм, а теперь извлечём из этого дела корень --- получим то, что там надо\n\n$$\nX_{\\mathrm{RMS}} = \\sqrt{\\frac{\\sum_{i=1}^n x_i^2}{n}}\n$$\n\nPer se[^per-se] мы его вряд ли ещё когда-то увидим, но пару раз оно внезапно всплывет.\n\n[^per-se]: Per se (лат.) --- «само по себе», «как таковое», «в чистом виде».\n\n\n#### Гармоническое среднее {#andan-descriptives-harmonic-mean}\n::: {.lab-chapter .lab-middle}\n:::\n\n> Суперэкзотичный покемон.\n\n$$\nH_X = \\frac{n \\prod_{i=1}^n x_i}{\\sum_{i=1}^n (\\tfrac{1}{x} \\prod_{j=1}^n x_j)} = \\frac{n}{\\sum_{i=1}^n \\tfrac{1}{x_i}}\n$$\n\n\n#### Взвешенное среднее {#andan-descriptives-weighted-mean}\n::: {.lab-chapter .lab-junior}\n:::\n\nЧасто бывает такая ситуация, что нас нужно посчитать среднее по каким-либо имеющимся параметрам, но одни параметры для нас важнее, чем другие. Например, мы хотим вычислить суммарный балл обучающегося за курс на основе ряда работ, выполненных в течение курса, однако мы понимаем, что тест из десяти вопросов с множественном выбором явно менее показателен, чем, например, аналитическое эссе или экзаменационная оценка. Что делать? Взвесить параметры!\n\nЧто значит взвесить? Умножить на некоторое число. На самом деле, любое. Пусть мы посчитали, что написать эссе в три абстрактных раза тяжелее, чем написать тест, а сдать экзамен в два раза тяжелее, чем написать эссе. Тогда мы можем присвоить баллу за тест вес $1$, баллу за аналитическое эссе вес $3$, а экзамену --- вес $6$. Тогда итоговая оценка за курс будет рассчитываться следующим образом:\n\n$$\n\\text{final score } = 1 \\cdot \\text{test} + 3 \\cdot \\text{essay} + 6 \\cdot \\text{exam}\n$$\n\nСуперкласс. Однако! Весьма вероятно, что в учебном заведении принята единая система оценки для всех видов работ (ну, скажем, некая абстрактная десятибалльная система в сферическом вакууме). Получается, если и за тест, и за эссе, и за экзамен у студента по 10 баллов, то суммарный балл 100, что, кажется, больше, чем 10. Чтобы вернуться к изначальным границам баллов, нужно моделить суммарный балл на сумму весов параметров:\n\n$$\n\\text{final score } = \\frac{1 \\cdot \\text{test} + 3 \\cdot \\text{essay} + 6 \\cdot \\text{exam}}{1 + 3 + 6}\n$$\n\nКайф! Собственно, это и есть взвешенное среднее. Коэффициенты, на которые мы умножаем значение парамернов, называются *весами параметров*. И в общем виде формула принимает следующий вид.\n\n$$\n\\bar x = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i} = \\sum_{i=1}^n w_i' x_i,\n$$\n\nгде $x_i$ --- значения конкретных параметров, $w_i$ --- веса конкретных параметров, $w_i'$ --- нормированные веса параметров.\n\nВторая часть формулы показывается нам, что можно облегчить себе вычислительную жизнь, если заранее нормировать веса, то есть разделить каждый коэффициент на сумму коэффициентов:\n\n$$\nw_i' = \\frac{w_i}{\\sum_{i=1}^n w_i}\n$$\n\nТогда сумма коэффициентов будет равна единице. Так чаще всего и поступают, так как тогда коэффициент будет представлять долю, которую весит данный параметр в суммарной оценке. Удобно, практично, красиво.\n\nВзвещенное среднее часто применяется именно во всякого рода ассессментах, и не только образовательных. Например, вы HR-аналитик и оцениваете персонал. Вы аналитически вычисляете веса коэффициентов (допустим, с помощью линейной регрессии), а далее на их основе высчитаете интегральный балл, по которому будете оценивать сотрудников. Это как один из индустриальных примеров.\n\nТакже оно применяется, когда в наших данных есть какая-то группировка (например, когорты), при этом группы неравномерны.\n\n\n\n### Среднее vs медиана {#andan-descriptives-mean-vs-median}\n::: {.lab-chapter .lab-junior}\n:::\n\nПомимо того, что среднее и медиана информативны сами по себе, полезно смотреть на их взаимное расположение.\n\n> Сравнивать будем моду, медиану и среднее [арифметическое].\n\nИтак, все три статистики --- мода, медиана и среднее --- описывают центральную тенденцию --- некоторое значение изучаемой нами переменной, вокруг которого собираются другие значения. Но если их три и все они используются, значит между ними должны быть какие-то различия. Посмотрим, какие.\n\nВо-первых, очевидно, что *моду невозможно посчитать для непрерывной переменной*.\n\n<div class=\"advanced\">\n<details>\n<summary>*Нет, не очевидно*</summary>\nТак как вероятность того, что непрерывная случайная величина принимает своё конкретное значение, равна нулю, каждое наблюдение в нашей выборке будет уникально --- встретится ровно *один раз*. Вспомните [посмотрите] пример из предыдущей главы, где мы набирали числа из отрезка. Получается, что мода теряет свой смысл.\n</details>\n</div>\n\nВо-вторых, *медиану нельзя посчитать на номинальной шкале*. Кстати, почему?\n\n<div class=\"advanced\">\n<details>\n<summary>*Потому что*</summary>\nна номинальной шкале нет отношения порядка между элементами. Помните, на ней нельзя сравнивать на больше-меньше. Поэтому невозможно отсортировать наблюдения, а значит, и найти медиану.\n</details>\n</div>\n\nВ-третьих, *среднее тоже нельзя посчитать на номинальной шкале*.\n\n<div class=\"advanced\">\n<details>\n<summary>*Можно, но осторожно*</summary>\nВообще, конечно, да --- нельзя, потому что на номинальной шкале не определена операция сложения, входящая в вычисление среднего. Однако если на номинальной шкале есть только *две категории*, которые закодированы `0` и `1`, то посчитать среднее можно. Но что оно будет значить?\n\nИсходный математический смысл среднего явно утерян. Посмотрим на это по-другому: посчитать сумму единиц это всё равно, что посчитать *количество* единиц. То есть, если мы сложим все нули и единицы, то получим количество единиц среди всех наших наблюдений. А разделив количество единиц на количество наблюдений, мы получим *долю единиц* --- то есть долю наблюдений с лейблом `1`.\n\nВот так вот.\n</details>\n</div>\n\nВ-четвертых, *для дискретной переменной значение среднего арифметического будет не особо осмысленно.* Ну, скажем, странно сказать, что в аудитории в среднем стоят 15.86 столов или в российских семьях в среднем 1.5 ребенка. Конечно, в ряде случаев можно это как-то более-менее водержательно интерпретировать, но это требует усилий, а мы ленивые, поэтому лучше использовать медиану.\n\n\n**Итого, делаем следующие выводы**:\n\n* для номинальной шкалы пригодна только мода\n* для дискретных переменных подходят мода и медиана\n   * мода иногда лучше, так как точно всегда будет целым числом\n* для непрерывных переменных подходят медиана и среднее\n\n\nТеперь нам надо разобраться, как будут себя вести меры центральной тенденции в зависимости от *формы распределения*.\n\n```{r central_tendency_sampling, include=FALSE}\nset.seed(108)\nsymm <- sample(\n  x = seq(1, 10, 0.5),\n  size = 600,\n  replace = TRUE,\n  prob = c(\n    .05,\n    .05,\n    .07,\n    .1,\n    .1,\n    .15,\n    .20,\n    .30,\n    .35,\n    .5,\n    .35,\n    .30,\n    .20,\n    .15,\n    .1,\n    .1,\n    .07,\n    .05,\n    .05\n  )\n)\nasymm_right <- sample(\n  x = seq(1, 10, 0.5),\n  size = 600,\n  replace = TRUE,\n  prob = c(\n    .1,\n    .2,\n    .25,\n    .4,\n    .5,\n    .5,\n    .4,\n    .35,\n    .3,\n    .25,\n    .2,\n    .25,\n    .2,\n    .15,\n    .1,\n    .1,\n    .07,\n    .05,\n    .05\n  )\n)\nasymm_left <- sample(\n  x = seq(1, 10, 0.5),\n  size = 600,\n  replace = TRUE,\n  prob = c(\n    .03,\n    .05,\n    .07,\n    .1,\n    .15,\n    .15,\n    .2,\n    .2,\n    .25,\n    .25,\n    .3,\n    .35,\n    .5,\n    .5,\n    .4,\n    .4,\n    .25,\n    .2,\n    .2\n  )\n)\nbimodal <- sample(\n  x = seq(1, 10, 0.5),\n  size = 600,\n  replace = TRUE,\n  prob = c(\n    .05,\n    .05,\n    .07,\n    .1,\n    .1,\n    .2,\n    .3,\n    .35,\n    .3,\n    .15,\n    .1,\n    .15,\n    .20,\n    .40,\n    .50,\n    .25,\n    .1,\n    .05,\n    .05\n  )\n)\ncolors <- c(\"Mean\" = \"red4\", \"Median\" = \"blue4\", \"Mode\" = \"green4\")\n```\n\n**На симметричном распределении мода, медиана и среднее совпадают** [или, по крайней мере, находятся очень близко друг к другу]. Здесь и далее: красная линия --- среднее, синяя --- медиана, зелёная --- мода.\n\n\n```{r central_tendency_symm, echo=FALSE}\nggplot(NULL, aes(symm)) +\n  geom_histogram(aes(y = ..density..), alpha =.5, binwidth = .5) +\n  geom_density() +\n  geom_vline(xintercept = mean(symm), color = colors['Mean']) +\n  geom_vline(xintercept = median(symm), color = colors['Median']) +\n  # geom_vline(xintercept = mode(symm)-0.04, color = colors['Mode']) +\n  labs(x = 'Value',\n       y = 'Density')\n```\n\n**На асимметричном распределении мода [практически] в пике.** Практически, потому что функция плотности вероятности [черная линия на графике] на всегда точно аппроксимирует (в данном случае то же, что и сглаживает) эмпирическое распределение. На картинке ниже мы видим, что на гистограмме мода --- самый высокий столбик, что и показывает нам зелёная линия, которой обозначена мода. Однако при сглаживании гистограммы пик немного съехал, и мода оказалась не совсем в вершине графика функции плотности вероятности.\n\nВообще-то это нормально, потому что мода для непрерывной величины, которую мы и визуализируем с помощью графика плотности, либо не может быть посчитана вовсе, либо --- если так получилось, и у нас все же есть повторяющиеся значения --- не слишком хорошая мера центральной тенденции. В целом, и на симметричном распределении мода тоже может находиться немного в стороне от пика.\n\n**На асимметричном распределении медиана и среднее смещены в сторону хвоста. Среднее смещено сильнее медианы.** Это связано с тем, что медиана зависит только от количества наблюдений, а среднее ещё и от самих значений. На картинке ниже пример для распределения с *правосторонней* асимметрии (потому что хвост справа) --- среднее (красная линия) *правее* медианы (синяя линия).\n\n\n```{r central_tendency_asymm_right, echo=FALSE}\nggplot(NULL, aes(asymm_right)) +\n  geom_histogram(aes(y = ..density..), alpha =.5, binwidth = .5) +\n  geom_density() +\n  geom_vline(xintercept = mean(asymm_right), color = colors['Mean']) +\n  geom_vline(xintercept = median(asymm_right), color = colors['Median']) +\n  # geom_vline(xintercept = mode(asymm_right), color = colors['Mode']) +\n  labs(x = 'Value',\n       y = 'Density')\n```\n\nА это пример для распределения с *левосторонней* асимметрией (так как хвост слева) --- среднее (красная линия) *левее* медианы (синяя линия).\n\n\n```{r central_tendency_asymm_left, echo=FALSE}\nggplot(NULL, aes(asymm_left)) +\n  geom_histogram(aes(y = ..density..), alpha =.5, binwidth = .5) +\n  geom_density() +\n  geom_vline(xintercept = mean(asymm_left), color = colors['Mean']) +\n  geom_vline(xintercept = median(asymm_left), color = colors['Median']) +\n  # geom_vline(xintercept = mode(asymm_left), color = colors['Mode']) +\n  labs(x = 'Value',\n       y = 'Density')\n```\n\nДля того, чтобы лучше разобраться с тем, как большие и малые значения влияют на моду и медиану посмотрим такой пример. Пусть у нас есть оценки за выпускную квалификационную работу. Например, такие:\n\n```{r marks_creating, include=FALSE}\nmarks <- c(6, 7, 7, 8, 8)\n```\n```{r marks_vector}\nmarks\n```\n\nПосчитаем медиану и среднее:\n\n```{r}\nmedian(marks)\nmean(marks)\n```\n\nСреднее $7.2$ округлиться до $7$, то есть можно считать, что среднее и медиана совпали. Ну, ок.\n\nНо в комиссии сидят два требовательных доктора наук, которые поставили оценки, сильно отличающиеся от остальных:\n\n```{r marks_creating_2, include=FALSE}\nmarks <- c(6, 7, 7, 8, 8, 3, 4)\n```\n```{r marks_vector_2}\nmarks\n```\n\nПосчитаем медиану и среднее теперь:\n\n```{r}\nmedian(marks)\nmean(marks)\n```\n\nМедиана осталась на месте --- всё ещё $7$. А вот среднее $6.1$ округлится до $6$. Казалось бы, это немного, но в смысле оценок --- это прилично, и может сильно повлиять на GPA.\n\nИтого, среднее более чувствительно к нетипичным значениям (очень большим или очень малым).\n\n\nЕсть ещё один интересный вариант распределений --- **бимодальные**. Значит ли, что у этого распределения две моды? Не всегда. Посмотрим пример ниже:\n\n```{r central_tendency_bimodal, echo=FALSE}\nggplot(NULL, aes(bimodal)) +\n  geom_histogram(aes(y = ..density..), alpha =.5, binwidth = .5) +\n  geom_density() +\n  geom_vline(xintercept = mean(bimodal), color = 'red4') +\n  geom_vline(xintercept = median(bimodal), color = 'blue4') +\n  # geom_vline(xintercept = mode(bimodal), color = 'green4') +\n  labs(x = 'Value',\n       y = 'Density')\n```\n\nМы видим, что на графике есть два пика, однако строго математически мода одна (зеленая линия) --- и она в более высоком пике. Это логично, ибо там самые часто встречающиеся значения.\n\n<div class=\"advanced\">\nИ все жё содержательно мы не можем пренебречь вторым пиком. Почему нам он важен? Обычно бимодальное распределение --- это повод задуматься о том, что наша выборка неоднородна. *Бимодальное распределение как бы сложено из двух с центрами в двух пиках.* То есть в нашей выборке как будто бы *две подвыборки*, которые обладают разными распределениями интересующего нам признака.\n\nЧто с этим делать? Хорошо всегда иметь в данным какие-либо дополнительные переменные --- как минимум соцдем --- чтобы мы могли по данным попытаться предположить, какую группировку мы могли забыть учесть при планировании исследования.\n</div>\n\nСо средним и медианой происходит примерно то же, что и в случае асимметричного распределения. Второй пик смещает к себе обе меры центральной тенденции, причем среднее вновь сильнее, чем медиану.\n\n\n\n## Меры разброса {#andan-descriptives-variability}\n::: {.lab-chapter .lab-junior}\n:::\n\nИтак, мы разобрались с мерами центральной тенденции. Однако для описания распределения их оказвается недостаточно. Почему?\n\n## Зачем нужны меры разброса {#why_we_need_variation}\n\nПосмотрим на несколько распределений:\n\n```{r distributions_with_the_same_means_generation}\nset.seed(123)\ntibble(id = 1:100,\n       x1 = rnorm(100, mean = 2, sd = 1),\n       x2 = rnorm(100, mean = 2, sd = 3),\n       x3 = rnorm(100, mean = 2, sd = 0.5)) -> rnorm_three\n```\n\n```{r creating_labeller, include=FALSE}\nla <- c(x1 = \"Variable 1\", x2 = \"Variable 2\", x3 = \"Variable 3\")\n```\n\n```{r distributions_with_the_same_means_vis, echo=FALSE}\nrnorm_three %>% \n  pivot_longer(cols = c(\"x1\", \"x2\", \"x3\")) %>% \n  ggplot(aes(value)) +\n  geom_histogram(binwidth = .5) +\n  facet_wrap(~ name,\n             labeller = labeller(name = la)) +\n  labs(x = \"Value\",\n       y = \"Count\") +\n  scale_x_continuous(breaks = -5:15)\n```\n\nМетодом пристального взгляда можно установить, что у всех распределений одинаковые средние:\n\n```{r distributions_with_the_same_means_mean, echo=FALSE}\nrnorm_three %>% \n  pivot_longer(cols = c(\"x1\", \"x2\", \"x3\")) %>% \n  ggplot(aes(value)) +\n  geom_histogram(binwidth = .5) +\n  geom_vline(xintercept = 2, size = 1) +\n  facet_wrap(~ name,\n             labeller = labeller(name = la)) +\n  labs(x = \"Value\",\n       y = \"Count\") +\n  scale_x_continuous(breaks = -5:15)\n```\n\nОднако мы видим, что значения по-разному группируются вокруг среднего. Как они группируются --- плотно, как на третьем рисунке, или не особо, как на втором --- можно описать с помощью **мер разброса**, или **мер вариативности**.\n\n\n### Основные характеристики статистических данных {#key_features_of_data}\n\nВообще если посмотреть на это более свысока, то необходимость описания разброса определяется тем, что статистические данные обладают двумя ключевыми особенностями --- *неопределенностью* и *вариативностью*.\n\n* **Неопределённость** нам говорит о том, что мы не знаем, что именно мы получим в результате наших измерений для конкретной выборки. В том числе потому, что мы работаем на просторах случайных величин.\n* **Вариативность** означает, что наши данные будут различатся ещё и от респондента к респонденту. И между выборками тоже. Здесь и ошибка измерения, и различные смешения и ещё куча всего.\n\nБолее того, вариативность настолько важна, что она входит в расчёт любого статистического критерия. Именно вариативность --- а не центральная тенденция --- позволяет нам сделать вывод о том, что наши выборки различаются (или нет).\n\n\n\n### Минимум, максимум, размах {#andan-descriptives-range}\n::: {.lab-chapter .lab-junior}\n:::\n\nНачнем с самого простого. Как наиболее просто описать вариативность? Мы работаем с выборкой, а в выборке, как известно, ограниченное число наблюдений. А если оно ограниченое, значит среди них точно есть наибольшее --- *максимальное* --- и наименьшее --- *минимальное*.\n\nДопустим, мы открыли ведомость по «Анатомии и физиологии ЦНС» некоторой академической группы и пронаблюли следующее:\n\n```{r anat_marks_gen, include=FALSE}\nset.seed(123)\nanat_marks <- sample(3:10, \n                     size = 30,\n                     replace = TRUE, \n                     prob = c(0.2, 0.5, 0.8, 0.7, 0.7, 0.5, 0.3, 0.2))\n```\n\n```{r anat_marks}\nanat_marks\n```\n\nМы можем посчитать минимальное и максимальне значение по этому ряду наблюдений:\n\n```{r min_max_anat_marks}\nmin(anat_marks)\nmax(anat_marks)\n```\n\nПолучается, что оценки варьируются от $3$ до $10$. Ну, приемлемо. Разница между максимальным и минимальным значением называется **размах (range)**:\n\n$$\n\\mathrm{range}(X) = \\max(X) - \\min(X)\n$$\n\nПравда вот функция `range` в `R` вернёт не само значение размаха, а минимальное и максимальное значение. Ну, ладно.\n\n```{r range_anat}\nrange(anat_marks)\n```\n\nИ вот мы преисполнившиеся идёт описывать вариативность переменной с помощью размаха, но обнаруживаем в другой ведомости этой же группы (по «Введению в психологию») вот что:\n\n```{r intro_to_psy_gen, include=FALSE}\nset.seed(8)\nintro_psy_marks <- sample(3:10, \n                     size = 30,\n                     replace = TRUE, \n                     prob = c(0.1, 0.3, 0.3, 0.8, 0.8, 0.9, 0.3, 0.2))\n```\n\n```{r intro_to_psy_marks}\nintro_psy_marks\n```\n\nРазмах вроде как такой же:\n\n```{r}\nrange(intro_psy_marks)\n```\n\nЗначит ли это, что вариативность одинаковая?\n\nНарисуем.\n\n```{r range_problem, echo=FALSE}\ntibble(Anatomy = anat_marks,\n       Intro = intro_psy_marks) %>%\n  pivot_longer(cols = c(\"Anatomy\", \"Intro\")) %>% \n  ggplot(aes(value)) +\n  geom_bar() +\n  facet_grid(name ~ .) +\n  scale_x_continuous(breaks = 3:10) +\n  scale_y_continuous(breaks = 1:10)\n```\n\nКажется, что вариативность различна. Распределение оценок по «Анатомии и физиологии ЦНС» более равномерное, в то время как оценки по «Введению в психологию» активнее группируются где-то в середине.\n\nШтош, размах хоть и дает нам некоторую информацию о вариативности, нам этого маловато. Будем искать другие меры разброса.\n\n\n\n### Среднее абсолютное отклонение {#andan-descriptives-average-absolute-deviation}\n::: {.lab-chapter .lab-middle}\n:::\n\n#### Среднее абсолютное отклонение от среднего {#andan-descriptives-mean-absolute-deviation-around-the-mean}\n::: {.lab-chapter .lab-middle}\n:::\n\n#### Среднее абсолютное отклонение от медианы {#andan-descriptives-mean-absolute-deviation-around-the-median}\n::: {.lab-chapter .lab-middle}\n:::\n\n#### Медианное абсолютное отклонение {#andan-descriptives-median-absolute-deviation}\n::: {.lab-chapter .lab-middle}\n:::\n\n\n### Дисперсия {#andan-descriptives-variance}\n::: {.lab-chapter .lab-junior}\n:::\n\nХотя описание разброса переменных с помощью квантилей (в частности, квартилей) может дать нам много полезной информации, все же у них есть существенный недостаток: они никак не взаимодействуют с самими значениями нашей переменной.\n\nДействительно, мы делим нашу сортированную выборку на равные части, и смотрим, что в эти части попало. Но хотелось бы как-то учесть ещё и сами значения переменной в некотрой числовой мере разброса.\n\nНу, хорошо. Поступим следующим образом. Мы все ещё хотим узнать, как наши значения группируются вокруг среднего. В предыдущей главе мы уже видели, что наши наблюдения отклоняются от среднего значения --- значит мы можем посчитать отклонение для каждого наблюдения:\n\n$$\n\\bar x - x_i\n$$\n\nОкей. Если мы сложим все отклоненияи и поделим на их количество (которое равно количеству наблюдений), то мы получим среднее отклонение, да?\n\n$$\n\\frac{1}{n} \\sum_{i=1}^n \\bar x - x_i\n$$\n\nДа. Однако есть одна проблема. В прошлой главе мы [выяснили](#mean_features), что сумма отклонений от среднего равна нулю, а значит и среднее отклонение также будет равно нулю.\n\nХорошо. Но отрицательные значения ведь можно победить! Есть два пути:\n\n* **Модуль.** Преимущество первого в том, что размерность величины разброса остается той же, что и у измеряемой переменной.\n* **Квадрат.** Преимущество второго в том, что сильные отклонения будут оказывать более сильное влияние на окончательное значение статистики, в то время как для первого малые и большие отклонения равноценны.\n\nВторой путь на практике оказывается полезнее, так как мы хотим, чтобы сильно отличающиеся наблюдения вносили вклад в меру разброса.\n\nВозведя отклонения в квадрат, получим формулу **дисперсии (вариации, variation)**:\n\n$$\nD(X) = \\mathrm{var}(X) = \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (\\bar x - x_i)^2\n$$\n\nГениально.\n\nНе совсем. Формула, которую мы получили, пригодна для расчета **дисперсии генеральной совокупности** --- на выборке же она будет давать неточную оценку.\n\nЧтобы получить точную (несмещенную) оценку **дисперсии по выборке**, нам нужно исправить знаменатель дроби --- вместо $n$ использовать $n-1$:\n\n$$\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^n (\\bar x - x_i)^2\n$$\n\nНо почему?\n\n#### Степени свободы {#andan-descriptives-df}\n\nВо всём виновата выборка.\n\nВзглянем на формулу дисперсии: в неё входит среднее арифметическое. То есть для того, чтобы рассчитать дисперсию на выборке, *сначала* нам необходимо *на этой же выборке* рассчитать среднее. Тем самым, мы как бы «фиксируем» нашу выборку этим средним значением --- у значений нашего распределения становится меньше свободы для варьирования. Теперь свободно варьироваться могут $n-1$ наблюдение, так как последнее всегда будет возможно высчитать, исходя из среднего значения. По этой причине нам необходимо корректировать исходную формулу расчета дисперсии.\n\nА что если не корректировать?\n\nМы стремимся к тому, чтобы наши расчеты на выборке достаточно точно [на столько, на сколько это возможно] отражали то, что происходит в генеральной совокупности. Математики-статистики выяснили, что та оценка, которая хорошо подходит для расчета дисперсии генеральной совокупности, при применении на выборке даёт *смещенные* оценки. То есть оценка выборочной дисперсии по формуле дисперсии для генеральной совокупности содержит в себе *смещение* --- некоторую систематическую ошибку. Это нехорошо.\n\nК концепту степеней свободы мы ещё неоднократно вернемся. Сейчас хотелось бы, чтобы сформировалось какое-то минимальное более-менее освязаемое понимание того, почему они вообще нам нужны. Если на основе предыдущих абзацев раздела этого сделать не получилось, то давайте попробуеи воспользоваться следующим рассуждением.\n\nНа выборке происходят некоторые статистические преколы, которые несколько портят нам жизнь, и нам их неободимо учесть, чтобы адекватно оценивать то, что происходит в генеральной совокупности. В частности, нам необходимо учитывать количество степеней свободы, которое есть в нашей выборке. Для расчета выборочной дисперсии оно равно $n-1$, так как мы для того, чтобы рассчитать дисперсию по выборке, нам сначала *по той же самой выборке* надо рассчитать *ещё одну* оценку --- среднее арифметическое. Этот расчет заберет одну степень свободы у нашей выборки.\n\n#### Дисперсия генеральной совокупности {#andan-descriptives-population-variance}\n::: {.lab-chapter .lab-junior}\n:::\n\n#### Дисперсия выборки {#andan-descriptives-sample-variance}\n::: {.lab-chapter .lab-junior}\n:::\n\n### Стандартное отклонение {#andan-descriptives-standard-deviation}\n::: {.lab-chapter .lab-junior}\n:::\n\nИ вот мы получили невероятное! У нас есть формула расчета меры разброса, которая позволяет учесть сами значения переменной! Ну не чудо ли!\n\nЧудо, конечно, однако есть некоторая проблема. Мы возводили отклонения в квадрат. Представим, что мы хотим посчитатить дисперсию роста студентов психфака. Пусть мы измеряли рост в метрах. Отклонения тоже будут в метрах (потому что среднее --- это тоже метры, а если из метров вычитать метры, то мы получим метры). А при возведении метров в квадрат получаются метры в квадрате. Очевидно, что если мы модели квадратные метры на некоторое число ($n$), они все еще останутся метрами в квадрате.\n\nО, нет! А счастье было так близко, так возможно! Получается, мы не можем интерпретировать эту меру разброса? Не сможем даже нарисовать?\n\nДа, но это не очень большая беда. Для того, чтобы вернуться обратно к единицам измерения нашей переменной, нам всего лишь нужно извлечь корень из дисперсии:\n\n$$\n\\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\bar x - x_i)^2}\n$$\n\nМы получили величину, называемую **стандартным отклонением (standard deviation)**. Чем она хороша? Тем, что её размерность совпадает с размерностью нашей переменной. Стандартное отклонение уже может быть достаточно интерпретабельно и хорошо визуализируемо.\n\nКстати, формула выше, которая [что-то очень напоминает](#quadratic_mean), --- это **стандартное отклонение генеральной совокупности**, потому что под корнем стоит дисперсия генеральной совокупности.\n\nЧтобы посчитать **стандартное отклонение по выборке**, нам надо извлечь корень из выборочной дисперсии:\n\n$$\ns = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (\\bar x - x_i)^2}\n$$\n\n\n\n### Свойства дисперсии и стандартного отклонения {#andan_descriptives_var_features}\n\n* **Если к каждому значению распределения прибавить некоторое число (константу), то дисперсия не изменится.**\n\n$$\nD_{x+c} = D_{x}\n$$\n\nВот почему:\n\n$$\nD_{x+c} = \\frac{\\sum_{i=1}^n \\big((\\bar x + c) - (x_i + c)\\big)^2}{n-1} = \\frac{\\sum_{i=1}^n \\big(\\bar x + c - x_i - c\\big)^2}{n-1} = \\frac{\\sum_{i=1}^n \\big(\\bar x - x_i\\big)^2}{n-1} = D_x\n$$\n\n* **Если каждое значение распределение умножить на некоторое число (константу), то дисперсия увеличится в** $c^2$ **раз.**\n\n$$\nD_{x \\times c} = D_{x} \\times c^2\n$$\n\nВот почему:\n\n$$\nD_{x \\times c} = \\frac{\\sum_{i=1}^n (c\\bar x - cx_i)^2}{n-1} = \\frac{\\sum_{i=1}^n c^2(\\bar x - x_i)^2}{n-1} = \\frac{c^2 \\sum_{i=1}^n (\\bar x - x_i)^2}{n-1} = D_x \\times c^2\n$$\n\n* **Если к каждому значению распределения прибавить некоторое число (константу), то стандартное отклонение не изменится.**\n\n$$\ns_{x+c} = s_x\n$$\n\nЭто следует из свойства дисперсии:\n\n$$\ns_{x+c} = \\sqrt{D_{x+c}} = \\sqrt{D_x} = s_x\n$$\n\nКак мы уже видели, распределение просто сдвигается на константу. Например, если к каждому значению синего распределения прибавить $2$, получится красное --- разброс у обоих распределений одинаковый:\n\n```{r creating_tibble_for_feature_sd_vis_1, include=FALSE}\nsmpl1 <- tibble(x1 = seq(-3, 3, by = .001),\n               y1 = dnorm(x1),\n               x2 = x1 + 2,\n               y2 = dnorm(x2, mean = 2))\n```\n\n```{r sd_feature_1, echo=FALSE}\nsmpl1 %>% \n  ggplot() +\n  geom_line(aes(x1, y1), color = \"blue4\") +\n  geom_line(aes(x2, y2), color = \"red4\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"blue4\") +\n  geom_vline(xintercept = 2, linetype = \"dashed\", color = \"red4\") +\n  labs(x = \"Value\", y = \"Density\")\n```\n\n\n* **Если каждое значение распределение умножить на некоторое число (константу), то стандартное отклонение увеличится во столько же раз.**\n\n$$\ns_{x \\times c} = s_x \\times c\n$$\n\nЭто также следует из свойства дисперсии:\n\n$$\ns_{x \\times c} = \\sqrt{D_{x \\times c}} = \\sqrt{D_x \\times c^2} = s_x \\times c\n$$\n\nНапример, здесь каждое значение синего распределения умножили на $3$ и получили красное --- разброс также увеличился в три раза, поэтому распределение более плоское:\n\n```{r creating_tibble_for_feature_vis_4, include=FALSE}\nsmpl2 <- tibble(x1 = seq(-2, 4, by = .001),\n               y1 = dnorm(x1, mean = 1),\n               x2 = x1 * 3,\n               y2 = dnorm(x2, mean = 3, sd = 3))\n```\n\n```{r sd_feature_2, echo=FALSE}\nsmpl2 %>% \n  ggplot() +\n  geom_line(aes(x1, y1), color = \"blue4\") +\n  geom_line(aes(x2, y2), color = \"red4\") +\n  geom_vline(xintercept = 1, linetype = \"dashed\", color = \"blue4\") +\n  geom_vline(xintercept = 3, linetype = \"dashed\", color = \"red4\") +\n  labs(x = \"Value\", y = \"Density\")\n```\n\n\n\n\n### Квантили {#andan-descriptives-quantiles}\n::: {.lab-chapter .lab-junior}\n:::\n\n## Квантили {#quantiles}\n\nВозьмем распределение суммарного балла по шкале «Доверие к техническим интеллектуальным системам». Выглядит оно как-то так:\n\n```{r taia_data, include=FALSE}\ntaia <- read_csv(\"https://github.com/angelgardt/taia/raw/master/data/taia.csv\")\npr_items_0 <- colnames(taia)[8:17]\nco_items_0 <- colnames(taia)[18:27]\nut_items_0 <- colnames(taia)[28:39]\nfa_items_0 <- colnames(taia)[40:49]\nde_items_0 <- colnames(taia)[50:60]\nun_items_0 <- colnames(taia)[61:72]\ntaia_items_0 <- colnames(taia)[8:72]\ntaia %>% \n  select(id, all_of(taia_items_0)) %>% \n  pivot_longer(all_of(taia_items_0),\n               names_to = \"subscale\",\n               values_to = \"score\") %>% \n  mutate(subscale = str_remove_all(subscale, \"[:digit:]{2}\") %>% toupper()) %>% \n  group_by(id, subscale) %>% \n  summarise(total_score = sum(score)) %>% \n  ungroup() %>% \n  pivot_wider(id_cols = id,\n              names_from = subscale,\n              values_from = total_score) %>% \n  # relocate(after = c(id, PR, CO, UT, FA, DE, UN)) %>% \n  mutate(DT = PR + CO + UT + FA + DE + UN) %>% \n  full_join(taia) -> taia\n```\n\n```{r taia_score_vis, echo=FALSE}\nggplot(taia, aes(DT)) +\n  geom_histogram(binwidth = 3) +\n  labs(x = \"Trust in Artificial Intelligent Agents\",\n       y = \"Count\")\n```\n\nТеперь нам понадобится определение квантиля распределения.\n\n**Квантиль** --- это значение переменной, которое *не превышается* с определенной вероятностью (обозначим её $p$). Иначе говоря, *слева* от значения квантиля лежит $p\\%$ наблюдений.\n\nПосмотрим на картинки.\n\n*Слева* относительно квантиля-0.05 ($x_{0.05}$) лежит 5% наблюдений:\n\n```{r fifth_vis, echo=FALSE}\nggplot() +\n  geom_histogram(data = taia, aes(DT),\n                 binwidth = 3) +\n  geom_histogram(data = NULL, aes(taia$DT[taia$DT < quantile(taia$DT, .05)]), \n                 binwidth = 3, fill = \"green4\") +\n  geom_vline(xintercept = quantile(taia$DT, .05),\n             size = 1.5) +\n  annotate(geom = \"text\", label = \"5%\", x = 100, y = 5) +\n  labs(x = \"Trust in Artificial Intelligent Agents\",\n       y = \"Count\")\n```\n\n*Слева* относительно квантиля-0.68 ($x_{0.68}$) лежит 68% наблюдений:\n\n```{r 68_vis, echo=FALSE}\nggplot() +\n  geom_histogram(data = taia, aes(DT),\n                 binwidth = 3) +\n  geom_histogram(data = NULL, aes(taia$DT[taia$DT < quantile(taia$DT, .68)]), \n                 binwidth = 3, fill = \"green4\") +\n  geom_vline(xintercept = quantile(taia$DT, .68),\n             size = 1.5) +\n  annotate(geom = \"text\", label = \"68%\", x = 160, y = 10) +\n  labs(x = \"Trust in Artificial Intelligent Agents\",\n       y = \"Count\")\n```\n\n*Слева* относительно квантиля-0.99 ($x_{0.99}$) лежит 99% наблюдений:\n\n```{r 99_vis, echo=FALSE}\nggplot() +\n  geom_histogram(data = taia, aes(DT),\n                 binwidth = 3) +\n  geom_histogram(data = NULL, aes(taia$DT[taia$DT < quantile(taia$DT, .99)]), \n                 binwidth = 3, fill = \"green4\") +\n  geom_vline(xintercept = quantile(taia$DT, .99) + .5,\n             size = 1.5) +\n  annotate(geom = \"text\", label = \"99%\", x = 175, y = 10) +\n  labs(x = \"Trust in Artificial Intelligent Agents\",\n       y = \"Count\")\n```\n\nИтак, мы поняли, а также приняли и осознали, что такое квантиль. Неясно только, как он нам поможет описать вариативность данных.\n\n\n### Квартили {#quartiles}\n\nДля этого нам пригодятся специально обученные квантили. Оказалось достаточно удобно поделить все наблюдение на *четыре* равные части --- вот так:\n\n```{r quartiles_vis, echo=FALSE}\nggplot() +\n  geom_histogram(data = taia, aes(DT),\n                 binwidth = 3) +\n  geom_histogram(data = NULL, aes(taia$DT[taia$DT < quantile(taia$DT, .26)]), \n                 binwidth = 3, fill = \"purple4\") +\n  geom_histogram(data = NULL, \n                 aes(taia$DT[taia$DT > quantile(taia$DT, .47) & taia$DT < quantile(taia$DT, .75)]), \n                 binwidth = 3, fill = \"purple4\") +\n  geom_histogram(data = NULL, \n                 aes(taia$DT[taia$DT > quantile(taia$DT, .25) & taia$DT < quantile(taia$DT, .51)]), \n                 binwidth = 3, fill = \"blue4\") +\n  geom_histogram(data = NULL, \n                 aes(taia$DT[taia$DT > quantile(taia$DT, .745)]), \n                 binwidth = 3, fill = \"blue4\") +\n  geom_vline(xintercept = quantile(taia$DT, .25),\n             size = 1.5) +\n  geom_vline(xintercept = quantile(taia$DT, .50),\n             size = 1.5) +\n  geom_vline(xintercept = quantile(taia$DT, .75),\n             size = 1.5) +\n  annotate(geom = \"text\", label = \"25%\", x = 135, y = 2, color = \"white\") +\n  annotate(geom = \"text\", label = \"25%\", x = 160, y = 2, color = \"white\") +\n  annotate(geom = \"text\", label = \"25%\", x = 182.5, y = 2, color = \"white\") +\n  annotate(geom = \"text\", label = \"25%\", x = 205, y = 2, color = \"white\") +\n  labs(x = \"Trust in Artificial Intelligent Agents\",\n       y = \"Count\")\n```\n\nЗначения переменной, которые делят выборку на *четыре* равные части называются **квартили**. Получается, что\n\n* слева от **первого (нижнего) квартиля** ($Q_1$, $x_{0.25}$) лежит 25% наблюдений\n* слева от **второго (среднего) квартиля** ($Q_2$, $x_{0.50}$) лежит 50% наблюдений\n    * а значит и справа 50% --- получается второй квартиль делит выборку пополам --- это **медиана**\n* слева от **третьего (верхнего) квартиля** ($Q_3$, $x_{0.75}$) лежит 75% наблюдений\n\nЧетвертый квартиль не используется, потому что это максимальное значение --- слева от него лежит 100% наблюдений.\n\nКстати, можно также отметить, что первый квартиль --- это медиана нижней (меньшей) половины наблюдений, а третий --- медиана верней (большей) половины наблюдений.\n\nВот такая вот прикольная история.\n\n\n\n### Децили {#deciles}\n\nК слову, делить выборку можно не только на четверти --- можно поделить, скажем, на 10 частей и получить **децили**. Так, слева от *первого дециля* ($x_{0.10}$) лежит 10% наблюдений, а слева от *третьего* ($x_{0.30}$) --- 30%.\n\nДецили встречаются редко (в основном в психометрике), но знать о них полезно.\n\n\n### Перцентили {#percentiles}\n\nГораздо чаще встречаются **перцентили** --- значения переменной, которые делят выборку на 100 равных частей. Например, так устроен ваш рейтинг. Только стоит помнить, что в рейтинге отсчет ведется от максимального среднего балла, поэтому если у вас *нулевой перцентиль* ($x_{0.00}$) по программе, значит *выше* вас в рейтинге никого нет. А если ваш перцентиль, скажем, 36-ой ($x_{0.36}$), то выше вас в рейтинге 36% ваших однокурсников, то есть вы все ещё в первой половине рейтинга, что очень неплохо!\n\n\n### Интерквартильный размах {#iqr}\n\nИ --- о, ура! --- мы наконец-то добрались до того, ради чего тут собрались! Зная первый и третий квартили распределения, можно рассчитать **интерквартильный (межквартильный) размах (interquartile range, IQR)**.\n\n$$\n\\mathrm{IQR}(X) = Q_3(X) - Q_1(X)\n$$\n\nИнтерквартильный размах --- это разница между третьим и первым квартилем распределения. Эта величина описывает интервал значений признака, в котором лежит 50% наблюдений.\n\n```{r iqr_vis, echo=FALSE}\nggplot() +\n  geom_histogram(data = taia, aes(DT),\n                 binwidth = 3) +\n  geom_histogram(data = NULL, \n                 aes(taia$DT[taia$DT > quantile(taia$DT, .25) & taia$DT < quantile(taia$DT, .75)]), \n                 binwidth = 3, fill = \"blue4\") +\n  geom_vline(xintercept = quantile(taia$DT, .25),\n             size = 1.5) +\n  geom_vline(xintercept = quantile(taia$DT, .75),\n             size = 1.5) +\n  annotate(geom = \"text\", label = \"50%\", x = 170, y = 2, color = \"white\") +\n  labs(x = \"Trust in Artificial Intelligent Agents\",\n       y = \"Count\")\n```\n\nВ данном случае он равен 40:\n\n```{r iqr}\nIQR(taia$DT)\n```\n\nТо есть 50% наблюдений лежит в пределах 40 единиц шкалы.\n\n\n\n\n### Визуализация квартилей. Боксплот {#boxplot}\n\nОтображать квартили на гистограмме, во-первых, совершенно неудобно, а во-вторых, не то чтобы график получается информативный. Для визуализации квартилей придумали специальный тип графика --- **ящик с усами, или боксплот (boxplot)**.\n\n```{r boxplot, echo=FALSE}\ntaia %>% \n  ggplot(aes(y = DT)) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(size = 0))\n```\n\nПрикольная ерунда. Научимся его читать.\n\nЗначения переменной идут по вертикальной оси (оси ординат). По горизонтальной оси (оси абсцисс) здесь ничего не идет[^x_boxplot]. Жирная линия по середине ящика --- медиана (второй квартиль). Нижняя граница ящика --- первый квартиль, верхняя --- третий. Получается, что границы ящика показывают нам значения, в пределах которых лежит половина наблюдений.\n\n[^x_boxplot]: Но если мы рисуем несколько боксплотов рядом, то на оси `x` будет категориальная переменная.\n\nНижний ус --- первый квартиль минус полтора межквартильных размаха. Верхний ус --- третий квартиль плюс полтора мехквартильных размаха.\n\n```{r boxplot_annotated, echo=FALSE, warning=FALSE}\ntaia %>% \n  ggplot(aes(y = DT)) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(size = 0)) +\n  annotate(geom = \"text\", x = -.4, y = 170, label = TeX(\"$Q_2$\")) +\n  annotate(geom = \"text\", x = -.4, y = 200, label = TeX(\"$Q_3$\")) +\n  annotate(geom = \"text\", x = -.4, y = 145, label = TeX(\"$Q_1$\")) +\n  annotate(geom = \"text\", x = -.07, y = 95, label = TeX(\"$Q_1 - 1.5 IQR$\")) +\n  annotate(geom = \"text\", x = -.07, y = 245, label = TeX(\"$Q_3 + 1.5 IQR$\"))\n```\n\n<div class=\"advanced\">\n<details>\n<summary>*Замечание*</summary>\nЯщик может быть асимметричным --- то есть верхняя его часть (расстояние между медианой и третьим квартилем) и нижняя его часть (расстояние между медианой и первым квартилем) могут быть разными. Это нам говорит об асимметричности распределения. Усы также могут быть неравными, если один из них упирается в максимум / минимум --- тоже по причине асимметричности распределения.\n</details>\n</div>\n\nНу, допустим. А что тогда точки?\n\n\n#### Выбросы {#andan-descriptives-XX-iqr_outliers}\n\nВообще справедливо было бы задаться вопросом, а зачем нам вообще усы на этом графике? И почему мы прибавляем полтора межквартильных размаха?\n\nЭто *один из подходов* к определению нехарактерных значений --- выбросов. При исследовании данных мы часто задаемся вопросом, если ли в наших данных такие значения, которые сильно отличаются от распределения той или иной переменной. Но как определить это самое «сильно»?\n\nВот один из подходов. Будем считать, что значения, которые укладываются в интервал $(Q_1 - 1.5 \\times \\mathrm{IQR}, \\, Q_3 + 1.5 \\times \\mathrm{IQR})$, нас устраивают. Все что попадает в этот интервал --- это «нормальные», типичные значения нашей переменной. Те же, которые будут находиться за пределами этого интервала, мы назовем нетипичными, аномальными значениями, или **выбросами**. Эти значения и будут отмечены точками на графике boxplot.\n\nЧто с ними делать? Во-первых, содержательной анализировать. Выбросы могут возникнуть по разным причинам. Может быть испытуемый отвлекся на прилетевшего в окно голубя, и у нас в данных появилось время реакции 200 секунд. Такие выбросы мы можем исключить из данных. А возможно в нашу выборку попали какие-то люди, которые, скажем, очень сильно или очень слабо доверяют искусственному интеллекту (как в примере на рисунке). Эти наблюдения необходимо дополнительно проанализировать --- возможно, это представители специфических групп нашей генеральной совокупности (например, программисты-разработчики или люди пенсионного возраста). Анализ принесет нам дополнительную информацию, которую мы могли не учесть при планировании исследования. Крч, думать надо. И собирать побольше данных, чтобы можно было найти содержательную интерпретацию происходящему.\n\n\n## Сравнение мер разброса {#andan-descriptives_variation_comparison}\n\nКак и разные меры центральной тенденции, разные меры разброса по-своему хороши. Более того, они дружат с мерами центральной тенденции. Так, *с медианой используется мехквартильных размах*, а *со средним арифметическим --- стандартное отклонение*.\n\nРазмах подходит для всего сразу. Его стоит рассчитать, чтобы составить самое первое представление в разбросе, о границах измерения изучаемого признака [на нашей выборке].\n\nСтоит также отметить, что все, что мы тут обсуждали, совершенно не годиться для номинативных переменных. Однако у них тоже есть вариативность. Согласитель, что выборка из Питера, Москвы, и Казани более вариативна, чем выборка из Москвы. Аналогом меры разброса для номинальной переменной можно назвать количество уникальных значений этой переменной.\n\n\n\n## Асимметрия {#andan-descriptives-skewness}\n::: {.lab-chapter .lab-junior}\n:::\n\n## Эксцесс {#andan-descriptives-kurtosis}\n::: {.lab-chapter .lab-junior}\n:::\n\n\n## Итоги {#andan-descriptives-final}\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github","output-file":"andan-descriptives.html"},"language":{"toc-title-document":"Содержание","toc-title-website":"Оглавление","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Приложения","section-title-footnotes":"Сноски","section-title-references":"Ссылки","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Автор","title-block-author-plural":"Авторы","title-block-affiliation-single":"Аффилиация","title-block-affiliation-plural":"Аффилиация","title-block-published":"Версия от","title-block-modified":"Изменено","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Danger","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Строка","code-lines":"Строки","copy-button-tooltip":"Копировать","copy-button-tooltip-success":"Скопировано!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"Ничего не найдено","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Очистить","search-detached-cancel-button-title":"Отмена","search-submit-button-title":"Отправить","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Рисунок","crossref-tbl-title":"Таблица","crossref-lst-title":"Listing","crossref-thm-title":"Теорема","crossref-lem-title":"Лемма","crossref-cor-title":"Следствие","crossref-prp-title":"Утверждение","crossref-cnj-title":"Conjecture","crossref-def-title":"Определение","crossref-exm-title":"Пример","crossref-exr-title":"Упражнение","crossref-ch-prefix":"Глава","crossref-apx-prefix":"Приложение","crossref-sec-prefix":"Раздел","crossref-eq-prefix":"Уравнение","crossref-lof-title":"Список иллюстраций","crossref-lot-title":"Список таблиц","crossref-lol-title":"List of Listings","environment-proof-title":"Доказательство","environment-remark-title":"Remark","environment-solution-title":"Решение","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","bibliography":["references.bib"],"editor":"source","cover-image":"pics/book-cover.png","theme":{"light":["css/lightstyle.scss","css/custom.scss","css/tasklight.scss"],"dark":["css/darkstyle.scss","css/custom.scss","css/taskdark.scss"]}},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","highlight-style":"github","output-file":"andan-descriptives.pdf"},"language":{"toc-title-document":"Содержание","toc-title-website":"Оглавление","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Приложения","section-title-footnotes":"Сноски","section-title-references":"Ссылки","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Автор","title-block-author-plural":"Авторы","title-block-affiliation-single":"Аффилиация","title-block-affiliation-plural":"Аффилиация","title-block-published":"Версия от","title-block-modified":"Изменено","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Danger","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Строка","code-lines":"Строки","copy-button-tooltip":"Копировать","copy-button-tooltip-success":"Скопировано!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"Ничего не найдено","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Очистить","search-detached-cancel-button-title":"Отмена","search-submit-button-title":"Отправить","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Рисунок","crossref-tbl-title":"Таблица","crossref-lst-title":"Listing","crossref-thm-title":"Теорема","crossref-lem-title":"Лемма","crossref-cor-title":"Следствие","crossref-prp-title":"Утверждение","crossref-cnj-title":"Conjecture","crossref-def-title":"Определение","crossref-exm-title":"Пример","crossref-exr-title":"Упражнение","crossref-ch-prefix":"Глава","crossref-apx-prefix":"Приложение","crossref-sec-prefix":"Раздел","crossref-eq-prefix":"Уравнение","crossref-lof-title":"Список иллюстраций","crossref-lot-title":"Список таблиц","crossref-lol-title":"List of Listings","environment-proof-title":"Доказательство","environment-remark-title":"Remark","environment-solution-title":"Решение","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"editor":"source","documentclass":"scrreprt"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}