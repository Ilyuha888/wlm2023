# Описательные статистики {#andan-descriptives}
::: {.lab-chapter .lab-junior}
:::

```{r andan-desc-pkgs, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
theme_set(theme_bw())
```

{{< include _symbols.qmd >}}

## Виды статистики {#andan-descriptives-kinds-of-stats}
::: {.lab-chapter .lab-junior}
:::

Напомним себе, что статистика [как набор методов и инструментов] делится на два вида --- описательная статистика и статистика вывода.

- **Описательная статистика (descriptive statistics[^desc-stats-1])** занимается обработкой статистических данных, их наглядным представлением, и собственно описанием через некоторые характеристики.
  - Эти характеристики, количественно описывающие особенности имеющихся данных, называются **описательными статистиками (descriptive statistics[^desc-stats-2])**.
  - *Задача описательной статистики* --- ёмко описать имеющиеся данные и составить на основе этих описаний общее представление о них, а также обнаружить особенности, которые могут повлиять на дальнейший анализ.
- **Статистика вывода (inferential statistics)** занимается поиском ответов на содержательные вопросы, которые мы задаем данным в ходе их анализа в рамках научных и практических исследований.
  - Состоит из двух компонентов --- *тестирования статистических гипотез* и *статистических методов*.

[^desc-stats-1]: Mass (uncountable) noun
[^desc-stats-2]: Countable noun, plural in this case

::: {.callout-note title="Замечание о машинном обучении"}

В названии книги упомянуто «машинное обучение». Иногда его причисляют к статистике, иногда рассматривают отдельно. На самом же деле, статистические методы лежат где-то между статистикой вывода и машинным обучением.

Почему?

Дело в том, что на статистические методы можно смотреть по-разному.

- Если нашей задачей является поиск ответов на **исследовательские** вопросы о закономерностях, о связи каких-либо факторов или влиянии переменных друг на друга, то мы будем смотреть на статистические модели с точки зрения статистики вывода. Это позволит нам находить ответы на интересующие нас вопросы --- причем не важно, говорим мы о научных исследованиях или об исследованиях в индустрии.
- Если перед нами стоит задача хорошо **предсказывать** одни переменные на основании значений других --- например, выдавать рекомендации на Яндекс.Музыке или в Яндекс.Лавке --- то мы будем смотреть на те же статистические модели с точки зрения машинного обучения.

То есть, модели в анализе данных и машинном обучении одни и те же, но то, какую модель мы назовем хорошей и как мы эту «хорошесть» определим, будет отличаться в зависимости от задачи --- *исследовательская* или *предиктивная* --- которая перед нами стоит.

:::



## Меры центральной тенденции {#andan-descriptives-central-tendency}
::: {.lab-chapter .lab-junior}
:::

Итак, мы хотим описать наши данные. Точнее, распределения переменных, которые у нас в данных есть. Хотим сделать это просто и ёмко. Насколько просто и ёмко? Ну, допустим максимально — одним числом. Для этого неплохо подойдет значение переменной, которое лежит *в центре* распределения.

Как мы будем искать, что там в центре распределения? Зависит от [шкалы]() [@stevens46], в которой измерена конкретная переменная (@tbl-scales-cental-tendencies).

|    **Шкала**   |        **Мера центральной тенденции**        |
|:---------------|:---------------------------------------------|
| _Номинальная_  | Мода                                         |
| _Порядковая_   | Медиана                                      |
| _Интервальная_ | Среднее арифметическое                       |
| _Абсолютная_   | Среднее арифметическое, геометрическое и др. |

: Шкалы и меры центральной тенденции {#tbl-scales-cental-tendencies tbl-colwidths="[25,75]"}

Однако есть некоторые нюансы.


### Мода {#andan-descriptives-mode}
::: {.lab-chapter .lab-junior}
:::

Самый простой вариант найти центральную тенденцию --- это определить наиболее часто встречающееся значение переменной. Это значение называется *модой (mode)*.

::: {#def-mode-discrete}
**Мода** [дискретной переменной] --- наиболее часто встречающееся значение данной переменной.
:::

Например, у нас есть следующий ряд наблюдений по какой-то переменной:

$$
\begin{bmatrix}
1 & 3 & 4 & 6 & 3 & 2 & 3 & 3 & 2 & 4 & 1
\end{bmatrix}
$$

Если мы посчитаем, сколько раз встретилась каждое значение переменной и составим таблицу частот, то получим следующее:

$$
\begin{matrix}
\text{Значение} & 1 & 2 & 3 & 4 & 6 \\
\text{Частота}  & 2 & 2 & 4 & 2 & 1
\end{matrix}
$$

Очевидно, что $3$ встречается чаще других значений --- это и есть мода.

Понятно, что если на нашей шкале нет чисел, а есть текстовые лейблы, это ничего не меняет. Пусть у нас есть переменная с кодами аэропортов:

$$
\begin{bmatrix}
\text{DME} & \text{LED} & \text{IST} & \text{AER} & \text{IST} &\text{SVO} & \text{LED} & \text{VKO} & \text{LED} & \text{IST} & \text{IST} & \text{VKO} & \text{AER} & \text{DME}
\end{bmatrix}
$$

$$
\begin{matrix}
\text{Значение} & \text{DME} & \text{LED} & \text{IST} & \text{AER} & \text{SVO} & \text{VKO}\\
\text{Частота}  & 2 & 3 & 4 & 2 & 1 & 2
\end{matrix}
$$

Мода --- $\text{IST}$ (Международный аэропорт Стамбула, İstanbul Havalimanı).

Так мы действуем в случае с эмпирическим распределением. Если нам известна [функция вероятности переменной (probability mass function, PMF)](), то мы можем определить моду, основываясь на ней:

::: {#def-mode-discrete-pmf}
**Мода** [дискретной переменной] --- это значение переменной, при котором её функция вероятности принимает своё максимальное значение.
:::

$$
\text{mode}(X) = \arg \max(\text{PMF}(X)) = \arg \max_{x_i}(\prob (X = x_i)),
$$ {#eq-mode-pmf}

где $X$ --- дискретная случайная величина, $x_i$ --- значение этой случайной величины.

```{r mode-pmf, echo=FALSE}
#| label: fig-mode-pmf
#| fig-cap: "Определение моды с помощью функции вероятности"


tibble(x = 1:10,
       y = c(.01, .03, .07, .1, .1, .15, .2, .1, .09, .15)) |> 
  ggplot(aes(x, y)) +
  annotate(geom = "point", x = 7, y = 0.2, size = 7, shape = 21, color = "darkred", fill = "red", alpha = .5) +
  geom_point() +
  geom_vline(xintercept = 7, color = "darkred", linetype = "dashed") +
  annotate(geom = "text", label = "это максимум функции", 
           x = 8.5, y = 0.2, color = "darkred") +
  annotate(geom = "text", label = "это мода", 
           x = 7, y = 0, color = "darkred") +
  scale_x_continuous(breaks = 1:10) +
  labs(x = "Value", y = "Probability")
```

Окей, мы видим, что *мода отлично считается на дискретных переменных*. А как же быть с непрерывными?

[Напомним себе](), что вероятность того, что непрерывная случайная величина примет своё конкретное значение, равна нулю. Из этого следует, что все значения непрерывной случайное величины уникальны --- каждое повторяется только один раз. Получается, что строить частотную таблицу бессмысленно...

По этой причине **для непрерывных переменных моду не считают**.


#### Мода для непрерывной переменной {#andan-descriptives-mode-contunious}
::: {.lab-chapter .lab-middle}
:::

Да, это так. Действительно, посчитать моду для непрерывной переменной способом, аналогичным тому, что мы увидели выше, не получится. Однако математиков это не остановило.

Если мы посмотрим на [график плотности вероятности]() (probability density function, PDF), который является аналогом PMF для дискретных переменных, мы увидим, что какие-то значения встречаются чаще, а какие-то реже. Что в общем-то логично. Напомним себе, [как это выглядит](), например, для любимого [стандартного] [нормального распределения]():

```{r mode-continuous-data, echo=FALSE}
mode_cont_data <- tibble(x = seq(-4, 4, by = .01), y = dnorm(x))
mode_high_freq <- mode_cont_data |> filter(x > -.5 & x < .5)
mode_low_freq <- mode_cont_data |> filter(x > -2.5 & x < -1.5)
```

```{r mode-continuous-freqs, echo=FALSE}
#| label: fig-continuous-freqs
#| fig-cap: "Частоты интервалов значений непрерывной случайной величины на функции плотности распределения"


ggplot(mode_cont_data,
       aes(x, y)) +
  geom_line() +
  geom_polygon(data = tibble(y = c(0, 0), x = c(.5, -.5)) |> 
                 bind_rows(mode_high_freq),
               fill = "seagreen", alpha = .5) +
  geom_polygon(data = tibble(y = c(0, 0), x = c(-1.5, -2.5)) |>
                 bind_rows(mode_low_freq),
               fill = "royalblue", alpha = .5) +
  annotate(geom = "text", label = "эти значения встречаются часто",
           x = 0, y = .2, angle = 90, color = "darkgreen") +
  annotate(geom = "text", label = "эти значения\nвстречаются реже",
           x = -2.2, y = .15, angle = 90, color = "darkblue") +
  labs(x = "Value", y = "Density")
```

То есть, самые часто встречающиеся значения --- это **пик распределения**. Там и должна быть мода. Визуально это выглядит достаточно справедливо.

Математики так и решили:

::: {#def-mode-continuous}
**Мода** [непрерывной переменной] --- это значение переменной, при котором её функция плотности вероятности достигает локального[^local-max-mode] максимума.
:::

[^local-max-mode]: Здесь в примере локальный максимум функции плотности вероятности на интервале $(-4, \, 4)$ совпадает с глобальным максимумом --- мы об этом знаем, потому что форма распределения нам известна. В случае эмпрического распределения корректнее говорить именно о локальном максимуме, так как глобальный максимум нам не доступен ввиду того, что мы работаем с выборкой.

$$
\text{mode}(X) = \arg \max(\text{PDF}(X)) = \arg \max_{x \in S}f(x),
$$ {#eq-mode-pdf}

гдe $X$ --- непрерывная случайная величина, $x$ --- значение этой случайной величины, $S$ --- имеющаяся выборка значений переменной.

```{r mode-continuous-mode, echo=FALSE}
#| label: fig-continuous-mode
#| fig-cap: "Положение моды на функции плотности [стандартного] нормального распределения"


ggplot(mode_cont_data,
       aes(x, y)) +
  geom_line() +
  geom_polygon(data = tibble(y = c(0, 0), x = c(.5, -.5)) |> 
                 bind_rows(mode_high_freq),
               fill = "seagreen", alpha = .5) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "darkgreen") +
  geom_point(aes(x = 0, y = .4), color = "darkgreen", size = 2) +
  annotate(geom = "text", label = "мода тут",
           x = 0, y = 0, color = "darkgreen") +
  annotate(geom = "text", label = "локальный максимум тут",
           x = 1.5, y = 0.4, color = "darkgreen") +
  labs(x = "Value", y = "Density")
```

Хотя моду для непрерывной переменной вычислить можно, обычно этого не делают, так как достаточно других мер центральной тенденции для описания распределения.

***

::: {.callout-warning title="Take-home: мода"}
- мода --- это значение переменной, которое встречается в выборке чаще всего
- на практике она рассчитывается через построение частотной таблицы
- используется с дискретными (номинальными и порядковыми) переменными
- для непрерывных переменных её рассчитать можно, но обычного этого не делают
:::



### Унимодальные и полимодальные распределения {#andan-descriptives-unimodal-bimodal}
::: {.lab-chapter .lab-junior}
:::

Нормальное распределение, как и ряд других --- биномиальное, отрицательное биномиальное, пуассоновское --- относятся к *унимодальным*. Такие распределения имеют только *одну моду* (см. @fig-norm-mode, @fig-binom-mode, @fig-poiss-mode).

```{r mode-norm-mode, echo=FALSE}
#| label: fig-norm-mode
#| fig-cap: "Нормальное распределение (μ = 2$, σ = 0.5). Пунктирной линией обозначено положение моды."


tibble(x = seq(0, 4, by = .01),
       y = dnorm(x, mean = 2, sd = 0.5)) |> 
  ggplot(aes(x, y)) +
  geom_line() +
  geom_vline(xintercept = 2, linetype = "dashed") +
  labs(x = "Value", y = "Density")
```

```{r mode-binom-mode, echo=FALSE}
#| label: fig-binom-mode
#| fig-cap: "Биномиальное распределение (n = 50, p = 0.3). Пунктирной линией обозначено положение моды."


tibble(x = seq(0, 20, by = 1),
       y = dbinom(x, prob = 0.3, size = 50)) |> 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_vline(xintercept = 15, linetype = "dashed") +
  labs(x = "Value", y = "Probability")
```

```{r mode-poiss-mode, echo=FALSE}
#| label: fig-poiss-mode
#| fig-cap: "Распределение Пуассона (λ = 5.5). Пунктирной линией обозначено положение моды."


tibble(x = seq(0, 20, by = 1),
       y = dpois(x, lambda = 5.5)) |> 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_vline(xintercept = 5, linetype = "dashed") +
  labs(x = "Value", y = "Probability")
```

Это теоретические распределения. С эмпирическими распределениями дело обстоит так же, хотя они обычно менее гладенькие и красивые (см. @fig-mode-norm-sample и @fig-mode-nbinom-sample).

```{r mode-norm-sample, echo=FALSE}
#| label: fig-mode-norm-sample
#| fig-cap: "Эмпирическое распределение, сгенерированное из нормального распределения (μ = 8, σ = 4, n = 100). `set.seed(314)`. Пунктирной линией обозначено положение моды."

set.seed(314)
tibble(x = rnorm(100, mean = 8, sd = 4)) |> 
  ggplot(aes(x = x)) +
  geom_histogram(aes(y = after_stat(density)), 
                 fill = "gray90", binwidth = .5) +
  geom_density() +
  geom_vline(xintercept = 8.5, linetype = "dashed") +
  labs(x = "Value", y = "Density")
```

```{r mode-norm-sample, echo=FALSE}
#| label: fig-mode-nbinom-sample
#| fig-cap: "Эмпирическое распределение, сгенерированное из логнормального распределения (μ = 1.1, σ = 1.39, n = 30). `set.seed(314)`. Пунктирной линией обозначено положение моды."

set.seed(314)
tibble(x = rlnorm(n = 30, meanlog = 1.1, sdlog = 1.39)) |> 
  ggplot(aes(x = x)) +
  geom_histogram(aes(y = after_stat(density)), 
                 fill = "gray90", binwidth = .5) +
  geom_density() +
  geom_vline(xintercept = 1.35, linetype = "dashed") +
  labs(x = "Value", y = "Density")
```

Однако на практике возможны и другие ситуации. Например, такие (@fig-bimodal, @fig-polymodal):

```{r mode-bimodal, echo=FALSE}
#| label: fig-bimodal
#| fig-cap: Бимодальное распределение. Сгенерировано из двух нормальных распределений (μ<sub>1</sub> = 1.5, σ<sub>1</sub> = 0.4, n<sub>1</sub> = 80; μ<sub>2</sub> = 4, σ<sub>2</sub> = 0.5, n<sub>2</sub> = 40). `set.seed(65)`. Пунктирными линиями обозначены положения мод.

set.seed(65)
tibble(x = c(rnorm(80, 1.5, 0.4), rnorm(40, 4, 0.5))) |> 
  ggplot(aes(x)) +
  geom_histogram(aes(y = after_stat(density)), 
                 fill = "gray90", binwidth = .4) +
  geom_density() +
  geom_vline(xintercept = 1.5, linetype = "dashed") +
  geom_vline(xintercept = 4.1, linetype = "dashed") +
  labs(x = "Value", y = "Density")
```

```{r mode-polymodal, echo=FALSE}
#| label: fig-polymodal
#| fig-cap: Полимодальное распределение. Сгенерировано из двух нормальных распределений (μ<sub>1</sub> = 1.5, σ<sub>1</sub> = 0.3, n<sub>1</sub> = 80; μ<sub>2</sub> = 3.4, σ<sub>2</sub> = 0.5, n<sub>2</sub> = 40) и бета-распределения (α = 2, β = 4, n = 50). `set.seed(65)`. Пунктирными линиями обозначены положения мод.

set.seed(65)
tibble(x = c(rnorm(80, 1.5, 0.3), rnorm(40, 3.4, 0.5), rbeta(50, 2, 4))) |> 
  ggplot(aes(x)) +
  geom_histogram(aes(y = after_stat(density)), 
                 fill = "gray90", binwidth = .4) +
  geom_density() +
  geom_vline(xintercept = 0.4, linetype = "dashed") +
  geom_vline(xintercept = 1.45, linetype = "dashed") +
  geom_vline(xintercept = 3.5, linetype = "dashed") +
  labs(x = "Value", y = "Density")
```

В первом случае (@fig-bimodal) мы видим *два локальных максимума* функции плотности вероятности --- такое распределение называется **бимодальным**. Во втором случае (@fig-polymodal) функция плотности вероятности имеет *три локальных максимума* --- такое распределение называется **полимодальным**. Бимональное распределение является частным случаем полимодального распределения.

В прицнипе, пиков может быть и больше, однако при работе с реальными данными чаще всего мы сталкиваемся с бимодальными распределениями.

**Что это значит и что с этим делать?**

Бимодальное распределение сигнализирует нам о **гетерогенности выборки**. Если мы видим два выделяющихся пика, стоит подумать о том, что наша выборка неоднородна и в ней выделяются две подвыборки. Посмотрим на структуру выборки из примера выше (@fig-bimodal-struct):

```{r mode-bimodal-struct, echo=FALSE}
#| label: fig-bimodal-struct
#| fig-cap: "Структура бимодального распределения из @fig-bimodal. Для удобства сопоставления графиков плотностей вероятности по оси ординат отложены частоты."

set.seed(65)
tibble(x = c(rnorm(80, 1.5, 0.4), rnorm(40, 4, 0.5)),
       d = c(rep("A", times = 80), rep("B", times = 40))) -> bimodal_struct_data
gridExtra::grid.arrange(
  bimodal_struct_data |> 
    ggplot(aes(x)) +
    geom_histogram(fill = "gray90", binwidth = .4) +
    geom_density(aes(y = after_stat(count))) +
    geom_vline(xintercept = 1.5, linetype = "dashed") +
    geom_vline(xintercept = 4.1, linetype = "dashed") +
    labs(x = "Value", y = "Count"),
  
  bimodal_struct_data |> 
    ggplot(aes(x)) +
    geom_histogram(aes(fill = d), 
                   alpha = .3, binwidth = .4) +
    geom_density(aes(y = after_stat(count), color = d)) +
    geom_vline(xintercept = 1.5, linetype = "dashed") +
    geom_vline(xintercept = 4.1, linetype = "dashed") +
    guides(color = "none", fill = "none") +
    labs(x = "Value", y = "Count")
)
```

Действительно, наше распределение состоит из двух других распределений, у каждого из которого есть своя мода --- поэтому итоговое распределение получается бимодальным. Конечно, сейчас нам это очень удобно показать, потому что мы знаем, как это распределение генерировалось. Когда же у нас есть реальные данные и мы там наблюдаем такого «верблюда», бывает достаточно сложно сказать, что «пошло не так».

Само по себе распределение не даст нам ответ на вопрос, почему оно бимодальное --- чтобы выяснить причины такого поведения переменной нам потребуются другие данные. Обычно у вас в данных есть «соцдем» --- пол, возраст, сфера и место работы, уровень обрвазования и др. Попробуйте построить распределение с разбиением исследуемой бимодальной переменной по переменным «соцдема». Это, к сожалению, не является рецептом успеха, поскольку причина гетерогенности выборки может и не содержаться в ваших данных, но такое изучение данных станет хорошим показателем того, что вы не просто «забили» на странное распределение своей переменной, а поисследователи возможные его причины.

Если вам удалось найти причины гетерогенности выборки --- допустим, у вас выделяются подвыборки «бакалавры» и «магистры» --- стоит подумать о том, как обойтись с этой переменной в планируемом анализе, так как игнорировать её, по-видимому, нельзя, поскольку она влияет на вариатиность данных.

::: {.callout-tip title="Соцдем лишним не бывает"}
На этапе планирования исследования подумайте о том, чем могут отличаться ваши респонденты или испытуемые между собой, помимо индивидуальных различий.

- Если в эксперименте используете задачу мысленного вращения (mental rotation, [@shepard71]), вполне возможно, испытуемые, работающие в сфере 3D-моделирования или дизайна интерьеров, могут сформировать подвыборку.
- В случае HR-исследования, где фиксируется доход респондента, необходимо записать город, в котором он проживает и/или работает.
- При изучении удовлетворенности городским пространством важными пунктами станут беременность, наличие/отсутствие детей, наличие/отсутствие автомобиля и др.

И так далее. Примеров для каждого случая можно подобрать много.

Стоит ли, скажем, в первом случае сразу исключить из выборки 3D-моделлеров? Зависит. От количества времени и денег на проведение исследования. Однако *как минимум эту информацию надо зафиксировать в данных*. А решить, исключать ли этих респондентов из выборки или нет, можно и позже. Главно об этом написать в отчете/статье, когда будете описывать предобработку данных.
:::

***

::: {.callout-warning title="Take-home: бимодальное распределение"}
- бимодальное распределение намекает на неоднородность данных --- скорее всего, в выборке есть две подвыборки
- необходимо поискать в данных причины этой неодноросности, например, в социально-демографических переменных
- если удалось найти переменную, объясняющую бимодальность, стоит подумать о том, как её учитывать в планируемом анализе
:::



### Медиана {#andan-descriptives-median}
::: {.lab-chapter .lab-junior}
:::

Для номинальной шкалы мода --- это единственно возможная мера центральной тенденции, потому что на этой шкале отсутствует порядок элементов. На других шкалах наблюдения уже можно сортировать по возрастнию или убыванию, поскольку начиная с ранговой (порядковой) шкалы на всех них определена операция сравнения на «больше-меньше».

Возьмем тот же ряд наблюдений, что и в предыдущем разделе:

$$
\begin{bmatrix}
1 & 3 & 4 & 6 & 3 & 2 & 3 & 3 & 2 & 4 & 1
\end{bmatrix}
$$

Отсортируем наблюдения по возрастанию:

$$
\begin{bmatrix}
1  & 1 & 2 & 2 & 3 & 3 & 3 & 3 & 4 & 4 & 6
\end{bmatrix}
$$

Наша задача --- определить центральную тенденцию. Давайте посмотрим, что оказалось в середине отсортированного ряда:


$$
\begin{bmatrix}
1 & 1 & 2 & 2 & 3 & \mathbf{3} & 3 & 3 & 4 & 4 & 6
\end{bmatrix}
$$

Это медиана. В данном случае она равна $3$.

::: {#def-median}
**Медиана (median)** --- это значение, которое располагается на середине отсортированного ряда значений переменной. 
:::

Медиана делит все наблюдения переменной ровно пополам и половина наблюдений оказывается по одну сторону от медианы, а половина --- по другую.

Если число наблюдений нечётное, то всё ясно --- в середине отсортированного ряда будет какое-то значение. А если число наблюдений чётное? Тогда мы попадаем между значениями. 

Возьмем для примера такой вектор наблюдений:

$$
\begin{bmatrix}
14 & 10 & 9 & 16 & 30 & 3 & 25 & 8 & 18 & 7
\end{bmatrix}
$$

Отсортируем:

$$
\begin{bmatrix}
3 & 7 & 8 & 9 & 10 & 14 & 16 & 18 & 25 & 30
\end{bmatrix}
$$

Найдем середину:

$$
\begin{bmatrix}
3 & 7 & 8 & 9 & 10 & | & 14 & 16 & 18 & 25 & 30
\end{bmatrix}
$$

В таком случае в качестве медианы берется среднее между двумя срединными значениями:

$$
\text{median} = \frac{10 + 14}{2} = 12
$$

Итого, формализовать вычисление медианы можно следующим образом:

$$
\text{median}(X) = X(a) =
\cases{
X\left(\frac{n+1}{2}\right), & if  2 | n \\
\dfrac{X(\frac{n}{2}) + X(\frac{n}{2} + 1)}{2}, & otherwise
}
$$ {#eq-median-formula}

где $X$ --- ряд наблюдений случайной величины, $n$ --- число наблюдений, $X(a)$ --- наблюдение с индексом $a$ в отсортированном векторе $X$.

Если мы будем смотреть на медиану с позиции описания распределения, то она будет той самой линией, которая разделит площадь под графиком функции плотности вероятности пополам:

```{r median-norm, echo=FALSE}
#| label: fig-median-norm
#| fig-cap: "Медиана нормального распределения"

tibble(x = seq(-4, 4, by = .01),
       y = dnorm(x)) -> median_norm_data 
median_norm_data |> filter(x >= 0) -> median_norm_upper
median_norm_data |> filter(x <= 0) -> median_norm_lower
median_norm_data |> 
  ggplot(aes(x, y)) +
  geom_line() +
  geom_polygon(data = tibble(y = c(0, 0), x = c(4, 0)) |> 
                 bind_rows(median_norm_upper),
               fill = "seagreen", alpha = 0.5) +
  geom_polygon(data = tibble(y = c(0, 0), x = c(0, -4)) |> 
                 bind_rows(median_norm_lower),
               fill = "royalblue", alpha = 0.5) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  annotate(geom = "text", label = "эти значения\nбольше медианы \n\n 50%", 
           color = "darkgreen", x = 1, y = 0.05) +
  annotate(geom = "text", label = "эти значения\nменьше медианы \n\n 50%", 
           color = "darkblue", x = -1, y = 0.05) +
  annotate(geom = "text", label = "это медиана", 
           x = -0.01, y = 0)
```

При этом форма распределения не имеет значения --- площадь под графиком всегда будет делиться пополам:

```{r median-left-skew, echo=FALSE}
#| label: fig-median-left-skew
#| fig-cap: Медиана распределения с отрицательной асимметрией.

set.seed(115)
tibble(x = seq(0.3, 1, by = .001),
       y = dbeta(x, 7, 1.5)) -> median_leftskew_data

median_leftskew_data |> 
  filter(x >= .845) %>%
  bind_rows(
    tibble(y = c(0, 0),
           x = c(max(median_leftskew_data$x), .845)),
    .) -> median_leftskew_data_upper

median_leftskew_data |> 
  filter(x <= .845) %>%
  bind_rows(
    tibble(y = c(0, 0),
           x = c(.845, min(median_leftskew_data$x))),
    .) -> median_leftskew_data_lower

# DescTools::AUC(median_leftskew_data_upper$x,
#                median_leftskew_data_upper$y)

median_leftskew_data |> 
  ggplot(aes(x, y)) +
  geom_line() +
  geom_polygon(data = median_leftskew_data_lower, 
               aes(x = x, y = y), fill = "royalblue", alpha = .5) +
  geom_polygon(data = median_leftskew_data_upper, 
               aes(x = x, y = y), fill = "seagreen", alpha = .5) +
  geom_vline(xintercept = 0.845, linetype = "dashed") +
  annotate(geom = "text", label = "50%", 
           color = "darkgreen", x = .9, y = 1) +
  annotate(geom = "text", label = "50%", 
           color = "darkblue", x = .8, y = 1) +
  labs(x = "Value", y = "Density")
```

```{r median-right-skew, echo=FALSE}
#| label: fig-median-right-skew
#| fig-cap: Медиана распределения с положительной асимметрией.

set.seed(115)
tibble(x = seq(0, 5, by = .001),
       y = dgamma(x, 2, 2)) -> median_rightskew_data

median_rightskew_data |>
  filter(x >= .85) %>%
  bind_rows(
    tibble(y = c(0, 0),
           x = c(max(median_rightskew_data$x), .85)),
    .) -> median_rightskew_data_upper

median_rightskew_data |>
  filter(x <= .85) %>%
  bind_rows(
    tibble(y = c(0, 0),
           x = c(.85, min(median_rightskew_data$x))),
    .) -> median_rightskew_data_lower

# DescTools::AUC(median_rightskew_data_lower$x,
#                median_rightskew_data_lower$y)

median_rightskew_data |> 
  ggplot(aes(x, y)) +
  geom_line() +
  geom_polygon(data = median_rightskew_data_lower, 
               aes(x = x, y = y), fill = "royalblue", alpha = .5) +
  geom_polygon(data = median_rightskew_data_upper, 
               aes(x = x, y = y), fill = "seagreen", alpha = .5) +
  annotate(geom = "text", label = "50%", 
           color = "darkgreen", x = 1.25, y = .2) +
  annotate(geom = "text", label = "50%", 
           color = "darkblue", x = .5, y = .2) +
  geom_vline(xintercept = 0.85, linetype = "dashed") +
  labs(x = "Value", y = "Density")
```

```{r median-bimodal, echo=FALSE}
#| label: fig-median-bimodal
#| fig-cap: Медиана бимодального распределения.

set.seed(115)
tibble(x = c(rnorm(80, 1.5, 0.4), rnorm(40, 4, 0.5))) -> median_bimodal_data

median_bimodal_data |> 
  ggplot(aes(x)) +
  geom_density() -> g
ggplot_build(g) -> b
b$data[[1]] -> median_bimodal_data_build

median_bimodal_data$x |> median() -> m
# median_bimodal_data_build$x |> median() -> m

median_bimodal_data_build |> 
  select(y, x) |> 
  filter(x >= m) %>% 
  bind_rows(
    tibble(y = c(0, 0),
           x = c(max(median_bimodal_data_build$x), m)),
    .) -> median_bimodal_data_build_upper

median_bimodal_data_build |>
  select(y, x) |> 
  filter(x <= m) %>%
  bind_rows(
    tibble(y = c(0, 0),
           x = c(m , min(median_bimodal_data_build$x))),
    .) -> median_bimodal_data_build_lower

# DescTools::AUC(x = median_bimodal_data_build_upper$x,
#                y = median_bimodal_data_build_upper$y)
# DescTools::AUC(x = median_bimodal_data_build_lower$x,
#                y = median_bimodal_data_build_lower$y)

median_bimodal_data_build |> 
  ggplot(aes(x, y)) +
  geom_histogram(data = median_bimodal_data,
                 aes(x = x, y = after_stat(density)), 
                 fill = "gray90", binwidth = .4) +
  geom_line() +
  geom_polygon(data = median_bimodal_data_build_lower, 
               aes(x = x, y = y), fill = "royalblue", alpha = .5) +
  geom_polygon(data = median_bimodal_data_build_upper, 
               aes(x = x, y = y), fill = "seagreen", alpha = .5) +
  geom_vline(xintercept = m, linetype = "dashed") +
  annotate(geom = "text", label = "50%", 
           color = "darkgreen", x = 2.25, y = .1) +
  annotate(geom = "text", label = "50%", 
           color = "darkblue", x = 1.25, y = .1) +
  labs(x = "Value", y = "Density")
```

***

::: {.callout-warning title="Take-home: медиана"}
- медиану можно расчитать только на шкалах, где задан порядок (ранговая, интервальная, абсолютная)
- медиана делит выборку наблюдений на две равные части
- линия медианы раздели площадь под графиком функции плотности вероятности пополам
:::



### Среднее {#andan-descriptives-mean}
::: {.lab-chapter .lab-junior}
:::

#### Арифметическое среднее {#andan-descriptives-arithmetic-mean}
::: {.lab-chapter .lab-junior}
:::

#### Усеченное среднее {#andan-descriptives-trimmed-mean}
::: {.lab-chapter .lab-junior}
:::

#### Геометрическое среднее {#andan-descriptives-geometric-mean}
::: {.lab-chapter .lab-middle}
:::

#### Квадратичное среднее {#andan-descriptives-quandratic-mean}
::: {.lab-chapter .lab-middle}
:::

#### Гармоническое среднее {#andan-descriptives-harmonic-mean}
::: {.lab-chapter .lab-middle}
:::

#### Взвешенное среднее {#andan-descriptives-weighted-mean}
::: {.lab-chapter .lab-junior}
:::

### Среднее vs медиана {#andan-descriptives-mean-vs-median}
::: {.lab-chapter .lab-junior}
:::

Помимо того, что среднее и медиана информативны сами по себе, полезно смотреть на их взаимное расположение.

ПРОДОЛЖИТЬ


## Меры разброса {#andan-descriptives-variability}
::: {.lab-chapter .lab-junior}
:::

### Минимум, максимум, размах {#andan-descriptives-range}
::: {.lab-chapter .lab-junior}
:::

### Среднее абсолютное отклонение {#andan-descriptives-average-absolute-deviation}
::: {.lab-chapter .lab-middle}
:::

#### Среднее абсолютное отклонение от среднего {#andan-descriptives-mean-absolute-deviation-around-the-mean}
::: {.lab-chapter .lab-middle}
:::

#### Среднее абсолютное отклонение от медианы {#andan-descriptives-mean-absolute-deviation-around-the-median}
::: {.lab-chapter .lab-middle}
:::

#### Медианное абсолютное отклонение {#andan-descriptives-median-absolute-deviation}
::: {.lab-chapter .lab-middle}
:::


### Дисперсия {#andan-descriptives-variance}
::: {.lab-chapter .lab-junior}
:::

#### Дисперсия генеральной совокупности {#andan-descriptives-population-variance}
::: {.lab-chapter .lab-junior}
:::

#### Дисперсия выборки {#andan-descriptives-sample-variance}
::: {.lab-chapter .lab-junior}
:::

### Стандартное отклонение {#andan-descriptives-standard-deviation}
::: {.lab-chapter .lab-junior}
:::


### Квантили {#andan-descriptives-quantiles}
::: {.lab-chapter .lab-junior}
:::


### Квартили {#andan-descriptives-quartiles}
::: {.lab-chapter .lab-junior}
:::


## Асимметрия {#andan-descriptives-skewness}
::: {.lab-chapter .lab-junior}
:::

## Эксцесс {#andan-descriptives-kurtosis}
::: {.lab-chapter .lab-junior}
:::


## Итоги {#andan-descriptives-final}

