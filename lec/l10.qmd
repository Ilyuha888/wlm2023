# L10 // Обобщенные линейные модели. Логистическая регрессия. Пуассоновская регрессия

{{< include ../other/_symbols.qmd >}}

```{r opts, echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE)
```

```{r pkgs, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
theme_set(theme_bw())
theme_update(legend.position = "bottom")
library(latex2exp)
```

## Ограничения общих линейных моделей

Модели, которые мы изучали на предыдущих занятиях носят название **общих линейных моделей (general linear models)**. Они достаточно просты и удобны в большинстве случаев, однако имеют существенное ограничений.

Вспомним, как выглядит уравнение такой модели:

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \varepsilon
$$

Предикторы в такой модели, как мы знаем, могут быть как дискретными, так и непрерывными. Однако ключевым допущением (и требованием) этой модели является распределение ошибки:

$$
\varepsilon \thicksim \norm (0, \sigma^2)
$$

Поскольку ошибка модели должна быть распределена нормально, а моделируется среднее значение, то можно сформулировать более общее допущение/требование:

$$
y \thicksim \norm (\mu, \sigma^2)
$$

Таким образом, общие линейные модели позволяют моделировать зависимости только для нормально-распределенных величин. Если же отклик модели (он же зависимая переменная) подчиняется другому распределению, эти модели не годятся.


## Идея обобщенных линейных моделей

Тем не менее, мы из своего опыта знаем, что существует много величин, распределение которых отличается от нормального. И дело даже не столько в асимметрии или эксцессе --- здесь разговор о самой природе величин. Так, например, есть величина «поступление в вуз», у которой только два возможных значения --- «поступил» и «не поступил». Или же, скажем, количество детей в семьей, варьирующееся в небольших пределах и принимающее только положительные целочисленые значения.

Нам хочется моделировать связь таких величин с предикторами так же, как мы делали в случае общих линейных моделей, или хотя бы в той же логике, которая нам хороша знакома из линейной регрессии. Для того, чтобы этого достичь, в модель вводится новый компонент --- **функция связи (link function)**.

* Пусть у нас есть некоторая переменная $y$, которая подчиняется какому-то закону распределения $f(y|\theta)$, где $\theta$ --- параметр(ы) распределения, и этот закон отличается от нормального.
* Мы всё так же хотим моделировать «среднее» значение, а точнее, *математическое ожидание*, переменной $\expect (y)$.
* Давайте возьмем некоторое преобразование (функцию) $g \big( \expect (y) \big) = \eta$, которое будет преобразовывать математическое ожидание нашей целевой переменной, *линеаризуя его*. Она и будет называться **функцией связи**.
* Теперь у нас есть линейная величина $\eta$, которую можно моделировать с помощью уже хорошо знакомой нам модели:

$$
\eta_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \varepsilon
$$

* Однако нас всё же интересует с точки зрения изученя закономерностей не $\eta$, а $\expect (y)$, но так как функция $g \big( \expect (y) \big)$ известна, мы возьмем *обратную функцию* $g^{-1} (\eta) = \expect(y)$ и получим интересующие нас значения.

Такова общая логика **обобщенных линейных моделей (generalized linear models, GLM)**. Какая именно функция связи (и, соответственно, обратная функция) будет использоваться, зависит от распределения целевой переменной. Далее мы рассмотрим два конкретных примера. Но уже сейчас мы можем написать общее уравнения для таких моделей:

$$
g \big( \expect(y_i) \big) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip} + \varepsilon_i
$$

Один из особых случаев возникает, когда к целевой переменной не применяется никакого преобразования --- то есть используется $g \big( \expect (y)) = \mu$ при $y \thicksim \norm (\mu, \sigma^2)$. Такая функция связи называется *функцией индентичности (identity function)*. В этом случае получается следующее:

$$
g \big( \expect(y_i) \big) = \mu_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip} + \varepsilon_i
$$

То есть мы получили обычную хорошо нам знакомую линейную регрессию. В этом смысле *обобщенные* линейные модели действительно *обобщают* случай обычной линейной регрессии на другие случай распределения целевой переменной.


## Биноминальная регрессия

Рассмотрение фреймворка обобщенных линейных моделей мы начнем с биномиальной, или логистической, регрессии, которая позволяет моделировать бинарные переменные.

### Бинарные переменные и биномиальное распределение

Бинарными называются переменные, которые могут принимать только два значения --- вообще, любых, но так как нам нужнв такие обозначения, которые сможет переварить математика, используем $0$ и $1$. Для нас это будут просто лейблы классов, так как мы, заменив обозначения, остались в рамках номинальной шкалы.

В теме про случайные величины мы вычисляли вероятность случайного прохождения теста из $n$ вопросов, каждый из которых мог быть решен правильно («успех», $1$) или неправильно («неудача», $0$), и описывали вероятность получить $k$ «успехов» в $n$ испытаниях как

$$
\prob (X = k) = C_n^k \, p^k \, q^{n-k}
$$

Эта формула и задает биномиальное распределение: $\prob(X = k) \thicksim \text{Bin}(n, p)$:

```{r}
tibble(x = 1:30,
       `10_0.2` = dbinom(x, 10, .2),
       `10_0.5` = dbinom(x, 10, .5),
       `30_0.2` = dbinom(x, 30, .2),
       `30_0.5` = dbinom(x, 30, .5)) %>% 
  pivot_longer(-x) %>% 
  separate(name, into = c("n", "p"), sep = "_") %>% 
  mutate(n = paste0("n = ", n),
         p = paste0("p = ", p)) %>% 
  ggplot(aes(x, value)) +
  geom_point() +
  facet_grid(n ~ p) +
  labs(x = "k", y = "P(X = k)")
```

От этого распределения и происходит название рассматриваемой регрессионной модели.


### Математическая модель

Тем не менее, хоть биномиальное распределения и существует, реализация $0$ и $1$ в отдельном наблюдении полчинается не ему[^bern], поэтому первым концептуальным шагом построения модели биномиальной регрессии является переход от моделирования $0$ и $1$ к моделированию вероятности получения $1$.

[^bern]: А [распределению Бернулли](https://mathworld.wolfram.com/BernoulliDistribution.html).

Рассмотрим картинку. Пусть мы пытаемся смоделировать связь между бинарной целевой переменной $Y$ и количественным предиктором $X$. Тогда её визуализация будет схематично выглядеть так:

<center>
<figure>
<img src="pics/bin_y.jpg">
</figure>
</center>

Тогда мы можем посчитать доли единиц в общем количестве исходов *при данном значении предиктора* $p_{Y=1|x_i}$ и использовать их как оценку вероятности $\prob (Y=1|x_i)$:

<center>
<figure>
<img src="pics/bin_prob.jpg">
</figure>
</center>

Отлично, мы получили непрерывную переменную! В принципе, можно попытаться смоделировать её с помощью линейной регрессии, однако возникнет некоторая проблема:

<center>
<figure>
<img src="pics/bin_prob_line.jpg">
</figure>
</center>

Переменная, которую мы получили, определяет *вероятность*, а значит она ограничена --- $0 \leq \prob (Y=1|x_i) \leq 1$. К тому же, чисто визуально заметно, что эта вероятность изменяется нелинейной, а по некоторой кривой --- примерно так:

<center>
<figure>
<img src="pics/bin_prob_curve.jpg">
</figure>
</center>

Супер... Теперь еще и искать кривую...

#### Логистическая кривая

К счаcтью, математики поработали за нас, и сообщили нам, что такая закономерность хорошо моделируется **логистической кривой (logistic curve)** --- отсюда второй название рассматриваемой нами модели (логистическая регрессия):

```{r}
tibble(x = seq(-6, 6, by = .01),
       y = exp(x) / (1 + exp(x))) %>% 
ggplot(aes(x, y)) +
  geom_line() +
  geom_hline(yintercept = c(0, 1), linetype = "dashed")
```

Эта прямая как раз зажата по $y$ между нулем и единицей, а её изгибы хорошо подходят под связь вероятности $\prob (Y=1|x_i)$ со значением предиктора $X$.

Её формула такова:

$$
y = \frac{e^x}{1 + e^x}
$$

Однако если мы хотим описывать зависимость вероятности от значения предиктора, то вместо $x$ нам необходимо подставить $\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots \beta_p x_{ip}$:

$$
\prob (Y=1|x_i) = p_i = \frac{e^{
\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots \beta_p x_{ip}}
}
{1 + e^{
\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots \beta_p x_{ip}}
}
$$

Выглядит, безусловно, страшно. Но наш путь еще не завершен...

#### Шансы и логиты

Мы победили дискретность целевой переменной $y$ и подобрали кривую, которая хорошо моделирует зависимость искомой вероятности от значений предиктора. Однако кривая всё ещё ограничена нулем и единицей, а значит смоделировать её м помощью линейной регрессии не получится.

Для того, чтобы победить ограниченность логистической кривой, используются шансы.

**Шанс (отношение шансов, odds, odds ratio)** --- это отношение вреоятности «успеха» ($1$) к вероятности «неудачи» ($0$). Эта величина хороша тем, что изменяется от $0$ до $+\infty$. Получается,

$$
\text{odds}_i = \frac{\prob(Y=1|x_i)}{1 - \prob(Y=1|x_i)} = \frac{p_i}{1 - p_i}
$$

Отлично, мы побороли ограниченность логистической кривой сверху --- движемся к успеху.


#### Logit-преобразование

Для того, чтобы победить ограниченность кривой снизу, возьмем логарифм от шанса. Получим следующее:

$$
\text{logit}(p_i) = \ln \lp \frac{p_i}{1 - p_i} \rp
$$

Такое преобразование вероятности разывается логит-преобразованием (logit-transformation). Значения логитов варьируются от $-\infty$ до $+\infty$, симметричны относительно нуля, и их удобно брать в каечестве целевой переменной для построения линейной модели. Кроме того, логит-преобразование еще и *линеаризует логистическую кривую*! Очень хорошее преобразование! Просто замечательное.

<details>
<summary>Линеаризация логистической кривой через logit-преобразование</summary>

озьмем случай в одним предиктором, чтобы было попроще. Логистическая кривая, моделирующая вероятность, имеет такой вид:

$$
p_i = \frac{e^{\beta_0 + \beta_1 x_{i1}}}
{1 + e^{\beta_0 + \beta_1 x_{i1}}}
$$

Обозначим $\beta_0 + \beta_1 x_{i1} = t$. Тогда необходимо показать, что логит-преобразование

$$
\text{logit} (p_i) = \ln \lp \frac{p_i}{1 - p_i} \rp
$$

делает логистическую функцию линейной, то есть обычной прямой. Иначе говоря, необходимо показать, что 

$$
\ln \lp \frac{p_i}{1 - p_i} \rp = t_i
$$

Доказывается это через расписывание формулы и раскрытие всех скобок и логарифмов (индекс $i$ опущен для упрощения записи):

$$
\begin{split}
\ln \lp \frac{p}{1-p} \rp &= \\
&= \ln \lp \frac{\frac{e^t}{1 + e^t}}{1 - \frac{e^t}{1 + e^t}} \rp = \\
&= \ln \lp \frac{e^t}{1 + e^t} \rp - \ln \lp 1 - \frac{e^t}{1 + e^t} \rp = \\
&= \ln \lp \frac{e^t}{1 + e^t} \rp - \ln \lp \frac{1 + e^t - e^t}{1 + e^t} \rp = \\ 
&= \ln \lp \frac{e^t}{1 + e^t} \rp - \ln \lp \frac{1}{1 + e^t} \rp = \\
&= \ln (e^t) - \ln (1 + e^t) - \big(\ln (1) - \ln (1+e^t)\big) = \\
&= \ln (e^t) - \ln (1) = \\
&= \ln (e^t) = t
\end{split}
$$

***

</details>

:::{.callout-warning}

#### Cаммари того, что происходило выше:

- От дискретной оценки событий (0 и 1) переходим к оценке вероятностей.
- Связь вероятностей с предиктором описывается логистической кривой.
- Если при помощи функции связи перейти от вероятностей к логитам, то связь будет описываться прямой линией.
- Параметры линейной модели для такой прямой можно оценить с помощью регрессионного анализа.

:::

В итоге мы получаем, что математическая модель логистической регрессии выглядит так:

$$
p_i = \frac{e^{\beta_0 + \beta_1 x_{i1}}}
{1 + e^{\beta_0 + \beta_1 x_{i1}}}
$$

Функция связи --- логит:

$$
\text{logit} (p_i) = \ln \lp \frac{p_i}{1 - p_i} \rp = \eta_i
$$

$$
\eta_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}
$$

А это ровно то, что нам было нужно!

Для перехода от логитов к вероятностям используется обратная функция вида

$$
p_i = \frac{e^{\eta_i}}{1 + e^{\eta_i}}
$$


### Идентификация модели

#### Метод максимального правдоподобия

### Тестирование качества модели

#### Девианса

#### Анализ девиансы

#### Информационные критерии

#### Проверка на сверхдисперсию

### Тестирование значимости предикторов

#### Тесты Вальда

#### Тесты отношения правдоподобий

### Интерпретация коэффициентов модели

### Предсказательная сила модели

#### Confusion mattrix

##### Accuracy

##### Precision

##### Recall

##### F1-мера

#### ROC-AUC


## Пуассоновская регрессия

### Счетные данные. Распределение Пуассона

### Почему общие линейные модели плохо работают?

### Математическая модель

### Тестирование качества модели

### Проверка на сверхдисперсию

### Квазипуассоновские модели
