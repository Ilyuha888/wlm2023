# L8 // Общие линейные модели. Простая и множественная линейная регрессия

{{< include ../book/_symbols.qmd >}}

```{r opts, echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE)
```

```{r andan-desc-pkgs, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
theme_set(theme_bw())
library(latex2exp)
```

## Простая линейная регрессия

### Ограничения корреляционного анализа

* Корреляционный анализ **позволяет** изучить линейную взаимосвязь между переменными, оценить её силу и направление, протестировать гипотезу о статистической значимости взаимосвязи.
* Корреляционный анализ **не позволяет** предсказывать значения одной переменной на основе значений другой. Изучения связей между несколькими переменными в корреляционном анализе также не очень удобно.

Необходимо построение некоторой модели.

### Идея регрессионной модели

Когда мы строили диаграммы рассеяния, мы добавляли на них *линию тренда*, которая отражала линейную составляющую связи между визуализируемыми переменными.

```{r}
set.seed(123)

tibble(x = rnorm(100),
       y = 3.2 * x + 1 + rnorm(100)) -> pics_data 

pics_data%>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

Визуально мы такую прямую проведём очень легко, а вот как мы нам получить её математическое выражение?

Первое, что нам нужно вспомнить --- это общее уравнение прямой. Оно выглядит так:

$$
y = kx + b,
$$

где $k$ --- угловой коэффициент (slope), задающий угол наклона прямой к оси $x$, а $b$ --- свободный член (intercept), который обозначает ординату точки пересечения прямой с осью $y$.

<center>
<figure>
<img src="pics/line.jpg">
</figure>
</center>

Таким образом, чтобы получить уравнение прямой, нам надо знать два этих числа.


### Формализация модели

Мы привыкли к тому, что неизвестными являются $x$ и $y$, но теперь, когда мы ищем уравнение прямой на основе имеющихся наблюдений, ситуация изменяется. Запишем уравнение прямой, используя общепринятые обозначения:

$$
y = b_0 + b_1 x
$$

Уравнение отражает зависимость между переменными $x$ и $y$, значения которых нам известны, так как у нас есть результаты измерений, а вот неизвестными теперь являются $b_0$ и $b_1$. 

В терминах статистической модели:

* переменная $y$ называется *зависимая*, *предсказываемая*, *целевая* переменная или *регрессант*
* переменная $x$ носит названия *независимая* переменная, *предиктор* или *регрессор*
* числа $b_0$ и $b_1$ называются *коэффициентами* или *параметрами* модели

::: {.callout-note}

#### Зависимые и независимые переменные

Несмотря на использование терминов зависимая и независимая переменные, необходимо чётко понимать, что сам регрессионный анализ, как и корреляционный, ничего нам не говорит о причинности. Мы выражаем $y$ через $x$, но точно так же можем выразить и $x$ через $y$ --- и модель будет подобрана, так как нет никаких математических ограничений. Поэтому если мы хотим сделать по результатам регрессионного анализа вывод о причинно-следственной связи между явлениями, нам необходимо либо серьёзное теоретическое обоснование нашего вывода --- почему мы выбрали в качестве зависимой и независимой переменных именно эти? --- либо использование экспериментельного дизайна исследования, где мы обосновываем причинно-следственный характер связи именно через дизайн эксперимента.

:::

Однако здесь необходимо еще несколько уточнений. Закономерность, которую мы будем моделировать, корректнее записать в следующем виде:

$$
y = \beta_0 + \beta_1x,
$$

где $\beta_0$ и $\beta_1$ --- параметры генеральной совокупности.

Кроме того, для каждого отдельного объекта генеральной совокупности значение целевой переменной $y_i$ будет также зависеть и он случайных факторов, которые не учитываются параметрами $\beta_0$ и $\beta_1$, то есть для конкретного объекта генеральной совокупности модель примет вид:

$$
y_i = \beta_0 + \beta_1 x + \varepsilon_i,
$$

где $\varepsilon_i$ --- случайная изменчивость целевой переменной.

На эту модель мы и будем опираться при оценке параметров $\beta_0$ и $beta_1$.


### Идентификация модели

Модель $y_i = \beta_0 + \beta_1 x + \varepsilon_i$ имеет место в генеральной совокупности, однако, как мы обсуждали много раз ранее, мы всегда работаем с выборкой, поэтому для выборки мы запишем модель в следующем виде:

$$
y_i = b_0 + b_1 x_i + e_i,
$$

где $b_0 = \hat \beta_0$ и $b_1 = \hat \beta_1$ --- оценка параметров генеральной совокупности, $e_i$ --- ошибки (или остатки, residuals) модели.

Идентификация регрессионной модели сводится к нахождению коэффициентов $b_0$ и $b_1$. Мы хотим провести такую прямую, которая наилучшим образом будет описывать имеющуюся в данных закономерность, поэтому необходимо найти метрику, по которому мы будем определять «хорошесть» нашей прямой.

Графически мы делаем вот что: проводим прямую через облако точек. Очевидно, что красная прямая описывает закономерность совсем плохо, зелёная --- чуть получше, а синяя --- то, что нам нужно.

```{r}
pics_data%>% 
  ggplot() +
  geom_point(aes(x, y)) +
  geom_smooth(aes(x, y),
              method = "lm", se = FALSE, size = 1) +
  geom_abline(intercept = 0,
              slope = 1.5,
              color = "green3",
              size = 1) +
  geom_hline(aes(yintercept = mean(x)),
             size = 1, color = "red3")
```

Из картинки также видно, что даже синяя прамая не описывает наши данные максимально точно --- не все точки попали на прямую. Ясно, что идеальную прямую мы провести и не сможем --- точек же целое облако. Поэтому любая построенная нами модель будет содержать ошибку --- те самые $e_i$ --- вновь по причине вариативности и неопределенности данных.

Уравнение подбираемой нами прямой --- синяя на рисунке выше --- запишем в следующем виде:

$$
\hat y_i = b_0 + b_1 x_i,
$$

где $\hat y_i$ --- модельное значение целевой переменной, то есть то, что лежит на построенной нами прямой для конкретного значения регрессора $x_i$.

Тогда мы сможем отобразить ошибки модели на графике:

```{r}
model1 <- lm(y ~ x, pics_data)
pics_data %>% filter(x < -1 & y > 0) -> point1
pics_data %>% filter(x < 1 & x > 0.8 & y < 2.5) -> point2
point1 %>% mutate(y_ = predict(model1, point1)) -> point1
point2 %>% mutate(y_ = predict(model1, point2)) -> point2

pics_data %>% 
  ggplot(aes(x, y)) +
  geom_segment(x = point1$x, y = point1$y, xend = point1$x, yend = point1$y_,
               color = "red3", size = 1) +
  geom_segment(x = point2$x, y = point2$y, xend = point2$x, yend = point2$y_,
               color = "red3", size = 1) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  annotate(geom = "text", label = TeX("$e_i$"), 
           x = -1.1, y = 0, color = "red3") +
  annotate(geom = "text", label = TeX("$e_i$"), 
           x = 0.9, y = 2.5, color = "red3")
```

Мы заинтересованы в том, чтобы наша модель ошибалась как можно меньше, то есть сумма ошибок $e_i$ была минимальна. Получается, нам надо подобрать такие параметры $b_0$ и $b_1$, при которых сумма ошибок модели будет наименьшей. Математически это можно записать так:

$$
Q_{\text{res}} = \sum_{i=1}^n e_i^2 \to \min_{b_0, b_1}
$$

Обратите внимание, что минимизируется *сумма квадратов ошибок*, так как отдельные ошибки могут быть как положительными, так и отрицательными --- что и отображено на графике выше ---- в силу чего сумма ошибок будет равна нулю. Ситуация аналогичная расчету дисперсии, где мы возводили отклонения в квадрат.

Если мы распишем, как определяется ошибка модели, то получится следующее:

$$
Q_{\text{res}} = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i - \hat y_i)^2 = \sum_{i=1}^n \big( y_i - (b_0 + b_1 x_i) \big)^2
$$

Выходит, что $Q_{\text{res}}$ является функцией, зависящей от $b_0$ и $b_1$, что можно обозначить как $f(b_0, b_1)$:

$$
Q_{\text{res}} = f(b_0, b_1) = \sum (y_i - \hat y_i)^2 = \sum (y_i - b_0 - b_1x_i)^2 \to \min_{b_0, b_1}
$$

В итоге задача идентификации модели линейной регрессии сводится к нахождению минимума функции $Q_{\text{res}} = f(b_0, b_1)$. Этим занимается метод наименьшиъ квадратов.


#### Метод наименьших квадратов

Метод наименьших квадратов работает следующим образом. Как уже отмечено выше, условие минимизации ошибки модели представляет собой функцию двух аргументов:

$$
f(b_0, b_1) = \sum (y_i - b_0 - b_1x_i)^2
$$

Это квадратичная функция, и чтобы нам дальше удобнее было с ней работать, раскроем скобки:

$$
f(b_0, b_1) = \sum (y_i - b_0 - b_1x_i) (y_i - b_0 - b_1x_i)
$$

$$
f(b_0, b_1) = 
\sum (y_i^2 - b_0 y_i - b_1 x_i y_i - b_0 y_i - b_1 x_i y_i + b_0 b_1 x_i + b_1^2 x_i^2 + b_0^2 + b_0 b_1 x_i)
$$

$$
f(b_0, b_1) = 
\sum(y_i^2 - 2 b_1 x_i y_i - 2 y_i b_0 + x_i^2 b_1^2 + b_0^2 + 2 x_i b_1 b_0)
$$

Чтобы определить, при каких значения $b_0$ и $b_1$ функция будет принимать минимальное значение, нужно взять две частные производные этой функции по $b_0$ и $b_1$ и приравнять их к нулю.

Берём частные производные:

$$
\frac{f(b_0, b_1)}{\partial b_0} = \sum (-2y_i + 2b_0 + 2x_ib_1) = 
-2 \sum \big( y_i - (b_0 + b_1 x_i) \big)
$$

$$
\frac{f(b_0, b_1)}{\partial b_1} = \sum (-2 x_i y_i + 2 x_i^2 b_1 + 2 x_i b_0) = -2 \sum \big( y_i - (b_0 + b_1 x_i) \big) x_i
$$

Приравниваем производные к нулю и решаем систему уравнений:

$$
\cases {
-2 \sum \big( y_i - (b_0 + b_1 x_i) \big) = 0 \\
-2 \sum \big( y_i - (b_0 + b_1 x_i) \big) x_i = 0
}
$$

$$
\cases{
\sum \big( y_i - (b_0 + b_1 x_i) \big) = 0 \\
\sum \big( y_i - (b_0 + b_1 x_i) \big) x_i = 0
}
$$

$$
\cases{
\sum y_i - \sum b_0 + \sum b_1 x_i = 0 \\
\sum y_i x_i - \sum b_0 x_i + \sum b_1 x^2_i = 0
}
$$

$$
\cases{
\sum b_0 + \sum b_1 x_i = \sum y_i \\
\sum b_0 x_i + \sum b_1 x_i^2 = \sum y_i x_i
}
$$

$$
\cases{
b1 \sum x_i + n b_0 = \sum y_i \\
b1 \sum x^2_i + b_0 \sum x_i = \sum y_i x_i
}
$$

$$
b_0 = \frac{\sum y_i}{n} - b_1 \frac{\sum x_i}{n} = \bar y - b_1 \bar x
$$

$$
b1 \sum x_i^2 + (\bar y - b_1 \bar x) \sum x_i = \sum x_i y_i
$$

$$
\underline{b_1 \sum x_i^2} + \bar y \sum x_i - \underline{b_1 \bar x \sum x_i} = \sum x_i y_i
$$

$$
b_1 \Big( \sum x_i^2 - \bar x \sum x_i \Big) = 
\sum x_i y_i - \bar y \sum x_i
$$

$$
b_1 = \frac{\sum x_i y_i - \bar y \sum x_i}{\sum x_i^2 - \bar x \sum x_i} = 
\frac{(\sum x_i y_i - \bar y \sum x_i) \times n}{(\sum x_i^2 - \bar x \sum x_i) \times n}
$$

$$
b_1 = \frac{\overline{xy} - \bar x \cdot \bar y}{\overline{x^2} - \bar x^2} = 
\frac{\overline{xy} - \bar x \bar y}{s_X^2}
$$

В сухом остатке из метода наименьших квадратов нам надо вынести две идеи:

- _задача идентификации модели линейной регрессии имеет аналитическое решение_ --- то есть мы можем подобрать коэффициенты модели, опираясь только на имеющиеся данные
- это аналитическое решение имеет следующий вид:

$$
\cases{
b_0 = \bar y - b_1 \bar x \\
b_1 = \frac{\overline{xy} - \bar x \cdot \bar y}{\overline{x^2} - \bar x^2} = 
\frac{\overline{xy} - \bar x \bar y}{s_X^2}
}
$$


#### Матричное вычисление коэффициентов

Частные производные это, конечно, хорошо, однако можно вычислить коэффициенты и проще через матрицы. Имеющуюся у нас модель мы модем записать следующим образом: пусть у нас есть $n$ наблюдений, каждое из которых описывается моделью $y_i = b_0 + b_1 x_i + e_i$. Тогда мы можем записать следующую систему:

$$
\cases{
b_0 + b_1 x_1 + e_1 = y_1 \\
b_0 + b_1 x_2 + e_2 = y_2 \\
\dots \\
b_0 + b_1 x_n + e_n = y_n \\
}
$$

Эту систему мы можем переписать в матричном виде:

$$
\vm X \vm b + \vm e = \vm y,
$$

где $\vm y$ --- вектор нашей целевой переменной, $\vm X$ --- матрица предикторов, $\vm b$ --- вектор коэффициентов модели, $\vm e$ --- вектор ошибок (остатков) модели.

Может возникнуть резонный вопрос: «почему $\vm X$ матрица, ведь у нас только одна независимая переменная?». Так как вектор коэффициентов модели $\vm b$ содержит два элемента $b_0$ и $b_1$, то для удобства вычислений к вектору значений предиктора добавляют вектор, состоящий из единиц, который будет отвечать за интерсепт нашей модели --- в результате получается матрица $\vm X$, которая имеет следующий вид:

$$
\vm X = \pmatrix{1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n}
$$

Опуская детали, сразу укажем матричное решение для коэффициентов модели:

$$
\vm b = (\vm X^\T \vm X)^{-1} \vm X^\T \vm y
$$

Отметим важную деталь из полученного решения: в ходе вычисления коэффициентов мы берём обратную матрицу от матрицы $\vm X^\T \vm X$. Этот факт нам пригодится в следующем разделе.


### Тестирование качества модели

#### F-статистика

#### Коэффициент детерминации

#### Метрики качества модели

##### MSE

##### MAE

##### RMSE

##### MAPE

### Тестирование значимости предикторов

### Допущения линейной регрессии

* Линейность связи
* Независимость наблюдений
* Нормальное распределение остатков
* Гомоскедастичность остатков
* Влиятельные наблюдения и выбросы

### Предсказания на основе модели

* доверительная зона регрессии для предсказаний
* интерполяция и экстраполяция




## Множественная линейная регрессия

### Ограничения простой линейной регрессии

### Множественная линейная регрессия с количественными предикторами без взаимодействия

### Множественная линейная регрессия с количественными и категориальными предикторами без взаимодействия

### Множественная линейная регрессия с количественными и категориальными предикторами со взаимодействием

### Множественная линейная регрессия со взаимодействием количественных предикторов

### Сравнение моделей

### Допущения линейной регрессии

### Проблема мультиколлинеарности









