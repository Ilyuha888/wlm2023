[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "L // WLM 2023",
    "section": "",
    "text": "Вступление\nКнига содержит конспекты лекция курса WLM 2023.",
    "crumbs": [
      "Вступление"
    ]
  },
  {
    "objectID": "l1.html#установка-r-и-rstudio",
    "href": "l1.html#установка-r-и-rstudio",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.1 Установка R и RStudio",
    "text": "1.1 Установка R и RStudio\nЧтобы стать счастливым пользователем R, надо установить на свой комп две программы:\n\nсобственно R\n\nна Win\nна Mac\nна Linux\n\nIDE RStudio1\n\nПричем во избежание возможных проблем, надо поставить программы именно в этом порядке — сначала R, а потом RStudio, иначе IDE может на найти R и будет ругаться.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#r-как-язык-программирования.-команды",
    "href": "l1.html#r-как-язык-программирования.-команды",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.2 R как язык программирования. Команды",
    "text": "1.2 R как язык программирования. Команды\nВ R нет команд и практически нет ключевых слов, распространенных в других языках программирования. Практически все операции обернуты в функции. Остаются только математические и логические операторы, которые, на самом деле, тоже функции.\n\n1.2.1 Математические операции\nВсе в наличии:\n\n2 + 3 # сложение\n\n[1] 5\n\n4 - 1 # вычитание\n\n[1] 3\n\n5 * 12 # умножение\n\n[1] 60\n\n5 ^ 8 # возведение в степень\n\n[1] 390625\n\n4 / 7 # деление\n\n[1] 0.5714286\n\n5 %/% 3 # целочисленное деление\n\n[1] 1\n\n5 %% 3 # остаток от деления\n\n[1] 2\n\n\nСкобки также существуют и привычно работают:\n\n6 / 3 + 2 * 4\n\n[1] 10\n\n6 / (3 + 2) * 4\n\n[1] 4.8\n\n6 / ((3 + 2) * 4)\n\n[1] 0.3\n\n\n\n\n1.2.2 Математические функции\nМожно посчитать корень:\n\nsqrt(16)\n\n[1] 4\n\n\nИли логарифм:\n\nlog(10)\n\n[1] 2.302585\n\nlog(8, base = 2)\n\n[1] 3\n\nlog(8, 2)\n\n[1] 3\n\n\nИли что-то на тригонометрическом:\n\nsin(5); cos(5); tan(5)\n\n[1] -0.9589243\n\n\n[1] 0.2836622\n\n\n[1] -3.380515\n\n\nКстати, можно и вот так — это к тому, что математические операторы тоже являются функциями:\n\n`+`(2, 3)\n\n[1] 5\n\n`^`(4, 5)\n\n[1] 1024\n\n`/`(8, 3)\n\n[1] 2.666667\n\n\n\n\n1.2.3 Логические операции\nК логическим операциями можно отнести операции сравнения:\n\n5 &gt; 4 # больше\n\n[1] TRUE\n\n6 &lt; 2 # меньше\n\n[1] FALSE\n\n5 &gt;= 5 # больше или равно\n\n[1] TRUE\n\n6 &lt;= 3 # меньше или равно\n\n[1] FALSE\n\n23 == 14 # равно\n\n[1] FALSE\n\n77 != 98 # не равно\n\n[1] TRUE\n\n\nА также логические операторы И (&) и ИЛИ (|):\n\nTRUE & TRUE\n\n[1] TRUE\n\nTRUE & FALSE\n\n[1] FALSE\n\nFALSE & FALSE\n\n[1] FALSE\n\nTRUE | TRUE\n\n[1] TRUE\n\nTRUE | FALSE\n\n[1] TRUE\n\nFALSE | FALSE\n\n[1] FALSE\n\n\n\n\n1.2.4 Переменные и объекты\nРезультаты вычислений и преобразований хотелось бы сохранять, поэтому в R существует оператор присваивания &lt;-:\n\nx &lt;- 5\ny &lt;- 4 * 8\n\nМожно, конечно, написать и x = 5, но сообщество вас не поймет и будет косо смотреть… Когда мы присвоим некоторой переменной какой-либо объект, он отобразиться в окошке Environment, и с ним можно будет работать. Например, совершать разные операции:\n\nx + y\n\n[1] 37\n\nsqrt(x)\n\n[1] 2.236068\n\nlog(y, base = x)\n\n[1] 2.153383\n\n\nОбъектом в R может быть вообще все, что угодно — число, строка, вектор, матрица, датафрейм, таблица, результат моделирования, функция и т.д.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#типы-данных",
    "href": "l1.html#типы-данных",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.3 Типы данных",
    "text": "1.3 Типы данных\nТип данных — это характеристика данных, которая определяет:\n\nмножество допустимых значений, которые могут принимать данные этого типа\nдопустимые операции над данными этого типа\n\n\n1.3.1 numeric\nЭто числа с десятичной частью.\n\nclass(3.14)\n\n[1] \"numeric\"\n\ntypeof(3.14)\n\n[1] \"double\"\n\n\n\"double\" нам говорит о том, что числа с десятичной частью храняться в R с двойной точностью. И это хорошо.\n\n\n1.3.2 integer\nЭто целые числа.\n\nclass(3)\n\n[1] \"numeric\"\n\n\nПравда чтобы создать именно целое число, надо указать, что мы хотим именно целое число с помощью литерала L:\n\nclass(3L)\n\n[1] \"integer\"\n\ntypeof(3L)\n\n[1] \"integer\"\n\n\nПо умолчанию объект типа 3 воспринимается R как 3.0, поэтому тип данных будет numeric.\n\n\n1.3.3 complex\nКомплексные числа тоже существуют, и мы с ними немного познакомимся, чтобы перестать их бояться.\n\nclass(2+3i)\n\n[1] \"complex\"\n\n\n\n\n1.3.4 character\nТекст тоже надо как-то хранить.\n\ns1 &lt;- 'a'\ns2 &lt;- \"это строка\"\n\nclass(s1)\n\n[1] \"character\"\n\nclass(s2)\n\n[1] \"character\"\n\n\nКавычки не важны, если у вас не встречаются кавычки внутри кавычек. Тогда надо использовать разные:\n\ns &lt;- 'Мужчина громко зашёл в комнату и высказал решительное \"здравствуйте\"'\ns\n\n[1] \"Мужчина громко зашёл в комнату и высказал решительное \\\"здравствуйте\\\"\"\n\n\n\n\n1.3.5 factor\nБывают такие переменные, которые группируют наши данные. Например,\n\nгород проживания (Москва, Санкт-Петербург, Казань, Екатеринбург)\nуровень образования (бакалавриат, специалитет, магистратура, аспирантура)\nэкспериментальная группа (group1, group2, control)\nи др.\n\nОбычно они текстовые. Для них был придуман тип данных factor, чтобы их было дешевле хранить. Однако большинство современных пакетов сами могут решить, когда надо текст перевести в фактор.\nOrdered factor (упорядоченный фактор) — тип данных, который позволяет задать порядок групп. Например,\n\nуровень образования: bachelor &lt; master &lt; phd &lt; postdoc\nсложность экспериментальной задачи: easy &lt; medium &lt; hard\nи др.\n\nВот упорядоченный фактор нам время от времени может понадобиться — например, для создания визуалиация или в некоторых статистических моделях.\n\n\n1.3.6 Специальные литералы\n\n1.3.6.1 NA\nПропущенное значение (Not Available). Обозначает отсутствие значения там, где оно вроде бы должно быть. Причины могут быть разные:\n\nтехнические ошибки записи данных\nошибки настройки платформы — забыли сделать ответы обязательными\nорганизация исследования — ограничили время на ответ\n«честный» пропуск — дали возможность не отвечать на вопрос\nпредобработка данных — специально создали NA, чтобы далее с ними работать\nи др.\n\n\n\n1.3.6.2 NaN\nЭто не число (Not a Number).\n\n0 / 0\n\n[1] NaN\n\n\n\n\n1.3.6.3 NULL\nЭто ничто. Пустота. Используется для задания аргументов функций.\n\nggplot(data = NULL)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#структуры-данных",
    "href": "l1.html#структуры-данных",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.4 Структуры данных",
    "text": "1.4 Структуры данных\nСтруктура данных — это способ и форма объединения однотипных и/или логически связанных данных.\n\n\n\n\nПример данных\n\n\n\n\n1.4.1 Датафрейм\nВоплощение привычной нам «таблицы» в R.\n\n\n# A tibble: 6 × 10\n  carat cut       color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\n\nЭто сложная структура данных. Чтобы понять всю её мощь, необходимо начать с более простых.\n\n\n1.4.2 Векторы\nВектор — это набор чисел.\n\\[\n\\pmatrix{1 & 4 & 36 & -8 & 90.1 & -14.5}\n\\]\nЕсли это утверждение вызывает у вас внутренний протест, давай проследим пусть от направленного отрезка в набору чисел.\nВозьмем направленный отрезок — вектор:\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\n\n\n\n\nИменно так мы понимали вектор в школе. Договоримся, что все векторы у нас начинаются из точки \\((0, 0)\\):\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\n\n\n\n\nЕсли теперь у нас все вектора начинаются из начал координат, то мы можем полностью описать вектор только координатами его конца. Поэтому уберем вектор:\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\n\n\n\n\nТо есть для нас теперь вектор равносилен точке на плоскости. А точка однозначно описывается двумя координатами. Получается, можно просто записать:\n\\[\n\\pmatrix{1 & 2}\n\\]\nПолучается, что это одно и то же:\n\\[\n\\pmatrix{1 & 0.5}, \\quad \\pmatrix{2 & 3}, \\quad \\pmatrix{4.2 & -3.5}\n\\]\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\n\n\n\n\nТеперь обобщим вектор на более общие случаи:\n\nВектор — это набор некоторого колчиества элементов одного типа.\n\n\nv_num &lt;- c(1, 6, -34, 7.7) # числовой вектор\nv_char &lt;- c(\"Москва\", \"Санкт-Петербург\", \"Нижний Новгород\", \"Пермь\") # текстовый вектор\nv_log &lt;- c(TRUE, FALSE, TRUE, TRUE) # логический вектор\n\n\nclass(v_num)\n\n[1] \"numeric\"\n\nv_num\n\n[1]   1.0   6.0 -34.0   7.7\n\nclass(v_char)\n\n[1] \"character\"\n\nv_char\n\n[1] \"Москва\"          \"Санкт-Петербург\" \"Нижний Новгород\" \"Пермь\"          \n\nclass(v_log)\n\n[1] \"logical\"\n\nv_log\n\n[1]  TRUE FALSE  TRUE  TRUE\n\n\n\n1.4.2.1 Индексация векторов\nИз вектора можно вытащить его элемент:\n\nv_char[2] # по номеру\n\n[1] \"Санкт-Петербург\"\n\nv_num[v_num &gt; 5] # по условию\n\n[1] 6.0 7.7\n\n\n\n\n1.4.2.2 Векторизация\nДля того, чтобы выполнить операцию на всем векторе поэлементно, не нужно перебирать его элементы.\n\nvec &lt;- 1:4\nvec - 1\n\n[1] 0 1 2 3\n\nvec^2\n\n[1]  1  4  9 16\n\nsqrt(vec)\n\n[1] 1.000000 1.414214 1.732051 2.000000\n\n\n\n\n1.4.2.3 Recycling\nЕсли мы будем, например, складывать два вектора разной длины, то более короткий зациклится.\n\nvec1 &lt;- 1:10\nvec2 &lt;- 1:2\n\nvec1\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nvec2\n\n[1] 1 2\n\nvec1 + vec2\n\n [1]  2  4  4  6  6  8  8 10 10 12",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#матрицы",
    "href": "l1.html#матрицы",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.5 Матрицы",
    "text": "1.5 Матрицы\nЕсли мы желаем приблизиться к датафрейму, то одного ряда элементов нам недостаточно — надо выходить во второе измерение! Поэтому уложим вектор в матрицу:\n\n\n\n\nВарианты преобразования вектора в матрицу\n\n\n\nИли вот еще разные варианты:\n\nv &lt;- 1:12\nm1 &lt;- matrix(v, nrow = 3)\nm1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nm2 &lt;- matrix(v, nrow = 4)\nm2\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\nm3 &lt;- matrix(v, nrow = 3, byrow = TRUE)\nm3\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\nm4 &lt;- matrix(v, nrow = 4, byrow = TRUE)\nm4\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n[4,]   10   11   12\n\n\n\n1.5.1 Индексация матриц\nИз матрицы можно вытащить её элементы:\n\nm1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nm1[2, 3] # отдельный элемент\n\n[1] 8\n\nm1[1, ] # целую строку\n\n[1]  1  4  7 10\n\nm1[, 4] # целый столбец\n\n[1] 10 11 12\n\nm1[1:2, 2:4] # часть матрицы\n\n     [,1] [,2] [,3]\n[1,]    4    7   10\n[2,]    5    8   11",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#массивы",
    "href": "l1.html#массивы",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.6 Массивы",
    "text": "1.6 Массивы\n\nВектор — одномерный массив.\nМатрица — двумерный массив.\nМассивы — структуры, которые объединяют данные только одного типа.\n\n\nc(2, TRUE)\n\n[1] 2 1\n\nc(2, TRUE, \"word\")\n\n[1] \"2\"    \"TRUE\" \"word\"\n\n\nПри объединении разных типов данных в одном массиве происходит приведение типов (coercion) по следующей иерархии:\n\nlogical → integer → numeric → complex → character\n\nЭто нам осложняет жизнь, так как мы бы хотели объединять данные разных типов в одну структуру.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#списки",
    "href": "l1.html#списки",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.7 Списки",
    "text": "1.7 Списки\nСписки позволяют объединять массивы различных типов данных, чем делают нашу жизнь значительно приятнее.\n\n\n\n\nСхема внутренней структуры списка\n\n\n\nНапример, так:\n\nl &lt;- list(v1 = v_num,\n          v2 = v_char,\n          m1 = m1,\n          ls = list(v = v,\n                    m = m3))\nl\n\n$v1\n[1]   1.0   6.0 -34.0   7.7\n\n$v2\n[1] \"Москва\"          \"Санкт-Петербург\" \"Нижний Новгород\" \"Пермь\"          \n\n$m1\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n$ls\n$ls$v\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\n$ls$m\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\n\n\n1.7.1 Индексация списков\n\nl[1] # по номеру элемента, возвращается список\n\n$v1\n[1]   1.0   6.0 -34.0   7.7\n\nl[[1]] # по номеру элемента, возвращается массив\n\n[1]   1.0   6.0 -34.0   7.7\n\nl$ls # по названию элемента\n\n$v\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\n$m\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\nl$ls$m # можно идти многоуровнево\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#собираем-датафрейм",
    "href": "l1.html#собираем-датафрейм",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.8 Собираем датафрейм",
    "text": "1.8 Собираем датафрейм\n\nвозьмем список\nпотребуем, чтобы его элементами были векторы\nпотребуем, чтобы эти векторы были одинаковой длины\nрасположим их «вертикально»\n\n\n\n\n\nСтруктура списка и датафрейма\n\n\n\n\n1.8.1 Индексация датафрейма\nДля примера возьмем датафрейм про бриллианты:\n\ndiam\n\n# A tibble: 6 × 10\n  carat cut       color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\n\nДатафрейм наследует свойства списка и матрицы, что делает его невероятно гибким в обращении и крайне удобным в работе:\n\ndiam$carat # вытащить столбец\n\n[1] 0.23 0.21 0.23 0.29 0.31 0.24\n\ndiam[diam$price &gt; 330, ] # отобрать строки по условию\n\n# A tibble: 3 × 10\n  carat cut       color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n2  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n3  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\ndiam[, c(2:3, 7)] # вытащить столбцы по номерам\n\n# A tibble: 6 × 3\n  cut       color price\n  &lt;ord&gt;     &lt;ord&gt; &lt;int&gt;\n1 Ideal     E       326\n2 Premium   E       326\n3 Good      E       327\n4 Premium   I       334\n5 Good      J       335\n6 Very Good J       336\n\ndiam[1:4, c(\"carat\", \"price\")] # вытащить отдельные строки по номерам и столбцы по названиям\n\n# A tibble: 4 × 2\n  carat price\n  &lt;dbl&gt; &lt;int&gt;\n1  0.23   326\n2  0.21   326\n3  0.23   327\n4  0.29   334",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#функции",
    "href": "l1.html#функции",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.9 Функции",
    "text": "1.9 Функции\n\nЕсли какой-либо кусок кода повторяется более трех раз, имеет смысл обернуть его в функцию.\n\nКак стоит понимать функцию?\nФункция — это некий черный ящик, который\n\nпринимает что-либо на вход\nпроделывает с этим какие-либо операции\nи что-то возвращает\n\n\n1.9.1 Синтаксис функции\nСинтаксис создания функции выглядит так:\n\nfunction_name &lt;- function(arguments) {\n    ...\n    body\n    ...\n    return()\n}\n\nЭлементы функции:\n\nимя функции (function_name) — как мы к ней будем обращаться при вызове\nаргументы функции (arguments) — какие значения и объекты она принимает на вход\nтело функции (body) — что она делает с входными объектами\nвозвращаемое значение (return()) — что функция вернет в качестве результата работы\n\nВызов функции:\n\nfunction_name(arguments)\n\n\n\n1.9.2 Пример функции\n\ncot &lt;- function(x) {\n  result &lt;- 1 / tan(x)\n  return(result)\n}\ncot(3)\n\n[1] -7.015253\n\n\nЕсли функция простая, можно не создавать временные объекты:\n\ncot &lt;- function(x) {\n  return(1 / tan(x))\n}\ncot(3)\n\n[1] -7.015253\n\n\nЕсли функция короткая, можно даже не писать return():\n\ncot &lt;- function(x) {\n  1 / tan(x)\n}\ncot(3)\n\n[1] -7.015253\n\n\n\n\n1.9.3 Пример более полезной функции\nОсторожно, большое!\nЭто функция, которая занималась предобработкой данных в реальном проекте. Прикиньте, если бы мы такой кусок кода повторяли для каждого датасета… — жуть!\n\n\n\n\n\n\nВажно!\n\n\n\nВам не нужно сейчас подробно понимать, что написано ниже — мы все разберем по ходу курса и научимся писать такое же! Сейчас главное ухватить структуру функции — где аргументы, где тело, где возвращаемое значение. Всё! Остальное освоим по ходу дела.\n\n\n\nmr_preproc &lt;- function(d) {\n\n  require(tidyverse)\n  \n  d |&gt; select(\n    # select columns we need\n    \"Индивидуальный_код\",\n    correctAns,\n    base_pic,\n    rotated_pic,\n    resp_MR_easy.keys,\n    resp_MR_easy.corr,\n    resp_MR_easy.rt\n  ) |&gt;\n    drop_na() |&gt; # remove technical NAs (recording artefacts, not missing data)\n    mutate(task = \"MR\",\n           # add task name (mental rotation)\n           level = \"easy\",\n           # add difficulty level\n           trial = 1:16) |&gt; # number trials\n    rename(\n      \"id\" = \"Индивидуальный_код\",\n      # rename columns for handy usage\n      \"key\" = resp_MR_easy.keys,\n      \"is_correct\" = resp_MR_easy.corr,\n      \"rt\" = resp_MR_easy.rt\n    ) -&gt; MR # ready to use\n  \n  return(MR)\n \n}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#условный-оператор",
    "href": "l1.html#условный-оператор",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.10 Условный оператор",
    "text": "1.10 Условный оператор\nИногда при написании функции может понадобиться обработать какие-то важные случаи.\n\nНапример, в двух запусках сбора данных столбцы были названы по-разному: если это не учесть, код будет ломаться.\n\nДля этого подойдет условный оператор.\n\n1.10.1 Структура условного оператора\n\nif (condition) {\n  ...\n  body\n  ...\n} else {\n  ...\n  body\n  ...\n}\n\n\n\n1.10.2 Пример функции с условным оператором\nДопустим, нам жизненно неободима функция, которая будет определять, является число четным или нечетным, потому что сами мы постоянно путаемся…\nВот она:\n\nodd_even &lt;- function(x) { # функция принимает на вход число\n  \n  if (x %% 2 == 0) { # проверяет, равняется ли нулю остаток от деления числа на два\n    \n    return(\"even\") # возвращает \"even\", если равняется\n    \n  } else {\n    \n    return(\"odd\") # возвращает \"odd\", если нет\n    \n  }\n  \n}\n\n\nodd_even(2)\n\n[1] \"even\"\n\nodd_even(34)\n\n[1] \"even\"\n\nodd_even(11)\n\n[1] \"odd\"\n\nodd_even(135)\n\n[1] \"odd\"\n\n\nРаботает!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#пример-функции-из-реального-проекта-с-условным-оператором",
    "href": "l1.html#пример-функции-из-реального-проекта-с-условным-оператором",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.11 Пример функции из реального проекта с условным оператором",
    "text": "1.11 Пример функции из реального проекта с условным оператором\n\n\n\n\n\n\nВажно!\n\n\n\nВам не нужно сейчас подробно понимать, что написано ниже — мы все разберем по ходу курса и научимся писать такое же! Сейчас главное ухватить структуру условного оператора — где условие, что выполняется, если условие верно, что выполняется, если условие ложно. Всё! Остальное освоим по ходу дела.\n\n\n\nms_preproc &lt;- function(d) {\n  \n  require(tidyverse)\n  \n  # Since we our participants could fill the fields in any order, \n  # here is a function which allows us to count correct inputs \n  # our subjects made.\n  \n  if (\"mouse_MSe.time\" %in% colnames(d)) { \n    ### здесь начинается условный оператор, который проверяет, есть ли такая колонка\n    ### если колонка есть, то запускается код ниже\n    \n    d |&gt; select(\n      \"Индивидуальный_код\",\n      matches(\"^noun\"),\n      matches(\"resp\\\\d\\\\.text$\"),\n      \"mouse_MSe.time\"\n    ) |&gt;\n      filter_at(vars(paste0(\"noun\", 1:3)), all_vars(!is.na(.))) |&gt;\n      filter_at(vars(paste0(\"noun\", 4:7)), all_vars(is.na(.))) |&gt;\n      mutate(task = \"MS\",\n             level = \"easy\") |&gt;\n      rename(\n        \"resp1\" = resp1.text,\n        \"resp2\" = resp2.text,\n        \"resp3\" = resp3.text,\n        \"id\" = \"Индивидуальный_код\",\n        \"rt\" = \"mouse_MSe.time\"\n      ) |&gt;\n      select(-c(paste0(\"noun\", 4:7))) -&gt; MS\n    \n  } else {\n    ### а если колонки нет, то запускается этот код\n    \n    d |&gt; select(\"Индивидуальный_код\",\n                matches(\"^noun\"),\n                matches(\"resp\\\\d\\\\.text$\")) |&gt;\n      filter_at(vars(paste0(\"noun\", 1:3)), all_vars(!is.na(.))) |&gt;\n      filter_at(vars(paste0(\"noun\", 4:7)), all_vars(is.na(.))) |&gt;\n      mutate(task = \"MS\",\n             level = \"easy\",\n             rt = NA) |&gt;\n      rename(\n        \"resp1\" = resp1.text,\n        \"resp2\" = resp2.text,\n        \"resp3\" = resp3.text,\n        \"id\" = \"Индивидуальный_код\"\n      ) |&gt;\n      select(-c(paste0(\"noun\", 4:7))) -&gt; MS\n\n  }\n  \n  return(MS)\n  \n}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#вне-функций",
    "href": "l1.html#вне-функций",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.12 Вне функций",
    "text": "1.12 Вне функций\nВне функций условный оператор практически не используется, потому что для предобработки данных есть удобная функция ifelse().",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#циклы",
    "href": "l1.html#циклы",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.13 Циклы",
    "text": "1.13 Циклы\nТо, что мы написали функция, чтобы не дублировать код — это хорошо, однако эту функцию нам все равно придется запускать много раз, если нам надо этот кусок кода повторить.\nПоэтому используем цикл:\n\nfor (i in a:b) {\n  ...\n  body\n  ...\n}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#пример-простеньких-циклов",
    "href": "l1.html#пример-простеньких-циклов",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.14 Пример простеньких циклов",
    "text": "1.14 Пример простеньких циклов\nПросто печатаем числа от 1 до 10:\n\nfor (i in 1:10) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nНу, или более сложные выражения:\n\nfor (j in 1:10) {\n  print(sqrt(j) + j^2)\n}\n\n[1] 2\n[1] 5.414214\n[1] 10.73205\n[1] 18\n[1] 27.23607\n[1] 38.44949\n[1] 51.64575\n[1] 66.82843\n[1] 84\n[1] 103.1623",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#пример-цикла-для-чтения-и-предоработки-данных",
    "href": "l1.html#пример-цикла-для-чтения-и-предоработки-данных",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.15 Пример цикла для чтения и предоработки данных",
    "text": "1.15 Пример цикла для чтения и предоработки данных\n\n\n\n\n\n\nВажно!\n\n\n\nВам не нужно сейчас подробно понимать, что написано ниже — мы все разберем по ходу курса и научимся писать такое же! Сейчас главное ухватить структуру цикла — где итератор и где тело, которое будет выполняться несколько раз. Всё! Остальное освоим по ходу дела.\n\n\n\nfor (i in 1:length(files)) { ## будем двигаться от 1 до количества файлов в папке с данными\n  \n  print(files[i]) ## печатает имя файла, чтобы видеть на каком файле сломалось, если сломается\n  \n  d &lt;- read_csv(files[i], show_col_types = FALSE) ## считывает один файл из папки\n  \n  ## запускаем функции предобработки\n  MR_data |&gt; bind_rows(mr_preproc(d) |&gt; mutate(file = files[i])) -&gt; MR_data\n  ST_data |&gt; bind_rows(st_preproc(d) |&gt; mutate(file = files[i])) -&gt; ST_data\n  MS_data |&gt; bind_rows(ms_preproc(d) |&gt; mutate(file = files[i])) -&gt; MS_data\n  NASATLX_data |&gt; bind_rows(nasatlx_preproc(d) |&gt; mutate(file = files[i])) -&gt; NASATLX_data\n  SEQUENCE_data |&gt; bind_rows(sequence_preproc(d) |&gt; mutate(file = files[i])) -&gt; SEQUENCE_data\n  \n  ## завершили цикл, идем на следующую итерацию\n\n}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#циклы-в-r-это-зло-они-долго-работают",
    "href": "l1.html#циклы-в-r-это-зло-они-долго-работают",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.16 Циклы в R — это зло! Они долго работают!",
    "text": "1.16 Циклы в R — это зло! Они долго работают!\n\nДа, циклы работают не быстро — это правда. Но, с другой стороны, мы и не терабайты данных анализируем.\n\nДопустим, у нас 50 респондентов. Цикл, подобный тому, что на предыдущем слайде, отбработает секунды за 3. Даже чай не успеете заварить.\nБезусловно, есть более изящные и быстрые инструменты, и с ними мы познакомимся на предобработке данных. Но в целом, можно и циклом обойтись.\nКонечно, если у вас огромные датасеты и вы работаете с Big Data, то прогон цикла может значительно затянуться — в этом случае разумно сразу использовать другие инструменты.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l2.html#форматы-файлов-данных",
    "href": "l2.html#форматы-файлов-данных",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.1 Форматы файлов данных",
    "text": "2.1 Форматы файлов данных\n\n2.1.1 CSV\n\nТекстовый формат\nЗначения разделены запятыми (Comma-Separated Values)\n\nНо это не точно\n\n\nПрочитать данные этого формата можно функцией\n\nread.csv()\n\n\n\n2.1.2 TSV\n\nТекстовый формат\nЗначения разделены знаком табуляции (\\t, Tab-Separated Values)\n\nПрочитать данные этого формата можно функцией\n\nread.table()\n\n\n\n2.1.3 TXT\n\nТекстовый формат\nРазделитель может быть любой\n\nПрочитать данные этого формата можно функцией\n\nread.table()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#табличные-форматы",
    "href": "l2.html#табличные-форматы",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.2 Табличные форматы",
    "text": "2.2 Табличные форматы\n\nФайлы Excel — .xls, .xlsx\n\nДля их чтения понадобится пакет readxl, в котором есть функции\n\nreadxl::read_xls()\nreadxl::read_xlsx()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#форматы-файлов-данных-c-особой-разметкой",
    "href": "l2.html#форматы-файлов-данных-c-особой-разметкой",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.3 Форматы файлов данных c особой разметкой",
    "text": "2.3 Форматы файлов данных c особой разметкой\nВстречаются редко и обычно связаны с данными, которые достаются из интернета (парсинг страниц или выгрзуки JavaScript).\n\n2.3.1 JSON\nВыглядят так:\n\n{\n  \"first_name\": \"John\",\n  \"last_name\": \"Smith\",\n  \"is_alive\": true,\n  \"age\": 27,\n  \"address\": {\n    \"street_address\": \"21 2nd Street\",\n    \"city\": \"New York\",\n    \"state\": \"NY\",\n    \"postal_code\": \"10021-3100\"\n  },\n  \"phone_numbers\": [\n    {\n      \"type\": \"home\",\n      \"number\": \"212 555-1234\"\n    },\n    {\n      \"type\": \"office\",\n      \"number\": \"646 555-4567\"\n    }\n  ],\n  \"children\": [\n    \"Catherine\",\n    \"Thomas\",\n    \"Trevor\"\n  ],\n  \"spouse\": null\n}\n\nДля работы с ними пригодится пакет jsonlite.\n\n\n2.3.2 XML\nВыглядят так:\n\n&lt;?xml version=\"1.0\"?&gt;\n&lt;catalog&gt;\n   &lt;book id=\"bk101\"&gt;\n      &lt;author&gt;Gambardella, Matthew&lt;/author&gt;\n      &lt;title&gt;XML Developer's Guide&lt;/title&gt;\n      &lt;genre&gt;Computer&lt;/genre&gt;\n      &lt;price&gt;44.95&lt;/price&gt;\n      &lt;publish_date&gt;2000-10-01&lt;/publish_date&gt;\n      &lt;description&gt;An in-depth look at creating applications \n      with XML.&lt;/description&gt;\n   &lt;/book&gt;\n   &lt;book id=\"bk102\"&gt;\n      &lt;author&gt;Ralls, Kim&lt;/author&gt;\n      &lt;title&gt;Midnight Rain&lt;/title&gt;\n      &lt;genre&gt;Fantasy&lt;/genre&gt;\n      &lt;price&gt;5.95&lt;/price&gt;\n      &lt;publish_date&gt;2000-12-16&lt;/publish_date&gt;\n      &lt;description&gt;A former architect battles corporate zombies, \n      an evil sorceress, and her own childhood to become queen \n      of the world.&lt;/description&gt;\n   &lt;/book&gt;\n   &lt;book id=\"bk103\"&gt;\n      &lt;author&gt;Corets, Eva&lt;/author&gt;\n      &lt;title&gt;Maeve Ascendant&lt;/title&gt;\n      &lt;genre&gt;Fantasy&lt;/genre&gt;\n      &lt;price&gt;5.95&lt;/price&gt;\n      &lt;publish_date&gt;2000-11-17&lt;/publish_date&gt;\n      &lt;description&gt;After the collapse of a nanotechnology \n      society in England, the young survivors lay the \n      foundation for a new society.&lt;/description&gt;\n   &lt;/book&gt;\n&lt;/catalog&gt;\n\nДля работы с ними пригодится пакет XML.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#организация-рабочего-пространства",
    "href": "l2.html#организация-рабочего-пространства",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.4 Организация рабочего пространства",
    "text": "2.4 Организация рабочего пространства\nРабочая директория — папка, в которую по умолчанию смотрит R, когда начинает искать файлы.\nЧтобы узнать рабочую директорию, воспользуйтесь функцией\n\ngetwd()\n\nЧтобы установить какую-либо папку в каечстве рабочей директории, используйте\n\nsetwd(\"/home/nglgrdt/R\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#путь-к-файлу",
    "href": "l2.html#путь-к-файлу",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.5 Путь к файлу",
    "text": "2.5 Путь к файлу\nБывает двух видов:\n\nабсолютный — /home/nglgrdt/R/wlm2023/pr1-course.R\nотносительный — wlm2023/pr1-course.R\n\nОтносительный путь вычисляется относительно текущей рабочей директории.\nПолезная вещь: .. — подняться на один уровень в иерархии папок.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#projects",
    "href": "l2.html#projects",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.6 Projects",
    "text": "2.6 Projects\n\nRStudio предоставляет возможность работать в проектах\nЭто существено упрощает организацию файлов, относящихся к одному исследованию\nВнутри проекта можно создавать необходимые папки (для данных, скриптов, результатов анализа, визуализаций и др.)\nПри запуске проекта автоматически устанавливается рабочая директория\nУдобно использовать относительные пути к файлам\nЧтобы код воспроизводился на другом коспьютере, достаточно заархивировать весь проект и отправить коллеге\n\n\n2.6.1 Как создать проект?\n\nВ правом верхнем углу нажмите на Project: (None) и выберите New Project....\n\n\n\n\n\nСоздание проекта. Шаг 1\n\n\n\n\nВ открывшемся окне выберите New Directory. Это опция создаст новую папку для проекта.\n\n\n\n\n\nСоздание проекта. Шаг 2\n\n\n\n\nДалее выберите New Project.\n\n\n\n\n\nСоздание проекта. Шаг 3\n\n\n\n\nВ поле Directory name укажите, как будет называться папка проекта. В поле Create project as subdirectory of: можно выбрать, внутри какой папки создастся папка проекта. Нажмите Create Project.\n\n\n\n\n\nСоздание проекта. Шаг 4\n\n\n\n\nПроект создан и открыт. Теперь в нем можно работать. Чтобы узнать, в каком проекте вы сейчас находитесь, посмотрите в правый верхний угол.\n\n\n\n\n\nСоздание проекта. Шаг 5",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#кодировка",
    "href": "l2.html#кодировка",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.7 Кодировка",
    "text": "2.7 Кодировка\n\nКомпьютер умеет хранить только числа, а скрипт — это текст\nДанные также часто содержат текст\nДоговорились, что буквеные символы будут храниться на железе компьютера в виде чисел и составили таблицы соответствий между числами и буквами\nТакие таблицы были названы кодировками\nКодировок много, так как они создавались для разных алфавитов и задач\nСтандартной считается UTF-8\nПри работе с нестандартными символами (например, диакритические знаки или фонетические символы) пригодится UTF-16",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#первичное-исследование-данных",
    "href": "l2.html#первичное-исследование-данных",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.8 Первичное исследование данных",
    "text": "2.8 Первичное исследование данных\nПрочитаем данные. В данном случае использован относительный путь.\n\nlec2 &lt;- read.csv(\"data/lecture2.csv\", encoding = \"UTF-8\")\n\nВыведем первый 6 строк датасета:\n\nhead(lec2)\n\n  id var1 var2  var3  var4 var5\n1  1  Abc    5  TRUE cond1 12.8\n2  2  Def   16 FALSE cond1 14.2\n3  3  Ghi   94 FALSE cond2 32.5\n4  4  Jkl   28 FALSE cond2  9.4\n5  5  Mno   11  TRUE cond3  6.3\n6  6  Pqr  100  TRUE cond3 11.7\n\n\nВыведем последние 3 строки датасета:\n\ntail(lec2, n = 3)\n\n  id var1 var2  var3  var4 var5\n5  5  Mno   11  TRUE cond3  6.3\n6  6  Pqr  100  TRUE cond3 11.7\n7  7  Stu   96 FALSE cond1 95.5\n\n\nПосмотрим на структуру датасета:\n\nstr(lec2)\n\n'data.frame':   7 obs. of  6 variables:\n $ id  : int  1 2 3 4 5 6 7\n $ var1: chr  \"Abc\" \"Def\" \"Ghi\" \"Jkl\" ...\n $ var2: int  5 16 94 28 11 100 96\n $ var3: logi  TRUE FALSE FALSE FALSE TRUE TRUE ...\n $ var4: chr  \"cond1\" \"cond1\" \"cond2\" \"cond2\" ...\n $ var5: num  12.8 14.2 32.5 9.4 6.3 11.7 95.5\n\n\nПосмотрим описательные статистики по переменным:\n\nsummary(lec2)\n\n       id          var1                var2          var3        \n Min.   :1.0   Length:7           Min.   :  5.0   Mode :logical  \n 1st Qu.:2.5   Class :character   1st Qu.: 13.5   FALSE:4        \n Median :4.0   Mode  :character   Median : 28.0   TRUE :3        \n Mean   :4.0                      Mean   : 50.0                  \n 3rd Qu.:5.5                      3rd Qu.: 95.0                  \n Max.   :7.0                      Max.   :100.0                  \n     var4                var5      \n Length:7           Min.   : 6.30  \n Class :character   1st Qu.:10.55  \n Mode  :character   Median :12.80  \n                    Mean   :26.06  \n                    3rd Qu.:23.35  \n                    Max.   :95.50  \n\n\nСделаем частотную таблицу по категориальной переменной:\n\ntable(lec2$var4)\n\n\ncond1 cond2 cond3 \n    3     2     2 \n\n\nПосмотрим уникальные значений по переменной-индетификатору:\n\nunique(lec2$id)\n\n[1] 1 2 3 4 5 6 7\n\n\nОтсортируем количественную переменную по возрастанию:\n\nsort(lec2$var5)\n\n[1]  6.3  9.4 11.7 12.8 14.2 32.5 95.5\n\n\nПроверим, есть ли пропущенные значения — выполним функцию is.na() на каждом столце датасета:\n\napply(lec2, 2, is.na)\n\n        id  var1  var2  var3  var4  var5\n[1,] FALSE FALSE FALSE FALSE FALSE FALSE\n[2,] FALSE FALSE FALSE FALSE FALSE FALSE\n[3,] FALSE FALSE FALSE FALSE FALSE FALSE\n[4,] FALSE FALSE FALSE FALSE FALSE FALSE\n[5,] FALSE FALSE FALSE FALSE FALSE FALSE\n[6,] FALSE FALSE FALSE FALSE FALSE FALSE\n[7,] FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nУпрощенным вариантом функции apply() является sapply() — она сразу запускает какую-либо функцию по столбцам датасета. В данному случае использована анонимная функция, которая посчитает количество пропущенных значений в каждом столбце.\n\nsapply(lec2, function(x) sum(is.na(x)))\n\n  id var1 var2 var3 var4 var5 \n   0    0    0    0    0    0",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#зачем-нам-предобрабатывать-данные",
    "href": "l2.html#зачем-нам-предобрабатывать-данные",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.9 Зачем нам предобрабатывать данные?",
    "text": "2.9 Зачем нам предобрабатывать данные?\n\nДанные разнообразны в зависимости от того\n\nкакая у нас исследовательская область\nкакая у нас исследуемый феномен\nс каким оборудованием мы работаем\n…\n\nДля того, чтобы мы могли работать с данными независимо от того, откуда они к нам пришли, нам нужно привести их к некому стандартному виду",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#концепция-tidy-data",
    "href": "l2.html#концепция-tidy-data",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.10 Концепция Tidy Data",
    "text": "2.10 Концепция Tidy Data\n\nВ каждом столбце содержится одна переменная\nВ каждой строке содержится одно наблюдение\nВ каждой ячейке содержится одно значение\n\n\n\n\n\nПринципы концепции Tidy Data",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#семейство-пакетов-tidyverse",
    "href": "l2.html#семейство-пакетов-tidyverse",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.11 Семейство пакетов tidyverse",
    "text": "2.11 Семейство пакетов tidyverse\nУстановить пакет:\n\ninstall.packages(\"tidyverse\")\n\nПодключить пакет к текущей сессии R:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#импорт-данных.-tibble",
    "href": "l2.html#импорт-данных.-tibble",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.12 Импорт данных. tibble",
    "text": "2.12 Импорт данных. tibble\nФункции импорта данных из tidyverse атоматически читают данные в tibble. С точки зрения пользователя tibble — это тот же самый датафрейм. Есть некоторые отличия в выводе в консоль, с которыми мы ближе познакомимся на практике.\nДля чтения CSV-файлов есть следующие функции:\n\nread_csv()\nread_csv2()\n\nДля чтения данных с другими разделителями есть функция\n\nread_delim()\n\nЗагрузим данные:\n\nds &lt;- read_csv(\"data/lecture2.csv\")\n\nRows: 7 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): var1, var4\ndbl (3): id, var2, var5\nlgl (1): var3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nds\n\n# A tibble: 7 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     2 Def      16 FALSE cond1  14.2\n3     3 Ghi      94 FALSE cond2  32.5\n4     4 Jkl      28 FALSE cond2   9.4\n5     5 Mno      11 TRUE  cond3   6.3\n6     6 Pqr     100 TRUE  cond3  11.7\n7     7 Stu      96 FALSE cond1  95.5",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#pipe",
    "href": "l2.html#pipe",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.13 Pipe",
    "text": "2.13 Pipe\nПрежде чем начать разговор о предобработке, познакомимся с одним очень порезным оператором из tidyverse — это пайп.\nСуществует два вида пайпа:\n\nмаггритеровский %&gt;%\n\nтребуется подключение пакета tidyverse\n\nнативный |&gt;\n\nнаходится в базовом R, включается через настройки\n\nTools &gt; Global Options &gt; Code &gt; Use native pipe operator\n\n\n\nС точки зрения пользователя практически не отличаются друг от друга. И тот, и другой вводится комбинацией Ctrl + Shift + M. Выбрать, какой именно будет использоваться, можно в настройках (Tools &gt; Global Options &gt; Code &gt; Use native pipe operator.)\n\n\n\n\nВыбор используемого пайпа в настройках\n\n\n\nПайп передает то, что слева от него, в функцию, которая справа от него, в качестве первого аргумента.\n\nsum(1:3)\n\n[1] 6\n\n1:3 %&gt;% sum()\n\n[1] 6\n\n\n\nround(pi, 2)\n\n[1] 3.14\n\npi %&gt;% round(2)\n\n[1] 3.14\n\n\nПозволяет выстраивать цепочки последовательных преобразований:\n\nsqrt(abs(log(abs(round(sin(1 / cos(3)), 2)), 3)))\n\n[1] 0.3846181\n\n3 %&gt;% cos() %&gt;% \n  `/`(1, .) %&gt;% \n  sin() %&gt;% \n  round(2) %&gt;% \n  abs() %&gt;% \n  log(3) %&gt;% \n  abs() %&gt;% \n  sqrt()\n\n[1] 0.3846181",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#переименование-переменных",
    "href": "l2.html#переименование-переменных",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.14 Переименование переменных",
    "text": "2.14 Переименование переменных\n\nds\n\n# A tibble: 7 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     2 Def      16 FALSE cond1  14.2\n3     3 Ghi      94 FALSE cond2  32.5\n4     4 Jkl      28 FALSE cond2   9.4\n5     5 Mno      11 TRUE  cond3   6.3\n6     6 Pqr     100 TRUE  cond3  11.7\n7     7 Stu      96 FALSE cond1  95.5\n\n\n\nds %&gt;% \n  rename(city = var1,\n         age = var2)\n\n# A tibble: 7 × 6\n     id city    age var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     2 Def      16 FALSE cond1  14.2\n3     3 Ghi      94 FALSE cond2  32.5\n4     4 Jkl      28 FALSE cond2   9.4\n5     5 Mno      11 TRUE  cond3   6.3\n6     6 Pqr     100 TRUE  cond3  11.7\n7     7 Stu      96 FALSE cond1  95.5\n\n\n\nds %&gt;% \n  set_names(vars(id, city, age, student, condition, score))\n\n# A tibble: 7 × 6\n  `~id` `~city` `~age` `~student` `~condition` `~score`\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;lgl&gt;      &lt;chr&gt;           &lt;dbl&gt;\n1     1 Abc          5 TRUE       cond1            12.8\n2     2 Def         16 FALSE      cond1            14.2\n3     3 Ghi         94 FALSE      cond2            32.5\n4     4 Jkl         28 FALSE      cond2             9.4\n5     5 Mno         11 TRUE       cond3             6.3\n6     6 Pqr        100 TRUE       cond3            11.7\n7     7 Stu         96 FALSE      cond1            95.5\n\n\n\nds %&gt;% \n  set_names(vars(id, city, age, student, condition, score)) %&gt;% \n  rename_all(str_remove, \"~\")\n\n# A tibble: 7 × 6\n     id city    age student condition score\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt;   &lt;chr&gt;     &lt;dbl&gt;\n1     1 Abc       5 TRUE    cond1      12.8\n2     2 Def      16 FALSE   cond1      14.2\n3     3 Ghi      94 FALSE   cond2      32.5\n4     4 Jkl      28 FALSE   cond2       9.4\n5     5 Mno      11 TRUE    cond3       6.3\n6     6 Pqr     100 TRUE    cond3      11.7\n7     7 Stu      96 FALSE   cond1      95.5",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#фильтрация-данных.-строки",
    "href": "l2.html#фильтрация-данных.-строки",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.15 Фильтрация данных. Строки",
    "text": "2.15 Фильтрация данных. Строки\n\nds\n\n# A tibble: 7 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     2 Def      16 FALSE cond1  14.2\n3     3 Ghi      94 FALSE cond2  32.5\n4     4 Jkl      28 FALSE cond2   9.4\n5     5 Mno      11 TRUE  cond3   6.3\n6     6 Pqr     100 TRUE  cond3  11.7\n7     7 Stu      96 FALSE cond1  95.5\n\nds %&gt;% \n  filter(var5 &gt; 10)\n\n# A tibble: 5 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     2 Def      16 FALSE cond1  14.2\n3     3 Ghi      94 FALSE cond2  32.5\n4     6 Pqr     100 TRUE  cond3  11.7\n5     7 Stu      96 FALSE cond1  95.5\n\nds %&gt;% \n  filter(var3)\n\n# A tibble: 3 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     5 Mno      11 TRUE  cond3   6.3\n3     6 Pqr     100 TRUE  cond3  11.7\n\nds %&gt;% slice(3:5)\n\n# A tibble: 3 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     3 Ghi      94 FALSE cond2  32.5\n2     4 Jkl      28 FALSE cond2   9.4\n3     5 Mno      11 TRUE  cond3   6.3",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#фильтрация-данных.-столбцы",
    "href": "l2.html#фильтрация-данных.-столбцы",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.16 Фильтрация данных. Столбцы",
    "text": "2.16 Фильтрация данных. Столбцы\n\nds\n\n# A tibble: 7 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     2 Def      16 FALSE cond1  14.2\n3     3 Ghi      94 FALSE cond2  32.5\n4     4 Jkl      28 FALSE cond2   9.4\n5     5 Mno      11 TRUE  cond3   6.3\n6     6 Pqr     100 TRUE  cond3  11.7\n7     7 Stu      96 FALSE cond1  95.5\n\nds %&gt;% \n  select(id, var1, var3, var4)\n\n# A tibble: 7 × 4\n     id var1  var3  var4 \n  &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt;\n1     1 Abc   TRUE  cond1\n2     2 Def   FALSE cond1\n3     3 Ghi   FALSE cond2\n4     4 Jkl   FALSE cond2\n5     5 Mno   TRUE  cond3\n6     6 Pqr   TRUE  cond3\n7     7 Stu   FALSE cond1\n\nds %&gt;% \n  select(starts_with(\"var\"))\n\n# A tibble: 7 × 5\n  var1   var2 var3  var4   var5\n  &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1 Abc       5 TRUE  cond1  12.8\n2 Def      16 FALSE cond1  14.2\n3 Ghi      94 FALSE cond2  32.5\n4 Jkl      28 FALSE cond2   9.4\n5 Mno      11 TRUE  cond3   6.3\n6 Pqr     100 TRUE  cond3  11.7\n7 Stu      96 FALSE cond1  95.5",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#сортировка-данных",
    "href": "l2.html#сортировка-данных",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.17 Сортировка данных",
    "text": "2.17 Сортировка данных\n\nds\n\n# A tibble: 7 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     2 Def      16 FALSE cond1  14.2\n3     3 Ghi      94 FALSE cond2  32.5\n4     4 Jkl      28 FALSE cond2   9.4\n5     5 Mno      11 TRUE  cond3   6.3\n6     6 Pqr     100 TRUE  cond3  11.7\n7     7 Stu      96 FALSE cond1  95.5\n\nds %&gt;% \n  arrange(var5)\n\n# A tibble: 7 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     5 Mno      11 TRUE  cond3   6.3\n2     4 Jkl      28 FALSE cond2   9.4\n3     6 Pqr     100 TRUE  cond3  11.7\n4     1 Abc       5 TRUE  cond1  12.8\n5     2 Def      16 FALSE cond1  14.2\n6     3 Ghi      94 FALSE cond2  32.5\n7     7 Stu      96 FALSE cond1  95.5\n\nds %&gt;% \n  arrange(desc(var5))\n\n# A tibble: 7 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     7 Stu      96 FALSE cond1  95.5\n2     3 Ghi      94 FALSE cond2  32.5\n3     2 Def      16 FALSE cond1  14.2\n4     1 Abc       5 TRUE  cond1  12.8\n5     6 Pqr     100 TRUE  cond3  11.7\n6     4 Jkl      28 FALSE cond2   9.4\n7     5 Mno      11 TRUE  cond3   6.3\n\nds %&gt;% \n  distinct(var4, var3)\n\n# A tibble: 4 × 2\n  var4  var3 \n  &lt;chr&gt; &lt;lgl&gt;\n1 cond1 TRUE \n2 cond1 FALSE\n3 cond2 FALSE\n4 cond3 TRUE",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#создание-и-изменение-переменных",
    "href": "l2.html#создание-и-изменение-переменных",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.18 Создание и изменение переменных",
    "text": "2.18 Создание и изменение переменных\n\nds\n\n# A tibble: 7 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     2 Def      16 FALSE cond1  14.2\n3     3 Ghi      94 FALSE cond2  32.5\n4     4 Jkl      28 FALSE cond2   9.4\n5     5 Mno      11 TRUE  cond3   6.3\n6     6 Pqr     100 TRUE  cond3  11.7\n7     7 Stu      96 FALSE cond1  95.5\n\nds %&gt;% \n  mutate(banch = 1)\n\n# A tibble: 7 × 7\n     id var1   var2 var3  var4   var5 banch\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8     1\n2     2 Def      16 FALSE cond1  14.2     1\n3     3 Ghi      94 FALSE cond2  32.5     1\n4     4 Jkl      28 FALSE cond2   9.4     1\n5     5 Mno      11 TRUE  cond3   6.3     1\n6     6 Pqr     100 TRUE  cond3  11.7     1\n7     7 Stu      96 FALSE cond1  95.5     1\n\nds %&gt;% \n  mutate(banch = 1,\n         var5_cat = ifelse(var5 &gt; mean(var5), \"high\", \"low\"),\n         var4 = recode(var4,\n                       \"cond1\" = \"easy\",\n                       \"cond2\" = \"medium\",\n                       \"cond3\" = \"hard\"))\n\n# A tibble: 7 × 8\n     id var1   var2 var3  var4    var5 banch var5_cat\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n1     1 Abc       5 TRUE  easy    12.8     1 low     \n2     2 Def      16 FALSE easy    14.2     1 low     \n3     3 Ghi      94 FALSE medium  32.5     1 high    \n4     4 Jkl      28 FALSE medium   9.4     1 low     \n5     5 Mno      11 TRUE  hard     6.3     1 low     \n6     6 Pqr     100 TRUE  hard    11.7     1 low     \n7     7 Stu      96 FALSE easy    95.5     1 high",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#группировка-и-аггрегация-данных",
    "href": "l2.html#группировка-и-аггрегация-данных",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.19 Группировка и аггрегация данных",
    "text": "2.19 Группировка и аггрегация данных\n\nds\n\n# A tibble: 7 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     2 Def      16 FALSE cond1  14.2\n3     3 Ghi      94 FALSE cond2  32.5\n4     4 Jkl      28 FALSE cond2   9.4\n5     5 Mno      11 TRUE  cond3   6.3\n6     6 Pqr     100 TRUE  cond3  11.7\n7     7 Stu      96 FALSE cond1  95.5\n\nds %&gt;% \n  summarise(v5_mean = mean(var5),\n            v2_median = median(var2))\n\n# A tibble: 1 × 2\n  v5_mean v2_median\n    &lt;dbl&gt;     &lt;dbl&gt;\n1    26.1        28\n\nds %&gt;% \n  group_by(var4) %&gt;% \n  summarise(n = n(),\n            v5_mean = mean(var5),\n            v2_median = median(var2))\n\n# A tibble: 3 × 4\n  var4      n v5_mean v2_median\n  &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 cond1     3    40.8      16  \n2 cond2     2    21.0      61  \n3 cond3     2     9        55.5\n\nds %&gt;% \n  summarise(n = n(),\n            v5_mean = mean(var5),\n            v2_median = median(var2),\n            .by = var4)\n\n# A tibble: 3 × 4\n  var4      n v5_mean v2_median\n  &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 cond1     3    40.8      16  \n2 cond2     2    21.0      61  \n3 cond3     2     9        55.5\n\n\n\nungroup()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#объединение-датасетов.-строки-и-столбцы",
    "href": "l2.html#объединение-датасетов.-строки-и-столбцы",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.20 Объединение датасетов. Строки и столбцы",
    "text": "2.20 Объединение датасетов. Строки и столбцы\n\na\n\n# A tibble: 4 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     2 Def      16\n3     3 Ghi      94\n4     4 Jkl      28\n\nb\n\n# A tibble: 4 × 3\n     id var3  var4 \n  &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt;\n1     1 TRUE  cond1\n2     2 FALSE cond1\n3     3 FALSE cond2\n4     4 FALSE cond2\n\nc\n\n# A tibble: 3 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     5 Mno      11\n2     6 Pqr     100\n3     7 Stu      96\n\na %&gt;% \n  bind_cols(b)\n\nNew names:\n• `id` -&gt; `id...1`\n• `id` -&gt; `id...4`\n\n\n# A tibble: 4 × 6\n  id...1 var1   var2 id...4 var3  var4 \n   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt;\n1      1 Abc       5      1 TRUE  cond1\n2      2 Def      16      2 FALSE cond1\n3      3 Ghi      94      3 FALSE cond2\n4      4 Jkl      28      4 FALSE cond2\n\na %&gt;% \n  bind_rows(c)\n\n# A tibble: 7 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     2 Def      16\n3     3 Ghi      94\n4     4 Jkl      28\n5     5 Mno      11\n6     6 Pqr     100\n7     7 Stu      96",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#объединение-датасетов.-ключ",
    "href": "l2.html#объединение-датасетов.-ключ",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.21 Объединение датасетов. Ключ",
    "text": "2.21 Объединение датасетов. Ключ\n\n\n\n\nСхема датасетов\n\n\n\n\n\nx\n\n# A tibble: 4 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     2 Def      16\n3     5 Mno      11\n4     7 Stu      96\n\ny\n\n# A tibble: 4 × 3\n     id var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     2 cond1  14.2\n3     3 cond2  32.5\n4     4 cond2   9.4",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#inner_join",
    "href": "l2.html#inner_join",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.22 inner_join()",
    "text": "2.22 inner_join()\n\n\n\n\nСхема inner_join()\n\n\n\n\n\nx\n\n# A tibble: 4 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     2 Def      16\n3     5 Mno      11\n4     7 Stu      96\n\ny\n\n# A tibble: 4 × 3\n     id var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     2 cond1  14.2\n3     3 cond2  32.5\n4     4 cond2   9.4\n\nx %&gt;% inner_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 2 × 5\n     id var1   var2 var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 cond1  12.8\n2     2 Def      16 cond1  14.2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#left_join",
    "href": "l2.html#left_join",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.23 left_join()",
    "text": "2.23 left_join()\n\n\n\n\nСхема left_join()\n\n\n\n\n\nx\n\n# A tibble: 4 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     2 Def      16\n3     5 Mno      11\n4     7 Stu      96\n\ny\n\n# A tibble: 4 × 3\n     id var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     2 cond1  14.2\n3     3 cond2  32.5\n4     4 cond2   9.4\n\nx %&gt;% left_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 4 × 5\n     id var1   var2 var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 cond1  12.8\n2     2 Def      16 cond1  14.2\n3     5 Mno      11 &lt;NA&gt;   NA  \n4     7 Stu      96 &lt;NA&gt;   NA",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#right_join",
    "href": "l2.html#right_join",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.24 right_join()",
    "text": "2.24 right_join()\n\n\n\n\nСхема right_join()\n\n\n\n\n\nx\n\n# A tibble: 4 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     2 Def      16\n3     5 Mno      11\n4     7 Stu      96\n\ny\n\n# A tibble: 4 × 3\n     id var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     2 cond1  14.2\n3     3 cond2  32.5\n4     4 cond2   9.4\n\nx %&gt;% right_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 4 × 5\n     id var1   var2 var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 cond1  12.8\n2     2 Def      16 cond1  14.2\n3     3 &lt;NA&gt;     NA cond2  32.5\n4     4 &lt;NA&gt;     NA cond2   9.4",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#full_join",
    "href": "l2.html#full_join",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.25 full_join()",
    "text": "2.25 full_join()\n\n\n\n\nСхема full_join()\n\n\n\n\n\nx\n\n# A tibble: 4 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     2 Def      16\n3     5 Mno      11\n4     7 Stu      96\n\ny\n\n# A tibble: 4 × 3\n     id var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     2 cond1  14.2\n3     3 cond2  32.5\n4     4 cond2   9.4\n\nx %&gt;% full_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 6 × 5\n     id var1   var2 var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 cond1  12.8\n2     2 Def      16 cond1  14.2\n3     5 Mno      11 &lt;NA&gt;   NA  \n4     7 Stu      96 &lt;NA&gt;   NA  \n5     3 &lt;NA&gt;     NA cond2  32.5\n6     4 &lt;NA&gt;     NA cond2   9.4",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#semi_join",
    "href": "l2.html#semi_join",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.26 semi_join()",
    "text": "2.26 semi_join()\n\n\n\n\nСхема semi_join()\n\n\n\n\n\nx\n\n# A tibble: 4 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     2 Def      16\n3     5 Mno      11\n4     7 Stu      96\n\ny\n\n# A tibble: 4 × 3\n     id var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     2 cond1  14.2\n3     3 cond2  32.5\n4     4 cond2   9.4\n\nx %&gt;% semi_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 2 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     2 Def      16\n\ny %&gt;% semi_join(x)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 2 × 3\n     id var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     2 cond1  14.2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#anti_join",
    "href": "l2.html#anti_join",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.27 anti_join()",
    "text": "2.27 anti_join()\n\n\n\n\nСхема anti_join()\n\n\n\n\n\nx\n\n# A tibble: 4 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     2 Def      16\n3     5 Mno      11\n4     7 Stu      96\n\ny\n\n# A tibble: 4 × 3\n     id var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     2 cond1  14.2\n3     3 cond2  32.5\n4     4 cond2   9.4\n\nx %&gt;% anti_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 2 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     5 Mno      11\n2     7 Stu      96\n\ny %&gt;% anti_join(x)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 2 × 3\n     id var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     3 cond2  32.5\n2     4 cond2   9.4",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#дублирование-ключа",
    "href": "l2.html#дублирование-ключа",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.28 Дублирование ключа",
    "text": "2.28 Дублирование ключа\n\n\n\n\nСхема работы .._join() при наличии дублей в колонке ключа\n\n\n\n\n\nx\n\n# A tibble: 5 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     1 Abc       7\n3     2 Def      16\n4     5 Mno      11\n5     7 Stu      96\n\ny\n\n# A tibble: 5 × 3\n     id var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     2 cond1  14.2\n3     2 cond2   2  \n4     3 cond2  32.5\n5     4 cond2   9.4\n\nx %&gt;% inner_join(y)\n\nJoining with `by = join_by(id)`\n\n\nWarning in inner_join(., y): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 3 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 4 × 5\n     id var1   var2 var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 cond1  12.8\n2     1 Abc       7 cond1  12.8\n3     2 Def      16 cond1  14.2\n4     2 Def      16 cond2   2  \n\nx %&gt;% left_join(y)\n\nJoining with `by = join_by(id)`\n\n\nWarning in left_join(., y): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 3 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 6 × 5\n     id var1   var2 var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 cond1  12.8\n2     1 Abc       7 cond1  12.8\n3     2 Def      16 cond1  14.2\n4     2 Def      16 cond2   2  \n5     5 Mno      11 &lt;NA&gt;   NA  \n6     7 Stu      96 &lt;NA&gt;   NA  \n\nx %&gt;% right_join(y)\n\nJoining with `by = join_by(id)`\n\n\nWarning in right_join(., y): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 3 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 6 × 5\n     id var1   var2 var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 cond1  12.8\n2     1 Abc       7 cond1  12.8\n3     2 Def      16 cond1  14.2\n4     2 Def      16 cond2   2  \n5     3 &lt;NA&gt;     NA cond2  32.5\n6     4 &lt;NA&gt;     NA cond2   9.4\n\nx %&gt;% full_join(y)\n\nJoining with `by = join_by(id)`\n\n\nWarning in full_join(., y): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 3 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 8 × 5\n     id var1   var2 var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 cond1  12.8\n2     1 Abc       7 cond1  12.8\n3     2 Def      16 cond1  14.2\n4     2 Def      16 cond2   2  \n5     5 Mno      11 &lt;NA&gt;   NA  \n6     7 Stu      96 &lt;NA&gt;   NA  \n7     3 &lt;NA&gt;     NA cond2  32.5\n8     4 &lt;NA&gt;     NA cond2   9.4",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#способы-объединения-по-ключу-на-диаграммах-венна",
    "href": "l2.html#способы-объединения-по-ключу-на-диаграммах-венна",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.29 Способы объединения по ключу на диаграммах Венна",
    "text": "2.29 Способы объединения по ключу на диаграммах Венна\n\n\n\n\nСхема работы .._join() на диаграммах Венна",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#широкий-и-длинный-форматы-данных",
    "href": "l2.html#широкий-и-длинный-форматы-данных",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.30 Широкий и длинный форматы данных",
    "text": "2.30 Широкий и длинный форматы данных\n\n\n\n\nПредставление одних и тех же данных в длинном и широком форматах\n\n\n\n\nlong\n\n# A tibble: 6 × 3\n     id var1   var2\n  &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     2 cond1  14.2\n3     1 cond2  32.5\n4     2 cond2   9.4\n5     1 cond3   6.3\n6     2 cond3  11.7\n\nlong %&gt;% \n  pivot_wider(names_from = var1,\n              values_from = var2) -&gt; wide\nwide\n\n# A tibble: 2 × 4\n     id cond1 cond2 cond3\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1  12.8  32.5   6.3\n2     2  14.2   9.4  11.7\n\nwide %&gt;% \n  pivot_longer(cols = -id) # names_to, values_to\n\n# A tibble: 6 × 3\n     id name  value\n  &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     1 cond2  32.5\n3     1 cond3   6.3\n4     2 cond1  14.2\n5     2 cond2   9.4\n6     2 cond3  11.7",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#строки",
    "href": "l2.html#строки",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.31 Строки",
    "text": "2.31 Строки\nДва специализированных пакета:\n\nstringr (является частью tidyverse)\nstringi (устанавливается отдельно)\n\n\n# install.packages(\"stringi\")\nlibrary(stringi)\n\n\n2.31.1 Как создать строку?\n\ns1 &lt;- \"сложившаяся структура организации влечет за собой процесс внедрения и модернизации новых предложений\"\ns1\n\n[1] \"сложившаяся структура организации влечет за собой процесс внедрения и модернизации новых предложений\"\n\n\n\ns2 &lt;- 'С другой стороны постоянный количественный \"рост\" и сфера нашей активности позволяет выполнять важные задания по разработке соответствующий условий активизации'\ns2\n\n[1] \"С другой стороны постоянный количественный \\\"рост\\\" и сфера нашей активности позволяет выполнять важные задания по разработке соответствующий условий активизации\"\n\n\n\ns3 &lt;- \"С другой стороны постоянный количественный \"рост\" и сфера нашей активности позволяет выполнять важные задания по разработке соответствующий условий активизации\"\ns3\n\nError: &lt;text&gt;:1:52: unexpected symbol\n1: s3 &lt;- \"С другой стороны постоянный количественный \"рост\n                                                       ^\n\n\n\ns4 &lt;- \"С другой стороны постоянный количественный 'рост' и сфера нашей активности позволяет выполнять важные задания по разработке соответствующий условий активизации\"\ns4\n\n[1] \"С другой стороны постоянный количественный 'рост' и сфера нашей активности позволяет выполнять важные задания по разработке соответствующий условий активизации\"\n\n\n\ns5 &lt;- \"С другой стороны постоянный количественный «рост» и сфера нашей активности позволяет выполнять важные задания по разработке соответствующий условий активизации\"\ns5\n\n[1] \"С другой стороны постоянный количественный «рост» и сфера нашей активности позволяет выполнять важные задания по разработке соответствующий условий активизации\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#генерация-случайных-строк",
    "href": "l2.html#генерация-случайных-строк",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.32 Генерация случайных строк",
    "text": "2.32 Генерация случайных строк\n\nstri_rand_strings(n = 10, length = 5)\n\n [1] \"cZshv\" \"5ijhp\" \"oHueG\" \"1e5Fd\" \"nP4K9\" \"CvqtQ\" \"TtrYc\" \"4SlDj\" \"23FBw\"\n[10] \"QdP19\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#операции-над-строками",
    "href": "l2.html#операции-над-строками",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.33 Операции над строками",
    "text": "2.33 Операции над строками\n\n2.33.1 Конкатенация строк\n\npaste('first', 'second', 'third')\n\n[1] \"first second third\"\n\n\n\npaste('first', 'second', 'third', sep = \"_\")\n\n[1] \"first_second_third\"\n\n\n\npaste0('first', 'second', 'third')\n\n[1] \"firstsecondthird\"\n\n\n\nstr_c('first', 'second', 'third')\n\n[1] \"firstsecondthird\"\n\n\n\nstr_c('first', 'second', 'third', sep = \"|\")\n\n[1] \"first|second|third\"\n\n\n\nstri_c('first', 'second', 'third')\n\n[1] \"firstsecondthird\"\n\n\n\nstri_c('first', 'second', 'third', sep = \" & \")\n\n[1] \"first & second & third\"\n\n\n\nds_1\n\n# A tibble: 150 × 4\n      id scale  item score\n   &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n 1     1 A         1     3\n 2     1 A         2     4\n 3     1 A         3     2\n 4     1 A         4     5\n 5     1 A         5     3\n 6     1 B         1     5\n 7     1 B         2     2\n 8     1 B         3     2\n 9     1 B         4     5\n10     1 B         5     1\n# ℹ 140 more rows\n\nds_1 %&gt;% \n  unite(scale_item, scale, item, sep = \"_\") -&gt; ds_2\nds_2\n\n# A tibble: 150 × 3\n      id scale_item score\n   &lt;int&gt; &lt;chr&gt;      &lt;int&gt;\n 1     1 A_1            3\n 2     1 A_2            4\n 3     1 A_3            2\n 4     1 A_4            5\n 5     1 A_5            3\n 6     1 B_1            5\n 7     1 B_2            2\n 8     1 B_3            2\n 9     1 B_4            5\n10     1 B_5            1\n# ℹ 140 more rows",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#операции-над-строками-1",
    "href": "l2.html#операции-над-строками-1",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.34 Операции над строками",
    "text": "2.34 Операции над строками\n\n2.34.1 Разделение строк\n\nstr_split(\"first second third\", pattern = \" \")\n\n[[1]]\n[1] \"first\"  \"second\" \"third\" \n\n\n\nds_2 %&gt;% \n  separate(scale_item, into = c(\"scale\", \"item\"))\n\n# A tibble: 150 × 4\n      id scale item  score\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n 1     1 A     1         3\n 2     1 A     2         4\n 3     1 A     3         2\n 4     1 A     4         5\n 5     1 A     5         3\n 6     1 B     1         5\n 7     1 B     2         2\n 8     1 B     3         2\n 9     1 B     4         5\n10     1 B     5         1\n# ℹ 140 more rows",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#сортировка-строк",
    "href": "l2.html#сортировка-строк",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.35 Сортировка строк",
    "text": "2.35 Сортировка строк\n\nunsorted_s\n\n [1] \"Odl\" \"Snm\" \"Nqr\" \"Cka\" \"Jgf\" \"Ruu\" \"Vlo\" \"Koi\" \"Ejt\" \"Tmp\" \"Nvd\" \"Vin\"\n[13] \"Ysv\" \"Zrk\" \"Eph\" \"Swx\" \"Yey\" \"Ytg\" \"Ixs\" \"Ccw\" \"Hfj\" \"Zab\" \"Gbz\" \"Jhe\"\n[25] \"Iyc\" \"Szq\"\n\n\n\nsort(unsorted_s)\n\n [1] \"Ccw\" \"Cka\" \"Ejt\" \"Eph\" \"Gbz\" \"Hfj\" \"Ixs\" \"Iyc\" \"Jgf\" \"Jhe\" \"Koi\" \"Nqr\"\n[13] \"Nvd\" \"Odl\" \"Ruu\" \"Snm\" \"Swx\" \"Szq\" \"Tmp\" \"Vin\" \"Vlo\" \"Yey\" \"Ysv\" \"Ytg\"\n[25] \"Zab\" \"Zrk\"\n\n\n\nstr_sort(unsorted_s)\n\n [1] \"Ccw\" \"Cka\" \"Ejt\" \"Eph\" \"Gbz\" \"Hfj\" \"Ixs\" \"Iyc\" \"Jgf\" \"Jhe\" \"Koi\" \"Nqr\"\n[13] \"Nvd\" \"Odl\" \"Ruu\" \"Snm\" \"Swx\" \"Szq\" \"Tmp\" \"Vin\" \"Vlo\" \"Yey\" \"Ysv\" \"Ytg\"\n[25] \"Zab\" \"Zrk\"\n\n\n\n# по умолчанию\nstr_sort(c(\"э\", \"а\", \"у\", \"i\"), locale = 'en')\n\n[1] \"i\" \"а\" \"у\" \"э\"\n\n\n\n# для русского языка\nstr_sort(c(\"э\", \"а\", \"у\", \"i\"), locale = 'ru')\n\n[1] \"а\" \"у\" \"э\" \"i\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#изменение-строк",
    "href": "l2.html#изменение-строк",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.36 Изменение строк",
    "text": "2.36 Изменение строк\n\n2.36.1 Выделение подстроки\n\nunsorted_s\n\n [1] \"Odl\" \"Snm\" \"Nqr\" \"Cka\" \"Jgf\" \"Ruu\" \"Vlo\" \"Koi\" \"Ejt\" \"Tmp\" \"Nvd\" \"Vin\"\n[13] \"Ysv\" \"Zrk\" \"Eph\" \"Swx\" \"Yey\" \"Ytg\" \"Ixs\" \"Ccw\" \"Hfj\" \"Zab\" \"Gbz\" \"Jhe\"\n[25] \"Iyc\" \"Szq\"\n\nstr_sub(unsorted_s, start = 1, end = 2)\n\n [1] \"Od\" \"Sn\" \"Nq\" \"Ck\" \"Jg\" \"Ru\" \"Vl\" \"Ko\" \"Ej\" \"Tm\" \"Nv\" \"Vi\" \"Ys\" \"Zr\" \"Ep\"\n[16] \"Sw\" \"Ye\" \"Yt\" \"Ix\" \"Cc\" \"Hf\" \"Za\" \"Gb\" \"Jh\" \"Iy\" \"Sz\"\n\n\n\n\n2.36.2 Замена подстроки\n\nunsorted_s\n\n [1] \"Odl\" \"Snm\" \"Nqr\" \"Cka\" \"Jgf\" \"Ruu\" \"Vlo\" \"Koi\" \"Ejt\" \"Tmp\" \"Nvd\" \"Vin\"\n[13] \"Ysv\" \"Zrk\" \"Eph\" \"Swx\" \"Yey\" \"Ytg\" \"Ixs\" \"Ccw\" \"Hfj\" \"Zab\" \"Gbz\" \"Jhe\"\n[25] \"Iyc\" \"Szq\"\n\nstr_replace(unsorted_s, pattern = \"O\", replacement = \"Ь\")\n\n [1] \"Ьdl\" \"Snm\" \"Nqr\" \"Cka\" \"Jgf\" \"Ruu\" \"Vlo\" \"Koi\" \"Ejt\" \"Tmp\" \"Nvd\" \"Vin\"\n[13] \"Ysv\" \"Zrk\" \"Eph\" \"Swx\" \"Yey\" \"Ytg\" \"Ixs\" \"Ccw\" \"Hfj\" \"Zab\" \"Gbz\" \"Jhe\"\n[25] \"Iyc\" \"Szq\"\n\n\n\n\n2.36.3 Удаление подстроки\n\nunsorted_s\n\n [1] \"Odl\" \"Snm\" \"Nqr\" \"Cka\" \"Jgf\" \"Ruu\" \"Vlo\" \"Koi\" \"Ejt\" \"Tmp\" \"Nvd\" \"Vin\"\n[13] \"Ysv\" \"Zrk\" \"Eph\" \"Swx\" \"Yey\" \"Ytg\" \"Ixs\" \"Ccw\" \"Hfj\" \"Zab\" \"Gbz\" \"Jhe\"\n[25] \"Iyc\" \"Szq\"\n\nstr_remove(unsorted_s, \"S\")\n\n [1] \"Odl\" \"nm\"  \"Nqr\" \"Cka\" \"Jgf\" \"Ruu\" \"Vlo\" \"Koi\" \"Ejt\" \"Tmp\" \"Nvd\" \"Vin\"\n[13] \"Ysv\" \"Zrk\" \"Eph\" \"wx\"  \"Yey\" \"Ytg\" \"Ixs\" \"Ccw\" \"Hfj\" \"Zab\" \"Gbz\" \"Jhe\"\n[25] \"Iyc\" \"zq\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#регулярные-выражения",
    "href": "l2.html#регулярные-выражения",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.37 Регулярные выражения",
    "text": "2.37 Регулярные выражения\n\ndates &lt;- c('21.92.2001', '01.04.1994', '5-3-2011', '6/04/1999')\ndates\n\n[1] \"21.92.2001\" \"01.04.1994\" \"5-3-2011\"   \"6/04/1999\" \n\n\n\n2.37.1 Метасимволы\n\nstr_view_all(dates, pattern = \".\")\n\nWarning: `str_view()` was deprecated in stringr 1.5.0.\nℹ Please use `str_view_all()` instead.\n\n\n[1] │ &lt;2&gt;&lt;1&gt;&lt;.&gt;&lt;9&gt;&lt;2&gt;&lt;.&gt;&lt;2&gt;&lt;0&gt;&lt;0&gt;&lt;1&gt;\n[2] │ &lt;0&gt;&lt;1&gt;&lt;.&gt;&lt;0&gt;&lt;4&gt;&lt;.&gt;&lt;1&gt;&lt;9&gt;&lt;9&gt;&lt;4&gt;\n[3] │ &lt;5&gt;&lt;-&gt;&lt;3&gt;&lt;-&gt;&lt;2&gt;&lt;0&gt;&lt;1&gt;&lt;1&gt;\n[4] │ &lt;6&gt;&lt;/&gt;&lt;0&gt;&lt;4&gt;&lt;/&gt;&lt;1&gt;&lt;9&gt;&lt;9&gt;&lt;9&gt;\n\n\n\nstr_view_all(dates, pattern = \"\\\\.\") # экранирование метасимволов\n\n[1] │ 21&lt;.&gt;92&lt;.&gt;2001\n[2] │ 01&lt;.&gt;04&lt;.&gt;1994\n[3] │ 5-3-2011\n[4] │ 6/04/1999\n\n\n\nstr_view_all(dates, pattern = \"^0\")\n\n[1] │ 21.92.2001\n[2] │ &lt;0&gt;1.04.1994\n[3] │ 5-3-2011\n[4] │ 6/04/1999\n\n\n\nstr_view_all(dates, pattern = \"9$\")\n\n[1] │ 21.92.2001\n[2] │ 01.04.1994\n[3] │ 5-3-2011\n[4] │ 6/04/199&lt;9&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#регулярные-выражения-1",
    "href": "l2.html#регулярные-выражения-1",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.38 Регулярные выражения",
    "text": "2.38 Регулярные выражения\n\n2.38.1 Классы знаков\n\nstr_view_all(dates, pattern = '\\\\d') # ищем цифры\n\n[1] │ &lt;2&gt;&lt;1&gt;.&lt;9&gt;&lt;2&gt;.&lt;2&gt;&lt;0&gt;&lt;0&gt;&lt;1&gt;\n[2] │ &lt;0&gt;&lt;1&gt;.&lt;0&gt;&lt;4&gt;.&lt;1&gt;&lt;9&gt;&lt;9&gt;&lt;4&gt;\n[3] │ &lt;5&gt;-&lt;3&gt;-&lt;2&gt;&lt;0&gt;&lt;1&gt;&lt;1&gt;\n[4] │ &lt;6&gt;/&lt;0&gt;&lt;4&gt;/&lt;1&gt;&lt;9&gt;&lt;9&gt;&lt;9&gt;\n\n\n\nstr_view_all(dates, pattern = '\\\\D') # ищем не-цифры\n\n[1] │ 21&lt;.&gt;92&lt;.&gt;2001\n[2] │ 01&lt;.&gt;04&lt;.&gt;1994\n[3] │ 5&lt;-&gt;3&lt;-&gt;2011\n[4] │ 6&lt;/&gt;04&lt;/&gt;1999\n\n\n\nstr_view_all('успешный успех', '\\\\s') # пробелы\n\n[1] │ успешный&lt; &gt;успех\n\n\n\nstr_view_all('успешный успех', '\\\\S') # не-пробелы\n\n[1] │ &lt;у&gt;&lt;с&gt;&lt;п&gt;&lt;е&gt;&lt;ш&gt;&lt;н&gt;&lt;ы&gt;&lt;й&gt; &lt;у&gt;&lt;с&gt;&lt;п&gt;&lt;е&gt;&lt;х&gt;\n\n\n\nstr_view_all('верно ведь, что здесь что-то есть', '\\\\w') # не пробелы и не знаки препинания\n\n[1] │ &lt;в&gt;&lt;е&gt;&lt;р&gt;&lt;н&gt;&lt;о&gt; &lt;в&gt;&lt;е&gt;&lt;д&gt;&lt;ь&gt;, &lt;ч&gt;&lt;т&gt;&lt;о&gt; &lt;з&gt;&lt;д&gt;&lt;е&gt;&lt;с&gt;&lt;ь&gt; &lt;ч&gt;&lt;т&gt;&lt;о&gt;-&lt;т&gt;&lt;о&gt; &lt;е&gt;&lt;с&gt;&lt;т&gt;&lt;ь&gt;\n\n\n\nstr_view_all('верно ведь, что здесь что-то есть', '\\\\W') # пробелы и знаки препинания\n\n[1] │ верно&lt; &gt;ведь&lt;,&gt;&lt; &gt;что&lt; &gt;здесь&lt; &gt;что&lt;-&gt;то&lt; &gt;есть",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#регулярные-выражения-2",
    "href": "l2.html#регулярные-выражения-2",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.39 Регулярные выражения",
    "text": "2.39 Регулярные выражения\n\n2.39.1 Квантификация\n\n? — ноль или один раз\n* — ноль или более раз\n+ — один или более раз\n{n} — n раз\n\n\nstr_view_all(dates, '\\\\d{2}')\n\n[1] │ &lt;21&gt;.&lt;92&gt;.&lt;20&gt;&lt;01&gt;\n[2] │ &lt;01&gt;.&lt;04&gt;.&lt;19&gt;&lt;94&gt;\n[3] │ 5-3-&lt;20&gt;&lt;11&gt;\n[4] │ 6/&lt;04&gt;/&lt;19&gt;&lt;99&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#дата-и-время",
    "href": "l2.html#дата-и-время",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.40 Дата и время",
    "text": "2.40 Дата и время\n\n2.40.1 Почему это особый тип данных?\n\nГод ≠ 365 дней: високосные года\nСутки ≠ 24 часа: переход на зимнее и летнее время\nМинута ≠ 60 секунд: компенсация замедления вращения земли (30 июня 23:59:60 или 31 декабря 23:59:60)\n\nВсе это автоматически обрабатывает lubridate.\n\ntoday()\n\n[1] \"2024-01-20\"\n\n\n\nnow()\n\n[1] \"2024-01-20 00:07:43 MSK\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#форматы-даты-и-времени",
    "href": "l2.html#форматы-даты-и-времени",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.41 Форматы даты и времени",
    "text": "2.41 Форматы даты и времени\n\nyyyy-mm-dd — является международным стандартом\n\nв таком формате с датой можно работать как со строкой, что оказывается удобно для баз данных\n\ndd/mm/yy, dd/mm/yyyy, dd.mm.yyyy — используется в Европе\nmm/dd/yy, mm/dd/yyyy — используется в США\nUnix-timestamp (число) — количество секунд с 01.01.1970, используется в базах данных",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#операции-над-датами",
    "href": "l2.html#операции-над-датами",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.42 Операции над датами",
    "text": "2.42 Операции над датами\n\n## текущий timestamp\nas.numeric(now())\n\n[1] 1705698463\n\n\n\nyear(now())\n\n[1] 2024\n\nmonth(now())\n\n[1] 1\n\nday(now())\n\n[1] 20\n\nhour(now())\n\n[1] 0\n\nminute(now())\n\n[1] 7\n\nsecond(now())\n\n[1] 43.38211\n\ndifftime(ymd_hm(\"2023-01-21, 21:00\"), ymd_hm(\"2023-01-21, 18:10\"), units = \"mins\")\n\nTime difference of 170 mins",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#зачем-нужна-визуализация",
    "href": "l2.html#зачем-нужна-визуализация",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.43 Зачем нужна визуализация?",
    "text": "2.43 Зачем нужна визуализация?\n\n2.43.1 Квартет Анскомба\n\n\nRows: 44 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): id, dataset, x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nspc_tbl_ [44 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ id     : num [1:44] 1 1 1 1 2 2 2 2 3 3 ...\n $ dataset: num [1:44] 1 2 3 4 1 2 3 4 1 2 ...\n $ x      : num [1:44] 10 10 10 8 8 8 8 8 13 13 ...\n $ y      : num [1:44] 8.04 9.14 7.46 6.58 6.95 8.14 6.77 5.76 7.58 8.74 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   id = col_double(),\n  ..   dataset = col_double(),\n  ..   x = col_double(),\n  ..   y = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n# A tibble: 4 × 7\n  dataset     n mean_x mean_y  sd_x  sd_y   cor\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1       1    11      9    7.5  3.32  2.03  0.82\n2       2    11      9    7.5  3.32  2.03  0.82\n3       3    11      9    7.5  3.32  2.03  0.82\n4       4    11      9    7.5  3.32  2.03  0.82\n\n\nНо если это нарисовать, получим такое:\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n2.43.2 Datasaurus\n\n\nRows: 1846 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): dataset\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nspc_tbl_ [1,846 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ dataset: chr [1:1846] \"dino\" \"dino\" \"dino\" \"dino\" ...\n $ x      : num [1:1846] 55.4 51.5 46.2 42.8 40.8 ...\n $ y      : num [1:1846] 97.2 96 94.5 91.4 88.3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   dataset = col_character(),\n  ..   x = col_double(),\n  ..   y = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n# A tibble: 13 × 7\n   dataset        n mean_x mean_y  sd_x  sd_y   cor\n   &lt;chr&gt;      &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 away         142   54.3   47.8  16.8  26.9  -0.1\n 2 bullseye     142   54.3   47.8  16.8  26.9  -0.1\n 3 circle       142   54.3   47.8  16.8  26.9  -0.1\n 4 dino         142   54.3   47.8  16.8  26.9  -0.1\n 5 dots         142   54.3   47.8  16.8  26.9  -0.1\n 6 h_lines      142   54.3   47.8  16.8  26.9  -0.1\n 7 high_lines   142   54.3   47.8  16.8  26.9  -0.1\n 8 slant_down   142   54.3   47.8  16.8  26.9  -0.1\n 9 slant_up     142   54.3   47.8  16.8  26.9  -0.1\n10 star         142   54.3   47.8  16.8  26.9  -0.1\n11 v_lines      142   54.3   47.8  16.8  26.9  -0.1\n12 wide_lines   142   54.3   47.8  16.8  26.9  -0.1\n13 x_shape      142   54.3   47.8  16.8  26.9  -0.1\n\n\nОднако если это визуализировать, то будет вот что:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#виды-графиков",
    "href": "l2.html#виды-графиков",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.44 Виды графиков",
    "text": "2.44 Виды графиков\n\n2.44.1 Столбчатая диаграмма (Bar plot, Bar graph)\n\n\n\n\n\n\n\n\n\n\n\n2.44.2 Лучевая диаграмма (Subburts)\n\n\n\n\n\n\n\n\n\n\n\n2.44.3 Круговая диаграмма (Pie chart)\n\n\n\n\n\n\n\n\n\n\n\n2.44.4 Линейная диаграмма (Line graph, Line plot)\n\n\n\n\n\n\n\n\n\n\n\n2.44.5 Гистограмма (Histogram)\n\n\n\n\n\n\n\n\n\n\n\n2.44.6 График плотности распределения (Density plot)\n\n\n\n\n\n\n\n\n\n\n\n2.44.7 Dot plot\n\n\n\n\n\n\n\n\n\n\n\n2.44.8 Ящик с усами (Boxplot)\n\n\n\n\n\n\n\n\n\n\n\n2.44.9 Violin plot\n\n\n\n\n\n\n\n\n\n\n\n2.44.10 График интервальных оценок (Error bar)\n\n\n\n\n\n\n\n\n\n\n\n2.44.11 Диаграмма рассенияния (Scatter plot)\n\n\n\n\n\n\n\n\n\n\n\n2.44.12 Пузырьковая диаграмма (Bubble plot)\n\n\n\n\n\n\n\n\n\n\n\n2.44.13 Корреляционная матрица (Corrplot)\n\n\nRows: 495 Columns: 133\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (11): id, oth_text, expoth_text, sex, edulvl1, spec1, edulvl2, spec2, j...\ndbl (122): e_dighelp, n_dighelp, e_socnet, f_socnet, n_socnet, gt_score, pr0...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#good-bad-practices-в-визуализации",
    "href": "l2.html#good-bad-practices-в-визуализации",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.45 Good & Bad practices в визуализации",
    "text": "2.45 Good & Bad practices в визуализации\n\n2.45.1 Хорошо\n\nПодписать оси так, чтобы было понятно, что они обозначают\nИспользовать контрастную палитру цветов (или обойтись черно-белой)\nПроверить, как видят график люди с цветовыми аномалиями зрения\nВыстроить из визуализаций историю\n\n\n\n2.45.2 Плохо\n\nИспользовать разные диапазоны шкал или разный масштаб на графиках, которые необходимо сравнивать\nСтроить столбчатую диаграмму не от нуля\nСтроить визуализацию в 3D на бумаге\nСтроить круговую диаграму\nСтроить круговую диаграмму в 3D\n\nБольше примеров странных визуализаций тут.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#философия-a-layered-grammar-of-graphics",
    "href": "l2.html#философия-a-layered-grammar-of-graphics",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.46 Философия A Layered Grammar of Graphics",
    "text": "2.46 Философия A Layered Grammar of Graphics\n\nГрафик состоит из нескольких независимых друг от друга элементов\n\nосновы графика (фон и оси)\nсистемы координат\nспособов отображения данных (геометрических объектов)\nшкал\nфасетов\n\nОднотипные элементы графика располагаются на отдельных слоях\nВычисления отделены от визуализации\nДизайн отделен от содержания\n\nЭти принципы легли в основу пакета ggplot2.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#итоги",
    "href": "l2.html#итоги",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.47 Итоги",
    "text": "2.47 Итоги\n\nФорматы файлов данных бывают разные и их нужно по-разному загружать в R\nRStudio Projects — это хорошо и помогает воспроизводимости\nПредобработка данных — это определенный набор операций, которые в tidyverse достаточно интуитивно реализованы\nСтроки и даты требуют к себе особого внимания\nВизуализаций много, они разнообразны и каждая по-своему хороша",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l3.html#дискретная-математика",
    "href": "l3.html#дискретная-математика",
    "title": "3  L3 // Математика для анализа данных",
    "section": "3.1 Дискретная математика",
    "text": "3.1 Дискретная математика\n\n3.1.1 Алгебра логики\nЛогика исследует закономерности мышления, но делает это отлично от того, как этим занимается, например, психология. Формальная логика абстрагируется от связей мышления с какими-либо сторонами сознания и сосредотачивается на логической непротиворечивости и последовательности мышления. Таким образом, формальная логика — это наука об общих структурах правильного мышления в его языковой форме (Зегет (1985)).\nЛогика — это нормативная наука, то есть она определяет, как оно должно быть, в то время как, например, психология исследуется как оно есть и как и почему логические законы нарушаются.\n\n3.1.1.1 Высказывания\nЛогика как наука имеет дело, прежде всего, с высказываваниями. Высказывание отражает определённую объективную1 связь между предметами. Высказывание истинно, если в нём содержится адеквантное отражение этой связи — в ином случае высказывание ложно. В общем случае высказывание существует с форме повествовательного предложения.\nНапример, высказывание «Земля вращается вокруг Солнца» отражает объективное отношение, поэтому оно истинно, а высказывание «страницы этой книги зелёного цвета» не адекватно отражает существующее положение дел, поэтому оно ложно.\nПрежде всего, нас будут интересовать атомарные высказывания. Это элементарные высказывания, которые невозможно разделить на составляющие — более мелкие высказывания. Например, «четыре — это целое число» — это атомарное высказывание.\nАтомарные высказывания могут быть либо истинны, либо ложны. Почему? Потому что мы находимся в рамках двузначной логики2. Атомарные высказывания могут быть обозначены пропозициональными переменными. Так же как и числа в математике могут быть заменены буквеными обозначениями для абстрагирования от значения числа, так же и высказывания заменяются переменными для абстрагирования от содержания высказывания.\nДля обозначения пропозициональных переменных используются латинские буквы. А так как само высказывания имеет опредлённое значение истинности (истина и ложь), то и переменная, которой мы обозначаем это высказывание, также будет обладать этим же значением истинности. Всё аналогично математике.\n\n\n3.1.1.2 Логические операции\nС атомарными высказываниями можно выполнять различные логические операции.\n\n3.1.1.2.1 Инверсия\nСамая простая операция — инверсия, или отрицание. Оно обозначается с помощью оператора \\(\\neg\\). Это унарная операция, то есть она применяется к одной переменной. При отрицании значение истинности высказывания изменяется на противоположное, поэтому мы можнм составить следующую таблицу истинности для отрицания:\n\n\n\n\\(p\\)\n\\(\\neg p\\)\n\n\n\n\nистина\nложь\n\n\nложь\nистина\n\n\n\nВ данном случае с помощью переменной \\(p\\) обозначено некоторые атомарное высказывание.\nОсобо стоит отметить, что при отрицании отрицается всё высказывание целиком, а не какой-то отдельный его элемент. То есть формально правильным вариантом отрицания высказывания «все лебеди белые» будет следующий — «неверно, что все лебеди белые».\nГрафически инверсия отображается так:\n\n\n\n\nЛогическое отрицание\n\n\n\n\nИз атомарных высказываний можно составлять сложные высказывания при помощи логических операторов. Например, высказывание «если четыре делится на два, то четыре — чётное число» является сложным, посколько состоит из двух атомарных — «четыре делится на два» и «четыре — чётное число» — соединённых союзом «если…, то…».\nДалее мы пристумаем с знакомству с бинарными операторами, то есть такими, которые функционируют на двух аргументах.\n\n\n3.1.1.2.2 Конъюнкция\nКонъюнкция (логическое умножение, логические И) представляет собой такое высказывание, которое наиболее точно передается следующей конструкцией естественного языка — «как \\(p\\), так и \\(q\\)».\n\\(p\\) и \\(q\\) в данном случае пропозициональные переменные, которые заменяют конкретные высказывания. Конъюнкция истинна тогда и только тогда, когда обе пропозициональные переменные, входящие в её состав, имеют значении истинности «истина». В любом ином случае конъюнкция ложна.\nКонъюнкция обозначается символом \\(\\wedge\\) и имеет следующую таблицу истинности:\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\wedge q\\)\n\n\n\n\nистина\nистина\nистина\n\n\nистина\nложь\nложь\n\n\nложь\nистина\nложь\n\n\nложь\nложь\nложь\n\n\n\nГрафически конъюнкция отображается так:\n\n\n\n\nКонъюнкция (логическое И)\n\n\n\n\n\n3.1.1.2.3 Дизъюнкция\nДизъюнкция (логическое сложение, логические ИЛИ) представляет собой такое высказывание, которое наиболее точно передается следующей конструкцией естественного языка — «или \\(p\\), или \\(q\\), или и то и другое», поэтому дизъюнкция истинна тогда, когда хотя бы одна пропозициональная переменная, входящая в её состав, имеет значении истинности «истина». В случае, если оба высказывания ложны, дизъюнкция будет ложна.\nДизъюнкция обозначается символом \\(\\vee\\) и имеет следующую таблицу истинности:\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\vee q\\)\n\n\n\n\nистина\nистина\nистина\n\n\nистина\nложь\nистина\n\n\nложь\nистина\nистина\n\n\nложь\nложь\nложь\n\n\n\nГрафически дизъюнкция отображается так:\n\n\n\n\nДизъюнкция (логическое ИЛИ)\n\n\n\n\n\n3.1.1.2.4 Разделительная дизъюнкция\nРазделительная дизъюнкция (исключающее ИЛИ) — это такое высказывание, которое наиболее полно описывается следующим выражением естественного языка — «либо \\(p\\), либо \\(q\\)». На её графическом представлении хорошо видно, чем она отличается от обычной дизъюнкции — она исключает ту часть пространства, где верны оба высказывания:\n\n\n\n\nРазделительная дизъюнкция (исключающее ИЛИ)\n\n\n\nДля обозначения разделительно дизъюнкции есть много различных операторов, но мы будем записывать её так — \\(p \\,\\text{XOR}\\,q\\). По иллюстрации можно восстановить таблицу истинности для этого оператора:\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\,\\text{XOR}\\,q\\)\n\n\n\n\nистина\nистина\nложь\n\n\nистина\nложь\nистина\n\n\nложь\nистина\nистина\n\n\nложь\nложь\nложь\n\n\n\nТаким образом, видно, что исключающее ИЛИ истинно тогда, когда значения истинности пропозициональных переменных, входящих в сложное высказывание, различны.\n\n\n\n3.1.1.3 Условные высказывания\n\n3.1.1.3.1 Импликация\nСложное высказывание, описываемое конструкцией естественного языка «если \\(p\\), то \\(q\\)» в формальной логике носит название импликации. Она отражает следование одного утверждения из другого и обозначается следующим образом — \\(p \\rightarrow q\\). Высказывание \\(p\\) называется антецедентом имликации, а \\(q\\) — консеквентом импликации.\nИмпликация имеет следующую таблицу истиности:\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\rightarrow q\\)\n\n\n\n\nистина\nистина\nистина\n\n\nистина\nложь\nложь\n\n\nложь\nистина\nистина\n\n\nложь\nложь\nистина\n\n\n\nКак видно из таблицы, импликация ложна только тогда, когда её антецедент истинен, и консеквент — ложен. Но что более интересно, так это то, что, согласно таблице, из ложного утверждения может следовать любое. Это факт мы вспомним, когда будем обсуждать тестирование статистических гипотез.\nИмпликацию утвержает то же самое, что и следующее сложное высказывание — \\(\\neg (p \\wedge \\neg q)\\). Отсюда можно получить графическое изображение импликации:\n\n\n\n\nИмпликация \\(p \\rightarrow q\\)\n\n\n\n\n\n3.1.1.3.2 Репликация\nРепликация похожа на импликацию, но действует как бы в обратном направлении, что отражено в её обозначении — \\(p \\leftarrow q\\). Эта конструкция читается как «\\(p\\) реплицирует \\(q\\)» и является эквивалентом естественноязыкового «только если \\(p\\), то \\(q\\)». Соответствующим образом изменяется и таблица истиности:\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\leftarrow q\\)\n\n\n\n\nистина\nистина\nистина\n\n\nистина\nложь\nистина\n\n\nложь\nистина\nложь\n\n\nложь\nложь\nистина\n\n\n\nИдея здесь в том, что если мы получили в результате истину, то лжи в начале быть не могло. Графическое изображение репликации выглядит так:\n\n\n\n\nРепликация \\(p \\leftarrow q\\)\n\n\n\n\n\n3.1.1.3.3 Эквиваленция\nЕсли мы соединим с помощью конъюнкции импликацию и репликацию, то есть запишем вот такое высказывание — \\((p \\rightarrow q) \\wedge (p \\leftarrow q)\\) — то получим эквиваленцию. По своей сути она является логическим отражением языковой конструкции «\\(q\\) только тогда, когда \\(p\\)», поэтому она обозначается вот так — \\(p \\leftrightarrow q\\) — и её таблица истинности выглядит соответствующим образом:\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\leftrightarrow q\\)\n\n\n\n\nистина\nистина\nистина\n\n\nистина\nложь\nложь\n\n\nложь\nистина\nложь\n\n\nложь\nложь\nистина\n\n\n\nИз неё выводится графическое представление эквиваленции:\n\n\n\n\nЭквиваленция \\(p \\leftrightarrow q\\)\n\n\n\n\n\n\n\n3.1.2 Элементы теории множеств\nНа базе теории множеств стоит вся современная математика. Мы ознакомимся только c некоторыми концепциями этого раздела математики, но вообще полезно с ним познакомиться гораздо глубже.\n\n3.1.2.1 Множество\nПонятие множества неопределимо. По крайней мере силами самой теории множеств. Но мы будем понимать под множеством совокупность, или набор, некоторых (в общем случае любых) объектов. Это могут быть числа, буквы, точки и любые другие объекты. Объекты, входящие в состав множества, называются элементами этого множества.\nМножества обозначают заглавными латинскими буквами (например, \\(A\\)), а его элементs — прописными латинскими буквами (например, \\(a_1\\), \\(a_2\\) и т.д.).\nМножества удобно изображать кружочками. Примерно так:\n\n\n\n\nМножество \\(А\\), содержащее элементы \\(a_1\\), \\(a_2\\), \\(a_3\\), \\(a_4\\), \\(a_5\\)\n\n\n\nЕсли элемент входит в данное множество, то мы говорим, что этот элемент принадлежит данному множеству, и записываем это следующим образом:\n\\[ a_i \\in A \\]\nСимвол \\(\\in\\) читается как «принадлежит».\nЕсли мы хотим задать множество через перечисление элементов, то можно это сделать с помощью фигурных скобок вот так:\n\\[ B = \\{ 0,1,2,3,4,5 \\} \\]\nВ данном случае множество \\(B\\) содержит 6 элементов — числа от нуля до пяти.\nПриведём примеры множеств.\n\nМножество букв русского алфавита: \\(L = \\{ а, б, в, г, д, \\dots, э, ю, я \\}\\)\nМножество всех натуральных чисел4: \\(\\mathbb{N}= \\{ 0, 1, 2, 3, \\dots \\}\\)\nМножество всех целых чисел: \\(\\mathbb{Z}= \\{0, 1, -1, 2, -2, 3, -3, \\dots \\}\\)\n\nТакже из числовых множеств мы можем вспомнить рациональные числа \\(\\mathbb{Q}\\), действительные (вещественные) числа \\(\\mathbb{R}\\) и комплексные числа \\(\\mathbb{C}\\).\nМы можем взять и рассмотреть не все элементы какого-то множества, а какую-то их часть. Например, взять элементы \\(a_1\\) и \\(a_2\\) и объединить их в множество поменьше.\n\n\n\n\nМножество \\(А_1\\), состоящее из элементов \\(a_1\\) и \\(a_2\\), является подмножеством множества \\(A\\)\n\n\n\nМы получим множество \\(A_1 = \\{a_1, a_2\\}\\), которое является подмножеством множества \\(A\\). Иначе говоря, множество \\(A_1\\) включается во множество \\(A\\):\n\\[\nA_1 \\subset A\n\\]\nВ частности, множество натуральных чисел включается во множество целых \\(\\mathbb{N}\\subset \\mathbb{Z}\\). А если продолжить эту цепочку, то можно получить следующую иерархию числовых множеств:\n\\[\n\\mathbb{N}\\subset \\mathbb{Z}\\subset \\mathbb{Q}\\subset \\mathbb{R}\\subset \\mathbb{C}\n\\]\nТривиальными подмножествами любого множества является пустое множество \\(\\varnothing\\) и само это множество. Пусть \\(M\\) — любое множество. Тогда можно записать два справедливых утверждения:\n\\[\n\\varnothing \\subset M\n\\]\n\\[\nM \\subset M\n\\]\n\n\n3.1.2.2 Операции над множествами\n\n3.1.2.2.1 Объединение\nНад множествами можно производить определённые операции. Во-первых, множества можно складывать, или объединять:\n\\[\nA + B = A \\cup B = \\{ x : x \\in A \\vee x \\in B \\}\n\\]\nТогда в новом множестве окажутся все элементы обоих исходных множеств.\n\n\n\n\nОбъединение множеств \\(A\\) и \\(B\\)\n\n\n\n\n\n3.1.2.2.2 Пересечение\nВо-вторых, множества можно умножать, или находить их пересечение:\n\\[\nA \\cdot B = A \\cap B = \\{ x : x \\in A \\wedge x \\in B \\}\n\\]\nТогда в новом множестве окажутся те элементы, которые принадлежат обоим множествам сразу.\n\n\n\n\nПересечение множеств \\(A\\) и \\(B\\)\n\n\n\n\n\n3.1.2.2.3 Разность множеств\nВ-третьих, можно искать разность множеств:\n\\[\nA ∖ B = \\{ x : x \\in A \\wedge x \\notin B \\}\n\\]\n\n\n\n\nРазность множеств \\(A\\) и \\(B\\)\n\n\n\nВ частности, если мы будем искать разность между универсумом \\(U\\) — множеством, которое содержит вообще все возможные элементы — и множеством \\(A\\), мы получим дополнение множества \\(A\\):\n\\[\nU ∖ A = A^c = \\bar A = \\{ x : x \\notin A \\}\n\\]\n\n\n\n\nДополнение множества \\(A\\)\n\n\n\n\n\n3.1.2.2.4 Симметрическая разность\nА ещё можно вычитать множества друг из друга, то есть искать их симметрическую разность:\n\\[\nA \\, \\Delta \\, B = ( A ∖ B ) \\cup ( B ∖ A ) = \\{ x : x \\in A \\text{XOR} \\x \\in B \\}\n\\]\n\n\n\n\nСимметрическая разность множеств \\(A\\) и \\(B\\)\n\n\n\n\n\n3.1.2.2.5 Декартово произведение\nНу, и самое красивое — декартово произведение двух множеств. Пусть у нас есть два множества \\(A\\) и \\(B\\). Тогда их декартово произведение представляет собой множество всех возможных упорядоченных пар \\((a,b), a \\in A, b \\in B\\).\n\\[\nA \\times B = \\{ (a,b) : a \\in A, b \\in B \\}\n\\]\n\n\n\n\nДекартово произведение множеств \\(A\\) и \\(B\\)\n\n\n\nУпорядоченность подразумевает, что если мы будем перемножать \\(A \\times B\\), то будут получаться пары \\((a,b)\\), а если \\(B \\times A\\), то пары \\((b,a)\\).\nК слову, вспомните координатную плоскость — это ни что иное, как декартово произведение двух координатных прямых: \\(\\mathbb{R}\\times \\mathbb{R}= \\mathbb{R}^2 = \\{ (x, y) : x \\in \\mathbb{R}, y \\in \\mathbb{R}\\}\\).\n\n\n\n3.1.2.3 Отображения\nМы можем сопоставлять элементы много множества элементам другого. Тогда мы получим отображение. Например, мы можем взять множество букв латинского алфавита и сопоставить каждому элменту этого множества число, которое будет отображать позицию буквы в алфавите. Пусть у нас есть множество букв латинского алфавита \\(L\\):\n\\[\nL = \\{ \\text{a}, \\text{b}, \\text{c}, \\text{d},\\dots, \\text{x}, \\text{y}, \\text{z} \\}\n\\]\nи множество натуральных чисел от 1 до 26 \\(N\\):\n\\[\nN = \\{ 1,2,3, \\dots,24,25,26 \\}\n\\]\nТогда мы можем задать такое отображение \\(F\\)\n\\[\nF : L \\rightarrow N,\n\\]\nгде каждой букве будет соответствовать её порядковый номер в алфавите.\nМы обозначили отображение буквой \\(F\\), которой в математике часто обозначают функции — это не случайно. Не вдаваясь в детали, можно сказать, что термины «отображение» и «функция» — синонимы. Ведь по сути что делает [математическая] функция? Сопоставляет между собой значения числовых множеств. То есть отображает одно числовое множество в другое. Множество \\(L\\) в примере выше будет областью определения функции (domain), а множество \\(N\\) — множеством её значений (range)3.\nНапример, простая функция \\(y = x\\), \\(x \\in \\mathbb{R}\\), \\(y \\in \\mathbb{R}\\) отображает множество вещественных чисел в само себя — \\(F : \\mathbb{R}\\rightarrow \\mathbb{R}\\). Функция модуля \\(y = |x|\\) отображает множество вещественных чисел во множество положительных вещественных чисел — \\(F : \\mathbb{R}\\rightarrow \\mathbb{R}_{+}\\). И так далее.\nВ случае с буквами мы могли бы задать функцию \\(f(l) = n\\), \\(l \\in L\\), \\(n \\in N\\) которая возвращала бы следующие результаты:\n\\[\nf(\\text{a}) = 1, \\quad f(\\text{b}) = 2, \\quad f(\\text{z}) = 26\n\\]\n\n\n\n\nОтображение \\(F : L \\rightarrow N\\)\n\n\n\n\n\n3.1.2.4 Мощность множества\nМножества могут содержать разное количество элементов. Характеристика, описывающая, сколько элементов содержит данное множество, называется мощностью множества.\nВо-первых, множества могут быть конечными и бесконечными.\n\nЕсли множество конечно, то его мощность равна количеству его элементов.\n\nНапример, множество очков, которое может выпать на стандартном игральном кубике — это \\(S_1 = \\{1,2,3,4,5,6\\}\\). Его мощность равна 6 — \\(|M| = 6\\).\nМножество значений пятибалльной шкалы Ликерта — это \\(S_2 = \\{1, 2, 3, 4, 5\\}\\). Его мощность равна 5 — \\(|S_2| = 5\\).\n\nЕсли множество бесконечно, то надо понять, насколько оно бесконечно.\n\nБесконечности бывают разного размера. В детали мы погружать не будем, однако отметим, что есть два вида бесконечностей.\n\nЕсли можно построить отображение, в котором каждом элементу некоторого множества \\(S\\) будет сопоставлено единственное натуральное число4, то такое множество называется счётным.\n\nЭто значит, что элементов во множестве \\(S\\) бесконечное количество — так как количество натуральных чисел бесконечно — однако при неограниченном количестве времени их все-таки можно пересчитать.\nМощность такого множетсва обозначается \\(\\aleph_0\\), то есть \\(|\\mathbb{N}| = \\aleph_0\\).\n\nЕсли количество элементов множества больше количества натуральных чисел, то такое множество обладает **мощностью континуума* \\(\\aleph_1\\).\n\nЭто множество будет равномощно множеству вещественных чисел \\(\\mathbb{R}\\).\n\n\nВозможно, это звучит весьма контринтуитивно — как одна бесконечность может быть больше другой? — однако это так: вещественных чисел больше, чем натуральных.\n\n\n\n3.1.3 Элементы комбинаторики\n\n3.1.3.1 Перестановки\nПредставим такую задачу: на черной пятнице мы накупили книг по анализу данных, и нам необходимо расставить их на полке. Всего у нас есть пять книг. Сколькими способами мы это сможем сделать?\nПодойдём к вопросу технически: возьмем и начнём расставлять. На первое место мы можем поставить любую из пяти книг, то есть вариантов заполнить первое место на полке — пять штук. Когда первое место заполнено, то вариантов заполнить второе место остаётся четыре. Всего возможных вариантов заполнения первых двух мест получается \\(5 \\times 4\\).\nЗаполняем далее: на третье место претендуют три оставшиеся книги, то есть вариантов, которыми мы можем расставить три книги — \\(5 \\times 4 \\times 3\\).\nСледуя далее этой логике мы получим, что всего возможных вариантов расставить все пять книг на полке будет\n\\[\n5 \\times 4 \\times 3 \\times 2 \\times 1 = 5!\n\\]\nМы получили формулу числа перестановок из \\(n\\) элементов:\n\\[\nP_n = n!\n\\]\nТо есть любые \\(n\\) объектов можно расставить на \\(n\\) мест \\(n!\\) способами.\n\n\n3.1.3.2 Размещения\n\n3.1.3.2.1 Без повторений\nТеперь представим, что у нас очень маленькая полка, и на ней умещается всего три книги. Сколькими способами мы можем заполнить такую полку, если всего в нашем распоряжении пять книг?\nВсего перестановок из пяти книг \\(5!\\), однако в силу того, что на полку умещаются только три первые книги из каждой перестановки, отличных друг от друга вариантов теперь будет меньше. Во сколько раз? В число раз, равно количеству перестановок из тех книг, которые на полку не помещаются. В нашем случае — \\((5 - 3)!\\).\nТо есть мы можем заполнить нашу полку \\(\\dfrac{5!}{(5-3)!}\\) способами.\nМы получили формулу для подсчета числа размещений (без повторений) из \\(n\\) элементов по \\(k\\) местам:\n\\[\nA_n^k = \\frac{n!}{(n-k)!}\n\\]\n\n\n3.1.3.2.2 С повторениями\nКаждой книги у нас по одному экземпляру, поэтому выше мы говорили о размещениях без повторений. Для полноты картины посмотрим на размещения с повторениями, хотя они встречаются в практике реже.\nТеперь у нас неограниченное количество копий каждой из книг, поэтому при размещении трех из пяти книг на полке все три могут оказаться одной и той же, или две одинаковые, а одна отличается и т.д. Суть — книги могут повторяться.\nПоскольку теперь, независимо от того, сколько книг мы уже поставили, у нас все равно осталось столько же, то есть 5 — ведь есть копии, каждый раз мы будем выбирать из пяти книг. Таких выборов нам нужно будет сделать три, так как три места на полке. Итого, заполнить нашу полку мы сможем \\(5^3\\) способами.\nМы получили формулу для подсчета числа размещений с повторениями из \\(n\\) элементов по \\(k\\) местам:\n\\[\n\\overline{A_n^k} = n^k\n\\]\n\n\n\n3.1.3.3 Сочетания\n\n3.1.3.3.1 Без повторений\nА теперь задача такова: нам не важно в каком порядке будут стоять книги на полке — нам нужно просто поставить три какие-то книги. Сколько возможно вариантов выбрать три книги из пяти?\nТак как мы теперь не учитываем порядок книг, то возможных вариантов будет в \\(3!\\) раз меньше, чем число размещений. Почему? Так как все перестановки этих трёх книг для нас теперь идентичны. Итого, всего вариантов выбрать три книги из пяти \\(\\dfrac{5!}{3!(5-3)!}\\).\nМы получили формулу для подсчета числа сочетаний из \\(n\\) элементов по \\(k\\):\n\\[\nС_n^k = \\frac{n!}{k!(n-k)!}\n\\]\nПоследняя формула на пригодится далее при обсуждении схемы испытаний Бернулли.\n\n\n3.1.3.3.2 С повторениями\nВновь для полноты картины посмотрим на случай с повторениями. У нас вновь есть неограниченное количество копий каждой книги, однако порядок выставления на полку нам не важен — как и было в сочетаниях. Количество сочетаний в этом случае будет равно\n\\[\n\\overline{С_n^k} = \\frac{(n+k-1)!}{k!(n-1)!}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>L3 // Математика для анализа данных</span>"
    ]
  },
  {
    "objectID": "l3.html#элементы-математического-анализа",
    "href": "l3.html#элементы-математического-анализа",
    "title": "3  L3 // Математика для анализа данных",
    "section": "3.2 Элементы математического анализа",
    "text": "3.2 Элементы математического анализа\nИз всего матана нам надо уловить два основных концепта — производную и интеграл. Эти и займёмся, захватив попутно немного пределов.\n\n3.2.1 Последовательности\nЧисловая последовательность — это последовательность чисел [внезапно]. В общем случае — любых. Она обозначается \\((x_n)^\\infty_{n=1}\\), где \\(x_n\\) — это некоторый элемент последовательности, а верхний и нижний индексы обозначают границы изменения индекса n. Например, \\(\\langle 1,-1,1,-1,\\dots \\rangle\\) — это числовая последовательность, которую можно обозначить \\(\\big( (-1)^n \\big)^\\infty_{n=1}\\).\nПоследовательность возникает на некотором множестве чисел. Если на таком множестве определено отношение порядка, то есть элементы этого множества можно сравнивать на «больше-меньше-равно», то можно сформировать монотонную последовательность. Это такая последовательность, которая не возрастает — то есть стоит на месте или убывает — или не убывает — то есть стоит на месте или возрастает. Более того, если существует такой объект (число), к которому элементы последовательности приближаются с ростом номер, то он является…\n\n\n3.2.2 Предел последовательности\n…пределом этой последовательности.\nРазберемся на примере. Пусть у нас есть вот такая простенькая последовательность:\n\\[\n\\Big( \\frac{1}{n} \\Big)^{\\infty}_{n=1} = \\Big \\langle 1, \\frac{1}{2}, \\frac{1}{3}, \\dots \\Big \\rangle\n\\]\nДостаточно очевидно, что каждый следующий её элемент, меньше предыдущего. Отрицательными элементы данной последовательности быть не могут, поэтому кажется, что всё идет к тому, что где-то там последовательность упрётся в ноль.\nФормально число \\(a\\) называется пределом последовательности \\(\\{x_n\\}\\), если для любого положительного числа \\(\\varepsilon\\) существует номер \\(N_\\varepsilon\\), такой что для любого \\(n &gt; N_\\varepsilon\\), выполняется равенство \\(|x_n - a| &lt; \\varepsilon\\), или на математическом:\n\\[\n\\lim_{n \\rightarrow \\infty} x_n = a \\Leftrightarrow \\forall \\varepsilon &gt; 0 \\exists N_\\varepsilon \\in \\mathbb{N}: n \\geq N_\\varepsilon, |x_n - a| &lt; \\varepsilon\n\\]\nТо есть, в случае нашей последовательности мы можем отсутупить на сколь угодно малое число \\(\\varepsilon\\) от нуля, и, начиная с какого-то номера, все элементы нашей последовательности окажутся в интервале \\((\\varepsilon, 0)\\). Поэтому \\(\\lim_{n \\rightarrow 0} \\frac{1}{n} = 0\\).\n\n\n3.2.3 Функции\nФункции (они же отображения, как мы выяснили выше) устанавливают соответствие между элементами двух множеств. Чаще всего мы имеет дело с числовыми функциями, то есть такими, которые ставят одни числа в соответствие другим. У любой функции есть область определения (множество \\(X\\)) и область значений (множество \\(Y\\)). Сама же функция представляет собой множество упорядоченных пар\n\\[\n(x,y) \\in X \\times Y,\n\\] таких что\n\nпары существуют для всех элементов \\(X\\), и\nесли первые элементы пар равны, то равны и их вторые элементы.\n\nПримерами функций могут быть \\(y = x^2\\), \\(y = kx + b\\), \\(y = |x|\\) и другие.\n\n\n\n\n\nПримеры функций\n\n\n\n\n\n3.2.3.1 Дискретные и непрерывные функции\nВ зависимости от того, какова область определение функции, то есть множество \\(X\\), функции могут быть дискретными и непрерывными. Например, если функция определена на множестве целых чисел \\(\\mathbb{Z}\\), то она будет дискретная, так как между, например, \\(1\\) и \\(2\\) будет пусто.\n\n\n\n\n\nПример графика дискретной функции\n\n\n\n\nЕсли функция определена на множестве \\(\\mathbb{R}\\), то она будет непрерывной. Например, функция \\(f(x) = x^2\\) является непрерывной, как ифункции \\(f(x) = \\sqrt{x}\\) и \\(f(x) = \\ln(x)\\). Если функция непрерывная, то она дифференцируема5.\n\n\n\n3.2.4 Производная\nА раз они дифференцируемы, то мы можем взять производную!\nПроизводная — очень полезная вещь. Во-первых, она показывает тангенс угла наклона касательной в данной точке, а во-вторых скорость и направление изменения функции в данной точке. На самом деле, и первое, и второе рассказывает нам примерно об одном и том же.\nДавайте издалека. Как нам узнать, куда двигается функция в данной точке?\nПусть дана функция \\(f(x) = 2x^3 + 3x^2-4x+6\\).\n\n\n\n\n\n\n\n\n\nВыберем точку \\(x_0\\), в которой мы хотим определить, куда и с какой скоростью движется наша функция. В этой точке функция имеет значение \\(y_0\\):\n\n\n\n\n\n\n\n\n\nШагнём на некоторую дистанцию \\(\\Delta x\\) вправо (по направлению оси \\(x\\)). Назовём эту дистанцию приращением аргумента. В точке \\(x_0 + \\Delta x\\) фунция будет иметь какое-то значение \\(y_0 + \\Delta y\\), где \\(\\Delta y\\) — приращение функции.\n\n\n\n\n\n\n\n\n\nНаша функция движется из точки \\((x_0,y_0)\\) в точку \\((x_0 + \\Delta x, y_0 + \\Delta y)\\). Имеем следующий треугольник — приблизим картинку:\n\n\n\n\n\n\n\n\n\nНас интересует угол \\(alpha\\) — именно он задает скорость и направление изменения функции. Если мы узнаем, каков угол \\(\\alpha\\) — а точнее \\(\\tan \\alpha\\), потому что так проще — то узнаем, куда движется функция.\n\\[\n\\tan \\alpha = \\frac{\\Delta y}{\\Delta x}\n\\]\nНу, хорошо. Но мы шагали достаточно далеко от точки, которая нас интесует. Если мы будем постепенно уменьшать шаг, то получим последовательность\n\\[\n\\langle \\tan \\alpha_1, \\tan \\alpha_2, \\tan \\alpha_3, ... \\rangle\n\\]\nУ этой последовательности есть предел, и если мы его рассчитаем, то как раз и получим значение производной в данной точке.\n\\[\nf'(x_0) = \\lim_{\\Delta x \\rightarrow 0} \\frac{\\Delta f_(x)}{\\Delta x} = \\frac{df}{dx}(x_0)\n\\]\nОчень маленькое приращение обозначается \\(dx\\) (или \\(df\\) , если это приращение функции). Вот мы и получили производную.\nМожно построить график производной. Это тоже будет функция.\nВажное свойство этой функции, которое нам понадобится в дальнейшем, заключается в том, что там, где график производной пересекает ось \\(x\\) — то есть там, где производная равна нулю — на исходной функции случаются точки смены монотонности — точки минимума и максимума.\n\n\n\n\n\n\n\n\n\nПроизводные основных элементарных функций можно найти тут.\n\n\n3.2.5 Частная производная\nМы хорошо знакомы с функциями одной переменной, где некий \\(y\\) зависит от некоего \\(x\\) и более ни от чего не зависит. Однако в общем случае никто нам не может помешать задать следующую функцию:\n\\[\nf(x, y) = 2x^2 + y^3\n\\]\nТеперь у нас две переменные — \\(x\\) и \\(y\\) — и от них обоих зависит значение функции. Это даже можно изобразить:\n\n\n\n\n\n\nВ общем-то наличие второй переменной практически ничего не меняет, однако появляется важная фича — мы можем смотреть, как изменяются значения функции при изменении каждой переменной в отдельности. Это позволяют делать частные производные.\nЧастные производные в целом беруться так же, как и обычные, только мы предполагаем, что все другие переменные — то есть те, по которым мы не берём производную в данный момент — это константы. Таким образом, мы как бы фиксируем другие переменные и получаем скорость изменения функции по какой-либо одной переменной.\nРассмотрим на примере нашей функции \\(f(x, y) = 2x^2 + y^3\\). Пусть мы хотим взять производную по \\(x\\). Тогда мы предполагаем следующее:\n\\[\ny = \\text{const}\\Rightarrow y^3 = \\text{const}= c\n\\]\nФцнкция примет следующий вид:\n\\[\nf(x, y) = 2x^2 + c,\n\\]\nа производная по \\(x\\) будет вычисляться следующим образом:\n\\[\n\\frac{\\partial f(x, y)}{\\partial x} = (2x^2 + c)' = (2x^2)' = 4x\n\\]\n\\(\\frac{\\partial f(x, y)}{\\partial x}\\) — это обозначение частной производной, аналогично тому, как мы обозначали производную через \\(\\frac{dy}{dx}\\).\nАналогично можно найти частную производную по \\(y\\):\n\\[\nx = \\text{const}\\Rightarrow 2x^2 = \\text{const}= c\n\\]\n\\[\n\\frac{\\partial f(x, y)}{\\partial y} = (c + y^3)' = (y^3)' = 3x^2\n\\]\nЗнакомство с вычислением частной производной понадобится нам, чтобы понять, как внутри устроена линейная регрессия и ухватить идею градиентного спуска.\n\n\n3.2.6 Интеграл\nИнтеграл — штука мощная, но нам он понадобится только с одной стороны. Нам надо будет искать площадь под кривой. Этим и займемся.\nПусть у нас есть функция \\(y = \\sqrt{x}\\). Нам надо найти площадь под её графиком на отрезке от \\(0\\) до \\(3\\).\n\n\n\n\n\n\n\n\n\nМы можем разбить этот отрезок на части размером \\(\\Delta x\\), а саму площадь на соответствующие прямоугольники. Это нам позволит оценить площадь. На рисунке ниже \\(\\Delta x = 0.25\\).\n\n\n\n\n\n\n\n\n\nПолучается, площадь можно оценить, сложив площади всех прямоугольников:\n\\[\nS \\approx \\sum_{i=1}^n y_i \\Delta x\n\\]\nЯсно, что чем более узкими прямоугольники у нас будут, тем точнее мы будем знать площадь. Ниже представлены рисунки для случая \\(\\Delta x = 0.1\\) и \\(\\Delta x = 0.05\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nЧтобы вычислить площадт точно, снова воспользуемся пределами, и определим с их помощью определенный интеграл:\n\\[\n\\lim_{\\Delta x \\rightarrow 0} \\sum_{i = 1}^n y_i \\Delta x = \\int_a^b \\sqrt{x}\\,dx\n\\]\nОпределенный он потому, что мы знаем, площадь в каких границах нас интересует. Определённый интеграл — это число.\n\\[\n\\int_0^3 \\sqrt{x} \\, dx \\approx 3.46\n\\]\nА раз есть определённый интеграл, значит есть и неопределённый. Рассмотрим следующую визуализацию:\n\n\n\n\nПостроение неопределенного интеграла\n\n\n\nМы смотрим, как изменяется площадь под графиком некоторой функции \\(f(x)\\) по мере нашего движения по оси \\(x\\) (нижний график), и строим график, по оси \\(y\\) которого расположена площадь под исходным графиком, левее данного \\(x\\). Этот график и отображает неопределенный интеграл, второе название которого первообразная. Неопределённый интеграл (первообразная) — это такая функция \\(F(x)\\), производная которой \\(F'(x)\\) равняется \\(f(x)\\), то есть исходной функции. Таким образом, справедливо равенство\n\\[\nF'(x) = f(x)\n\\]\nА также справедливо соответствие между определённым и неопредленным интегралом:\n\\[\n\\int_a^b f(x) dx = F(b) - F(a)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>L3 // Математика для анализа данных</span>"
    ]
  },
  {
    "objectID": "l3.html#элементы-линейной-алгебры",
    "href": "l3.html#элементы-линейной-алгебры",
    "title": "3  L3 // Математика для анализа данных",
    "section": "3.3 Элементы линейной алгебры",
    "text": "3.3 Элементы линейной алгебры\n\n«Увы, невозможно объяснить, что такое матрица. Ты должен увидеть это сам.» Морфеус (Матрица, 1999)\n\n\n3.3.1 Системы линейных уравнений\nЛинейная алгебра занимается решением систем линейных уравнений. Да в общем-то и все. В самом общем виде система линейных уравнений выглядит так:\n\\[\n\\cases{\na_{11}x_1 + a_{12}x_2 + \\cdots + a_{1m}x_m = b_1 \\\\\na_{21}x_1 + a_{22}x_2 + \\cdots + a_{2m}x_m = b_2 \\\\\n\\cdots \\\\\na_{n1}x_1 + a_{n2}x_2 + \\cdots + a_{nm}x_m = b_n \\\\\n}\n\\]\nВ данном случае это система из \\(n\\) уравнений с \\(m\\) неизвесными. Хотя такая запись математически предельно верна, выглядит она достаточно громоздко, и ею весьма трудно пользоваться. Поэтому обычно системы линейных уравнений записывают в матричном виде:\n\\[\n\\mathbf{A}\\mathbf{x} = \\mathbf{b}\n\\]\nСогласитесь, читать такое значительно проще. Здесь \\(\\mathbf{A}\\) — это матрица коэффициентов системы, \\(\\mathbf{x}\\) — вектор неизвестных, а \\(\\mathbf{b}\\) — вектор свободных членов системы. Внутри они устроены вот так:\n\\[\n\\mathbf{A} = \\pmatrix{\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n}\n\\quad\n\\mathbf{x} = \\pmatrix{x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m}\n\\quad\n\\mathbf{b} = \\pmatrix{b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n}\n\\]\nРазберемся с этими структурами подробнее.\n\n\n3.3.2 Векторы\nКак мы уже знаем, вектор — это набор чисел. В зависимости от того, как он расположен, он может быть вектором-строкой (\\(\\mathbf{r}\\)) или вектором-столбцом (\\(\\mathbf{c}\\)):\n\\[\n\\mathbf{r} = \\pmatrix{r_1 & r_2 & r_3 & \\dots & r_n}\n\\] \\[\n\\mathbf{c} = \\pmatrix{c_1 \\\\ c_2 \\\\ c_3 \\\\ \\vdots \\\\ c_m}\n\\]\nГеометрически элементы вектора (числа) являются его координатами в некотором \\(n\\)-мерном пространстве. О переходе от вектора как направленного отрезка к вектору как набору чисел мы говорили в первой лекции.\nПосмотрим, что можно делать с векторами на примере векторов \\(\\mathbf{v}\\) и \\(\\mathbf{w}\\), в которых есть по \\(n\\) элементов.\n\\[\n\\mathbf{v} = \\pmatrix{v_1 & v_2 & v_3 & \\dots & v_n}\n\\]\n\\[\n\\mathbf{w} = \\pmatrix{w_1 & w_2 & w_3 & \\dots & w_n}\n\\]\n\n3.3.2.1 Сложение векторов\nВекторы одинакоых размерностей можно складывать друг с другом:\n\\[\n\\mathbf{v} + \\mathbf{w} = \\pmatrix{v_1 + w_1 & v_2 + w_2 & v_3 + w_3 & \\dots & v_n + w_n}\n\\]\n\n\n3.3.2.2 Умножение вектора на число\nВектор можно умножать на произвольное вещественное число:\n\\[\n\\alpha \\cdot \\mathbf{v} = \\pmatrix{\\alpha \\cdot v_1 & \\alpha \\cdot v_2 & \\alpha \\cdot v_3 & \\dots & \\alpha \\cdot v_n}, \\quad \\forall \\alpha \\in \\mathbb{R}\n\\]\n\n\n3.3.2.3 Скалярное произведение векторов\nВекторы одинаковых размерностей можно скаларярно умножить друг с другом. Скаларное произведение векторов определено как сумма произведений их соответствующих координат:\n\\[\n\\mathbf{v} \\cdot \\mathbf{w} = v_1 w_1 + v_2 w_2 + v_3 w_3 + \\dots + v_n w_n\n\\]\nТаким образом, в результате скалярного произведения векторов получается одно число.\n\n\n\n3.3.3 Матрицы\nКак мы уже знаем, матрица — это двумерный массив чисел. Своего рода табличка. Соответственно, в матрице есть строки и столбцы. Это значит, что одну и ту же матрицу можно записать по-разному. Например, матрицу \\(\\mathbf{A}\\), с которой мы встречались выше, можно записать:\n\nуказав все элементы:\n\n\\[\n\\mathbf{A} = \\pmatrix{\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n}\n\\]\n\nзадав её через векторы-столбцы:\n\n\\[\n\\mathbf{A} = \\pmatrix{\n\\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\\\\n}\n\\]\n\nзадав её через векторы-строки:\n\n\\[\n\\mathbf{A} = \\pmatrix{\n\\mathbf{a_1} \\\\ \\mathbf{a_2} \\\\ \\cdots \\\\ \\mathbf{a_n}\n}\n\\]\nВсе три способа записи обозначают один и тот же объек и используются в зависимости от того, какой способ рассмотрения матрицы в данный момен удобнее.\nМатрица характеризуется, прежде всего, размером. В размере сначала указывается количество строк, затем — количество столбцов. Рассматриваемая нами матрица \\(\\mathbf{A}\\) имеет размер \\(n \\times m\\), что можно записать как \\(\\mathbf{A}_{n \\times m}\\).\n\nЕсли количество строк и столбцов в матрице совпадает, она называется квадратной.\nВ матрице есть главная диагональ — слева сверху вправо вниз — и побочная диагональ — справа сверху влево вниз.\nКвадратная матрица, все элементы которой, кроме стоящий на главной диагонали, равны нулю, называется диагональной матрицей.\nЕсть две замечательные матрицы: единичная \\(\\mathbf{I}\\) (или \\(\\mathbf{E}\\)) и нулевая \\(\\mathbf{O}\\).\n\n\\[\n\\mathbf{I} = \\mathbf{E} = \\pmatrix{\n1 & 0 & \\dots & 0 \\\\\n0 & 1 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 1 \\\\\n}\n\\quad\n\\mathbf{O} = \\pmatrix{\n0 & 0 & \\dots & 0 \\\\\n0 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 0 \\\\\n}\n\\]\n\n3.3.3.1 Сложение матриц\nДве матрицы одинакового размера можно складывать друг с другом:\n\\[\n\\mathbf{A}_{n \\times m} + \\mathbf{B}_{n \\times m} = \\pmatrix {\na_{11} + b_{11} & a_{12} + b_{12} & \\dots & a_{1m} + b_{1m} \\\\\na_{21} + b_{21} & a_{22} + b_{22} & \\dots & a_{2m} + b_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} + b_{n1} & a_{n2} + b_{n2} & \\dots & a_{nm} + b_{nm} \\\\\n}\n\\]\nСвойства сложения матриц:\n\nКоммутативность: \\(\\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}\\)\nАссоциативность: \\((\\mathbf{A} + \\mathbf{B}) + \\mathbf{C} = \\mathbf{A} + (\\mathbf{B} + \\mathbf{C})\\)\nСуществование нулевого элемента: \\(\\mathbf{A} + \\mathbf{O} = \\mathbf{A}\\)\nСуществование противоположного элемента: \\(\\mathbf{A} + (-\\mathbf{A}) = \\mathbf{O}\\)\n\n\n\n3.3.3.2 Умножение матрицы на число\nЛюбую матрицу можно умножить на вещественное число:\n\\[\n\\alpha \\cdot \\mathbf{A} = \\pmatrix{\n\\alpha \\cdot a_{11} & \\alpha \\cdot a_{12} & \\alpha \\cdot \\cdots & a_{1m} \\\\\n\\alpha \\cdot a_{21} & \\alpha \\cdot a_{22} & \\cdots & \\alpha \\cdot a_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\alpha \\cdot a_{n1} & \\alpha \\cdot a_{n2} & \\cdots & \\alpha \\cdot a_{nm}\n} \\quad \\forall \\alpha \\in \\mathbb{R}\n\\]\n\n\n3.3.3.3 Матричное умножение\nШтош, приступим.\n\nМатрицы можно матрично перемножить друг с другом, если у них совпадают внутренние размерности.\nРезультатом перемножения \\(\\mathbf{A}_{n \\times k} \\times \\mathbf{B}_{k \\times m}\\) является матрица \\(\\mathbf{C}_{n \\times m}\\).\nЭлемент \\(c_{ij}\\) матрицы \\(\\mathbf{C}\\) равен скалярному произведению \\(i\\)-го вектора-строки матрицы \\(\\mathbf{A}\\) и \\(j\\)-го вектора-столбца матрицы \\(\\mathbf{B}\\).\n\nВизуально:\n\n\n\n\nПринцип матричного умножения\n\n\n\nТеперь попробуем это расписать. Пусть есть две матрицы \\(\\mathbf{A}_{n \\times k} \\times \\mathbf{B}_{k \\times m}\\), которые выглядят следующим образом:\n\\[\n\\mathbf{A} =\n\\pmatrix{\\mathbf{a}_1 \\\\ \\mathbf{a}_2 \\\\ \\vdots \\\\ \\mathbf{a}_n} =\n\\pmatrix{\na_{11} & a_{12} & \\dots & a_{1k} \\\\\na_{21} & a_{22} & \\dots & a_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\dots & a_{nk}\n}\n\\]\n\\[\n\\mathbf{B} =\n\\pmatrix{\\mathbf{b}_1 & \\mathbf{b}_2 & \\dots & \\mathbf{b}_n} =\n\\pmatrix{\nb_{11} & b_{12} & \\dots & b_{1m} \\\\\nb_{21} & b_{22} & \\dots & b_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{k1} & b_{k2} & \\dots & b_{km}\n}\n\\]\n\\[\n\\mathbf{A} \\times \\mathbf{B} = \\pmatrix{\n\\mathbf{a}_1 \\cdot \\mathbf{b}_1 & \\mathbf{a}_1 \\cdot \\mathbf{b}_2 & \\dots & \\mathbf{a}_1 \\cdot \\mathbf{b}_m \\\\\n\\mathbf{a}_2 \\cdot \\mathbf{b}_1 & \\mathbf{a}_2 \\cdot \\mathbf{b}_2 & \\dots & \\mathbf{a}_2 \\cdot \\mathbf{b}_m \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{a}_n \\cdot \\mathbf{b}_1 & \\mathbf{a}_n \\cdot \\mathbf{b}_2 & \\dots & \\mathbf{a}_n \\cdot \\mathbf{b}_m \\\\\n} =\n\\]\n\\[\n= \\pmatrix{\n(a_{11}b_{11} + a_{12}b_{21} + \\dots + a_{1k}b_{k1}) & (a_{11}b_{12} + a_{12}b_{22} + \\dots + a_{1k}b_{k2}) & \\dots & (a_{11}b_{1m} + a_{12}b_{2m} + \\dots + a_{1k}b_{km}) \\\\\n(a_{21}b_{11} + a_{22}b_{21} + \\dots + a_{2k}b_{k1}) & (a_{21}b_{12} + a_{12}b_{22} + \\dots + a_{2k}b_{k2}) & \\dots & (a_{21}b_{1m} + a_{22}b_{2m} + \\dots + a_{2k}b_{km}) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n(a_{n1}b_{11} + a_{n2}b_{21} + \\dots + a_{nk}b_{k1}) & (a_{n1}b_{12} + a_{n2}b_{22} + \\dots + a_{nk}b_{k2}) & \\dots & (a_{n1}b_{1m} + a_{n2}b_{2m} + \\dots + a_{nk}b_{km}) \\\\\n} =\n\\]\n\\[\n= \\pmatrix{\nc_{11} & c_{12} & \\dots & c_{1m} \\\\\nc_{21} & c_{22} & \\dots & c_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nc_{n1} & c_{n2} & \\dots & c_{nm}\n} = \\mathbf{C}\n\\]\n\\[\nc_{ij} = \\sum_{t=1}^k a_{it}b_{tj}\n\\]\nСвойства матричного умножения:\n\nАссоциативность: \\(\\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}\\) и \\(\\alpha(\\mathbf{A}\\mathbf{B}) = (\\alpha\\mathbf{A})\\mathbf{B} = \\mathbf{A}(\\alpha\\mathbf{B})\\)\nДистрибутивность: \\(\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{B}\\mathbf{C}\\) и \\((\\mathbf{A} + \\mathbf{B})\\mathbf{C} = \\mathbf{A}\\mathbf{C} + \\mathbf{B}\\mathbf{C}\\)\nОтсутствие коммутативности: в общем случае \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\)\nУмножение на единичный элемент: \\(\\mathbf{I}\\mathbf{A} = \\mathbf{A}\\), \\(\\mathbf{A}\\mathbf{I} = \\mathbf{A}\\)\nУмножение на нулевой элемент: \\(\\mathbf{O}\\mathbf{A} = \\mathbf{O}\\), \\(\\mathbf{A}\\mathbf{O} = \\mathbf{O}\\)\nУмножение на обратный элемент — выполняется только для некоторых квадратных матриц (см. ниже): \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\)\n\n\n\n3.3.3.4 Транспонирование матрицы\nЭто очень простая операция — строки и столбы матрицы меняются местами:\n\\[\n\\mathbf{A} =\n\\pmatrix{\\mathbf{a}_1 & \\mathbf{a}_2 & \\dots & \\mathbf{a}_n} =\n\\pmatrix{\na_{11} & a_{12} & \\dots & a_{1k} \\\\\na_{21} & a_{22} & \\dots & a_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\dots & a_{nk}\n}\n\\]\n\\[\n\\mathbf{A}^T =\n\\pmatrix{\\mathbf{a}_1 \\\\ \\mathbf{a}_2 \\\\ \\vdots \\\\ \\mathbf{a}_n} =\n\\pmatrix{\na_{11} & a_{21} & \\dots & a_{n1} \\\\\na_{12} & a_{22} & \\dots & a_{n2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1k} & a_{2k} & \\dots & a_{nk}\n}\n\\]\nЕсли размер исходной матрицы был \\(n \\times k\\), то размер транспонированной матрицы будет \\(k \\times n\\).\n\n\n3.3.3.5 Определитель и обратная матрица\nОпределитель (детерминант) матрицы \\(\\det \\mathbf{A}\\), \\(|\\mathbf{A}|\\), \\(\\Delta A\\) — это величина, которая может быть вычислена и поставлена в соответствие квадратной матрице. Он «определяет» свойства матрицы, в том числе одно из ключевых — её обратимость.\nВычисление детерминанта матрицы в общем случае является достаточно сложной задачей и требует введения дополнительных определений, поэтому ограничимся интерпретацией его значений:\n\nЕсли матрица не является квадратной, то детерминант не определен.\nЕсли детерминант матрицы не равен нулю, то:\n\nсистема линейных уравнений, задаваемая данной матрицей имеет единственное решение\nдля данной матрицы существует обратная матрица, обладающая следующим свойством — \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\\).\n\nЕсли детерминан матрицы равен нулю, то система линейных уравнений имеет несколько решений.\n\nОдной из причин может быть линейная зависимость между столбцами или строками матрицы — то есть ситуация, при который один из столбцов (одна из строк) линейной выражается через другой (другую).\n\n\n\n\n3.3.3.6 След матрицы\nСлед матрицы — это сумма элементов главной диагонали (квадратной) матрицы.\n\\[\n\\text{tr}(\\mathbf{A}) = \\sum_i a_{ii}\n\\]\n\n\n\n\nЗегет, В. 1985. Элементарная Логика. Москва: Высшая школа.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>L3 // Математика для анализа данных</span>"
    ]
  },
  {
    "objectID": "l4.html#l4_measures",
    "href": "l4.html#l4_measures",
    "title": "4  L4 // Теория измерений",
    "section": "4.1 Измерения",
    "text": "4.1 Измерения\n\n\n\n\n\n\nNote\n\n\n\nЗдесь мы будем говорить об измерениях в социальных науках. Они имеют определенную спефицику по сравнению, скажем, с физическими измерениями.\n\n\nНачнем с наиболее общего определения измерения.\nИзмерение — процедура приписывания определенным психологическим объектам определенных чисел на определенной шкале.\n\nТехнически звучит не сложно.\nНо часто приходится доказывать, что мы реально что-то померили и сделали это адекватно.\n\nТем не менее, такое общее опреление измерения позволяет нам говорить, что измерить мы может всё, что угодно:\n\nрост\nвозраст\nпол\nнациональность\nколичество детей в семье\nрейтинг студентов\nкурс / уровень обучения\nгеографические координаты (долгота и широта)\nтемпература\nдата\nIQ\nнарциссизм / макиавеллизм / психопатия\nвремя реакции\nточность ответов испытуемого в эксперименте\nи т.д.\n\n\n4.1.1 Какие существуют измерения в разных областях психологии?\nПодходы к измерениям можно поделить по отраслям (областям) психологической науки1:\n\nНейронаука и психофизиология\n\nРегистрируются физиологические (= физические) процессы\nПроцессы [как правило] являются реакциями на физическую стимуляцию\nПредполагается, что эти физиологические процессы являются коррелятами некоторых психических процессов\n\\(\\Phi \\rightarrow \\Phi (\\sim \\Psi)\\)\n\nПсихофизика\n\nИзмеряются субъективные феномены (реакции, ощущения, пороги)\nСубъективные реакции происходят на физическую стимуляцию\n\\(\\Psi \\rightarrow \\Phi\\)\n\nПоведенческие исследования\n\nИзмеряются поведенческие реакции с помощью объективных метрик (времени реакции, точности кликов, последовательности поиска)\nПоведенческие реакции [как правило] обусловлены физической стимуляцией\nПредполагается, что за поведенческими реакциями стоят некоторые психические процессы\n\\(B \\rightarrow \\Psi\\)\n\nПсихометрика\n\nИзмеряются субъективные феномены, не связанные напрямую с физиологическим процессами\nЧасто (= всегда) изучаются гипотетические конструкты\nИзмерение происходит с помощью субъективных методик\n\\(\\Psi \\rightarrow \\Psi\\)\n\n\nИз подобного методологического безобразия происходят две важных мысли:\n\nВ любой области психологической науки нам необходимо с теоретических позиций обосновать связь измеряемых в ходе исследования переменных с изучаемыми психическими феноменами.\nПри любом измерении нам необходимо выбрать адекватный способ числового отражения изучаемых феноменов, чтобы мы могли использовать статистические методы анализа.\n\nТаким образом, если мы уточним в свете последней важной мысли определение измерения, то оно будет звучать так:\nИзмерение — это процедура приписывания психологическим объектам чисел таким образом, чтобы отношения между числами соответствовали отношениям между психологическими объектами2.\nДействительно, с числами можно делать всё, что угодно, что не запрещено математикой — а не запрещено ею много чего, однако не любые математические операции имеют смысл по отношению к исходными психологическим объектам. Из этих ограничений возникают шкалы.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>L4 // Теория измерений</span>"
    ]
  },
  {
    "objectID": "l4.html#l4_scales",
    "href": "l4.html#l4_scales",
    "title": "4  L4 // Теория измерений",
    "section": "4.2 Шкалы",
    "text": "4.2 Шкалы\nМожно найти много технических определений шкалы, одно из которых звучит весьма красиво:\n\nШкала — это числовая структура, изоморфная эмпирической структуре.\n\nНо такое определение мало что проясняет относительно содержания шкалы. Для нас удобнее будет менее точное, но более осязаемое понимание:\n\nШкала — это набор чисел с органичениями на допустимые по отношению к ним операции.\n\nВ таком понимании ещё С. Стивенсом в 1946 году были предложены четыре вида шкал — это классификацией мы пользуемся до сих пор:\n\nНоминальная шкала (шкала наименований, nominal scale)\nПорядковая шкала (ранговая шкала, ordinal scale)\nИнтервальная шкала (шкала разностей, interval scale)\nАбсолютная шкала (шкала отношений, ratio scale)\n\nШкалы отличаются друг от друга по математическому содержанию используемых на них чисел, допустимым на них математическим (и логическим) операциям и преобразованиям, наличию и характеру нуля, типу шкалы и типу данных.\n\n4.2.1 Признаки и переменные\nВ ходе исследования мы измеряем различные признаки изучаемых объектов. Попытаемся эти признаки как-то систематизировать. В терминах данных признаки — это переменные, поэтому далее мы будем чаще употребляться именно этот термин — переменная — имея в виду то, что мы намерили, изучая интересующий нас признак изучаемого объекта. Во многом признак и переменная — это синонимы, только первый термин больше из теории измерений, а второй из статистики и анализа данных. Измерение же от отдельного человека / объекта выборки называется наблюдение. В общем-то с этим мы уже сталкивались, когда обсуждали данные.\nИтак, типы переменных:\n\nКоличественные переменные — те, которые принимают числовые значения. Они могут быть:\n\nнепрерывными — принимают любые значения (рост, возраст, время реакции и др.)\nдискретными — могут принимать только определенные значения (количество детей в семье, число отчисленных студентов, количество пачек гречи, которое человек скупил на карантине и др.)\n\n\nЧисло, приписываемое количественному признаку (переменной) ведёт себя как привычное нам математическое число в том смысле, что выражает некоторое количество — сантиметров, лет, секунд, детей, студентов, пачек гречи…\n\nНоминальные (категориальные) переменные — используются для разделения наших наблюдений на группы (пол, национальность, курс обучения, используемая операционная ситема компьютера и др.)\n\nЗаписаны эти переменные обычно текстом (скажем, пол — male и female или операционная система — Win, MacOS, Linux). Однако, например, курс обучения можно записать по-разному: текстом — freshman, sophomore, junior, senior — и числом — 1, 2, 3, 4. Однако в данном случае цифры не несут никакого математического смысла — это просто лейблы, с помощью которых мы различаем группы наблюдений. Ведь и пол мы можем записать с помощью чисел — пусть male = 0, female = 1. Ведь не будем же мы складывать-вычитать девушек и парней?\nВнимательный читатель мог заметить, что курс обучения это не совсем категориальная переменная, ведь «второкурсник» в каком-то смысле «больше», чем «первокурсник». Но мы не можем сказать «на сколько» или «во сколько» больше! Что же делать?\n\nНельзя сказать, что «второкурсник» выражает большую выраженность признака «год обучения», чем «первокурсник».\nВместе с тем «второкурсник» дольше учился и освоил больше дисциплин, чем «первокурсник». При этом «третькурсник» учился дольше «второкурсника». То есть есть порядок категорий.\nТакая переменная называется ранговой.\n\nДругой пример рагновой переменной — это студенческий рейтинг. Что делает рейтинг? Упорядочивает студентов. Можно ли сказать, что четвертый в рейтинге студент в два раза менее успешен, чем второй? Нет — тот же GPA может отличаться на десятые или сотые доли.\nИтого, переменные:\n\nколичественные\n\nнепрерывные\nдискретные\n\nранговые\nноминальные\n\n\n4.2.1.1 Виды шкал\nОт того, в какой шкале измерена переменная, которую мы исследуем, будет зависеть:\n\nкакие графики мы сможем нарисовать\nкакие статистики на ней имеют смысл\nкакие статистические модели дадут адекватный результат\n\nВ общем, почти весь анализ определяется тем, с какой шкалой мы работаем, поэтому разберем каждую шкалу подробнее.\n\n\n4.2.1.2 Номинальная шкала\n\nНаименее мощная шкала\nНеметрическая — расстояния между делениями не определены\nТип данных — категориальные\nДопустимые операции\n\nсравнение на (не)равенство\n\nНоль — отсутствует\nДопустимые преобразования — любое, сохраняющее взаимно однозначное соответствие\n\nВ этой шкале, что весьма ожидаемо, измеряются номинальные переменные. Даже если на этой шкале используются числа для задания категорий, они не несут никакого математического смысла, что следует из допустимых операций данной шкалы.\nСтоит отдельно оговорить, что значит преобразование, сохраняющее взаимно однозначное соответствие. Пусть у нас есть самая типичная социально-демографическая номинальная переменная исследований — пол. И пусть он у нас закодирован как male и female. Мы можем преобразовать эту переменную как угодно. Единственное условие, которое у нас есть — это возможность опознать мужчин и женщин по присвоенным лейблам. Так, мы можем использовать числа 1 и 0 или 618 и 1040, задать текстовые лейблы m и f или м и ж, или даже выдумать что-то ещё типа gfbc и rtsu. Последний вариант технически совершенно не удобен, но устройства шкалы его вполне допускает, так как сохранено взаимно-однозначное соответствие между реальным объектами и используемыми лейблами.\n\n\n4.2.1.3 Порядковая шкала\n\nНеметрическая — расстояния между делениями не равны между собой\nТип данных — категориальные / ранговые\nДопустимые операции\n\nсравнение на (не)равенство\nсравнение на больше-меньше\n\nНоль — отсутствует\nДопустимые преобразования — любое монотонное\n\nНа этой шкале появляется порядок значений, а значит и операция сравнения на больше-меньше. Нет делений — вернее, даже если есть, то они разного размера — поэтому складывать и вычитать ещё нельзя.\nПреобразование на этой шкале должно сохранять порядок её значений, так как этой ключевая характеристика данной шкалы. Так, в принципе мы можем извлечь квадратный корень из переменной уровень обучения (бакалавриат, специалитет, магистратура, аспирантура), которая закодирована как 1, 2, 3, 4 — получится 1, 1.4, 1.7, 2. Это нам усложнит жизнь, несомненно, однако шкалу не сломает — порядок элементов сохранен.\n\n\n\n\n\n\nО термине «качественные данные».\n\n\n\nПочему-то номинальную и ранговую шкалы в литературе часто называют «качественными». Видимо, потому что качественные данные обычно рассматриваются как оппозиция количественным.\nЭто в некоторой мере справедливо, поскольку есть два типа исследований — качественные и количественные. Они различаются методологией и используемыми методиками и, как следствие, собираемыми данными.\nВ рамках качественных исследований чаще всего собираются тексты, поэтому во многом качественные данные по факту обычно текстовые. Количественные данные — это, как правило, таблицы с цифрами из любой из четырёх шкал. Безусловно, анализ качественных и количественных данных тажке существенно различается.\nИтого, кажется, называть «качественными» номинальную и ранговые шкалы — странно, потому что качественные данные — это неструктурированный текст. Лучше их именовать категориальными. Правда, например, рейтинг студентов (ранговая шкала) тоже не совсем категориальные данные… ай, ладно — будут ранговые!\n\n\n\n\n4.2.1.4 Интервальная шкала\n\nМетрическая — расстояния между делениями одинаковые\nТип данных — количественные\nДопустимые операции\n\nсравнение на (не)равенство\nсравнение на больше-меньше\nсложение и вычитание\n\nНоль — относительный\nДопустимые преобразования — любое линейное\n\nНа этой шкале появляется возможность складывать и вычитать, так как есть точка отсчета — ноль — и деления становятся одинакового размера. Правда выбран этот ноль случайно, поэтому он не отражает полное отсутствие признака у изучаемого объекта. По этой причине операции умножения и деления на этой шкале невозможны.\nКоличество возможных преобразований также сокращается — теперь при преобразовании шкалы нам важно сохранить равенство интервалов. Этому требованию соответствуют линейные преобразования, так как они выполняют условия линейности:\n\\[\nf(x+y) = f(x) + f(y)\n\\] \\[\nf(\\alpha x) = \\alpha f(x)\n\\]\nТак, квадратный корень из значений шкалы извлечь уже не получится, потому что равенство интервалов нарушится.\n\n\n4.2.1.5 Абсолютная шкала\n\nСамая мощная шкала\nМетрическая — расстояния между делениями одинаковые\nТип данных — количественные\nДопустимые операции\n\nсравнение на (не)равенство\nсравнение на больше-меньше\nсложение и вычитание\nумножение и деление\n\nНоль — абсолютный\nДопустимые преобразования — любое преобразование подобия\n\nНаличие абсолютного нуля на данной шкале позволяет производить с её значениями все математические операции. Однако это же существенно ограничивает набор допустимых преобразований — теперь нам важно сохранять этот самый абсолютный ноль, поэтому невозможно прибавить или вычесть какие-либо число из всех значений шкалы (сдвинуть её вправо или влево, вверх или вниз). Получается, можно только умножить или разделить шкалу на некоторое значение, что и является преобразованием подобия.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>L4 // Теория измерений</span>"
    ]
  },
  {
    "objectID": "l4.html#психометрические-измерения",
    "href": "l4.html#психометрические-измерения",
    "title": "4  L4 // Теория измерений",
    "section": "4.3 Психометрические измерения",
    "text": "4.3 Психометрические измерения\nВыше мы уже выяснили, что психометрические измерения особо выделяются среди всех других измерений в психологических науках. Еще раз обозначим, почему:\n\nИзмеряются ненаблюдаемые (латентные) конструкты с помощью субъективных шкал\n\nне знаем достоверно, существуют ли наши конструкты\n\nПоследствия\n\nВ измерениях всегда есть существенная доля ошибки → нужно знать надёжность (точность) измерения\nМы не всегда уверены, действительно ли измерили то, что хотим → нужно обосновать валидность измерения\n\nОдного вопроса обычно недостаточно, чтобы задать содержание конструкта или добиться нужной точности измерения\n\n\n\n\n4.3.1 Операциональная классификация методик\nВстает вопрос, а как мы вообще можем измерять в психометрике что-либо?\nОперациональная классификация распределяет методики в зависимости от того, насколько результат их процедуры зависит от субъективного опыта респондента и самого диагноста.\n\nВыше черты — прямые методы\n\nОтветы респондента используются для интерпретации напрямую и обычно количественно\n\nНиже черты — непрямые методы\n\nОтветы респондента интерпретируются не напрямую и во многом методами качественного анализа\n\n\nПолучается следующая картина:\n\nПриборные психофизиологические методики\nАппаратурные поведенческие методики\nОбъективные тесты с выбором ответа (тесты способностей или тесты знаний)\nТесты-опросники (прямой субъективный самоотчет)\nСубъективное шкалирование\n\n———————————————————————————\n\nПроективные методики\nНаблюдение\nКонтент-анализ\nПсихологическая беседа\nРолевая игра\nОбучающий эксперимент\n\nЗдесь, конечно, методики взяты шире, чем обычно используются в психометрике. Чаще всего, сталкиваясь с психометрикой, мы имеет дело с объективными тестами3 и тестами-опросниками.\nОтдельный вопрос касается тестов-опросников — почему прямой субъективный самоотчет вообще работает? Здесь есть две ключевых идеи:\n\nВновь С. Стивенс нам сказал, что люди могут прямо оценивать интенсивность стимулов, приписывая им числа. Тем самым, он легитимизировал прямой субъективный самоотчет.\nКогда мы говорим об исследовательской работе, мы проводим психометрические измерения и диагностику в ситуации, которая предполагает свободное дальнейшее поведение4, то есть респонденты сами решают, как им обойтись с результатами тестирования — поэтому искажения предполагаются минимальными.\n\n\n\n4.3.2 Психометрика в двух словах\nТак или иначе, мы оказывается в потрясающей ситуации:\n\nизмеряем непонятно что, непонятно чем и непонятно как\n\n\nС одной стороны, кажется, что ситуация довольно безвыходная, однако психометрики придумали множество инструментов, чтобы совладать с подобным стечением объстоятельств. С другой стороны, если вы не занимаетесь психометрикой непосредственно и оказались к ней критически близко в силу необходимости использования психометрических инструментов в собственном исследовании, стоит понимать, что психометрика — это обычная научная область, со своими особенностями, проблемами и кризисами, которая она пытается решать, как и любая другая. И это окей.\n\n\n4.3.3 Концепт. Конструкт. Операционализация\nЧтобы начать конструировать психометрический инструмент, прежде всего необходимо определить, что мы собираемся измерять, поэтому первым шагом является определение измеряемого конструкта.\nКонструкт — это прямым образом ненаблюдаемая переменная, характеризующая различия в поведении людей в специфической группе ситуаций (Messick (1993), Barrett (2005)).\n\nхарактеристика, навык, способность человека, которую мы хотим оценить или измерить\nбазируется на одной или нескольких теориях\nне может быть измерен непосредственно, но с помощью различных индикаторов или переменных\nможет быть простым и сложным\n\nНапример, простыми конструктами будут:\n\nПринадлежность к политической партии\nСтаж\nУмение умножать\nДетский эгоцентризм (по Пиаже)\nЗнание букв\n\nСложными же могут выступить:\n\nУдовлетворенность работой\nМатематическая грамотность\nУчебная мотивация\nКоммуникация\nНавык достижения цели\n\nЗа конструктом может стоять ещё более обобщенная идея, задающая теоретическу рамку, в которой определяется конструкт — концепт.\nКонцепт — это обобщенная идея, разделяемая многими людьми (сообществом), которая может быть представлена в рамках той или иной теории или подхода.\nСуществуют некоторые различия в том, как смотрят на измеряемые конструкты в образовательном и психологическом тестировании:\n\nПсихологическое тестирование\n\nИзмерение какого-либо латентного конструкта\n\nопределение конструкта, основанное на теориях и / или исследованиях\nоперационализация конструкта\nобласти содержания\n\n\nОбразовательное тестирование\n\nОбразовательные результаты (освоение программы, курса, года, и т. д.)\n\nсоответствует ФГОС\nсоответствует учебному плану / программе\nотражает цели обучения\n\n\n\nПосле определения конструкта наступет этап операционализации:\n\nопределение конструкта в терминах операций, необходимых для его измерения (Machery (2007))\nпроцесс и документ, описывающий переход от теоретического, абстрактного понятия к наблюдаемому поведению, измеряемому в тесте\n\nОперационализация подразумевает разработку:\n\nсубконструктов и их взаимосвязей,\nгрупп ситуаций, в которых они проявляются,\nспособов сбора информации об их проявлении (Mislevy, Almond, Lukas (2003), Brennan (2006))\n\n\n\n4.3.4 Тестовые задания. Области содержания конструкта. Структура опросника\nКогда операционализация разработана, наступает этап разработка тестовых заданий. Мы не будем останавливаться здесь на принципах их разработки — это отдельная большая область с массой нюансов и деталей. Остановимся на двух важных вещах.\n\nПри разработке пунктов опросника должны быть учтены области содержания конструкта — ситуации и контексты, в которых он может проявляться\n\nЧем шире конструкт, тем в большем количестве контекстов он может проявляться и тем больше областей содержания должен охватывать опросник\n\nВ структуру психометрического инструмента может входит несколько субшкал или несколько субтестов\n\nЕсли опросник состоит из нескольких субшкал, то его можно использоваться только как единый психометрический инструмент — не допускается использование отдельных субшкал опросника при сборе данных.\nЕсли опросник состоит из нескольких субтестов, то возможно использование отдельных субтестов при сборе данных.\n\n\n\n\n4.3.5 Виды шкал в психометрических инструментах\nВ классификации шкал психометрических инструментов частично дублируется классификация измерительных шкал, однако добавляются и новые основания.\n\nПо характеру отношений\n\nнеметрические (номинальная и порядковая)\nметрические (разностей и отношений)\n\nПо числовому соответствию\n\nдискретные\nнепрерывные (континуальные)\n\nПо наличию и/или смыслу полюсов\n\nбиполярные\nуниполярные\n\nПо материалу\n\nграфические\nтекстовые\nчисловые\n…\n\n\nВид используемой шкалы зависит от целевой аудитории теста (дети, взрослые, клиническая выборка и др.), диагностической ситуации, особенностей измеряемого конструкта, теоретических основания и т.д.\nКлассическая психометрическая шкала — это шкала Ликерта. Она обладает следующими характеристиками:\n\nбиполярная\nзаданы текстом все альтернативы\nравные интервалы [визуально]\nгоризонтальная\nданы целые числа [от 1 до 5]\n\n\n\n4.3.6 Психометрические характеристики шкалы\n\nНадежность — это мера свободы результатов от ошибки измерения (standard error of measurement, SEM).\nНадежность нельзя рассчитать напрямую — можно только аппроксимировать\nРазными методами — ни один из них не является полностью верным\nНо нам приходится с этим как-то жить\n\nМетоды расчета надежности:\n\nCronbach’s \\(\\alpha\\) — надежность-внутренняя согласованность\nМетод расщепленных половин\nРетестовая надежность\n\n\n\n4.3.7 Психометрические характеристики пунктов\n\n4.3.7.1 Трудность задания\n\nДля дихотомического случая\n\n\\[\nb_j = \\frac{s_{1j} \\cdot p_{1j} + s_{2j} \\cdot p_{2j}}{N_j} = \\frac{0 \\cdot p_{1j} + 1 \\cdot p_{2j}}{N_j} = \\frac{p_{2j}}{N_j}\n\\]\n\nДля политомического случая\n\n\\[\nb_j = \\frac{\\sum_{k=1}^{K_j}(s_{kj} \\cdot p_{kj})}{N_j \\cdot s_{Kj}}\n\\]\nИнтерпретация значений трудности:\n\nЧем выше показатель трудности, тем легче справиться с заданием\nСлишком трудные — [0.00, 0.05] — и слишком легкие — [0.95, 1.00] — задания плохо дифференцируют выборку\nНачинать кодировку ответов лучше с 0 — так проще жить и интерпретировать результаты\n\n\n\n4.3.7.2 Дискриминативность задания\n\nТест направлен на измерение некоторого конструкта\nСуммарный тестовый балл отражает выраженность конструкта\nЧем сильнее коррелирует балл по заданию с баллом по тесту, тем лучше задание различает респондентов\nЛучше использовать скорректированную меру — корреляцию балла по заданию с суммой баллов по всем другим заданиям\n\n\n\n\n4.3.8 Валидность\nВалидность — это соответствие результатов тестирования заявленной цели тестирования, в частности, тому психическому свойству (или свойствам), которое измеряется.\n\nВ широком смысле — сведения о поведении и психических явлениях, находящихся в причинной зависимости от диагностируемого свойства.\nАналогично можно говорить о валидности тестового задания.\n\n\nГлавная цель разработки психометрического инструмента — сбор как можно большего количества разных свидетельств валидности.\n\n\n\nСбор разных свидетельств валидности обеспечивает обоснованный вывод о том, что по результатам теста можно выносить соответствующие суждения о тестируемых.\n\n\n\n4.3.8.1 Виды валидности\n\n4.3.8.1.1 Концептуальная валидность\n\nобоснование тестовой методики с позиций соответствия авторским (теоретическим) представлениям об особенностях диагностируемых свойств\nмера соответствия содержания заданий теста авторской концепции этих свойств.\n\n\n\n4.3.8.1.2 Конструктная валидность\n\nопределяет область теоретической структуры психологических явлений, измеряемых тестом\nтест, базирующийся на развитой, логически-связной теории, обеспеченной высоко-операционализированными понятиями, обладает конструктной валидностью\n\nВиды конструктной валидности:\n\nвнутренняя валидность / надежность–внутренняя согласованность\nдифференциальная валидность\nконвергентная валидность\nдискриминантная валидность\n\n\n\n4.3.8.1.3 Внутренняя валидность\n\nподчиненность пунктов (заданий, вопросов) теста основному направлению теста как целого\nориентированность пунктов на изучение одних и тех же конструктов\nанализ осуществляется путем коррелирования ответов на каждое задание с общим результатом теста\nдинамика изучаемого конструкта\n\n\n\n4.3.8.1.4 Дифференциальная валидность\n\nвнутренние взаимоотношения между диагностируемыми факторами\nтесты интересов\n\nобычно умеренного коррелируют с показателем общей академической успеваемости\nно связаны с успеваемостью по отдельным дисциплинам\nособенно важна как показатель диагностической ценности методик в профотборе\n\n\n\n\n4.3.8.1.5 Эмпирическая валидность\n\nсовокупность характеристик валидности теста, полученных экспериментально-статистическим способом\n\nкритериальная\n\nтекущая / диагностическая / конкурентная\nретроспективная\nпрогностическая\n\nконвергентная\nдискриминантная\nконцессуальная\n\n\n\n\n4.3.8.1.6 Конвергентная валидность\n\nстепень соответствия баллов двух тестовых методик, направленных на измерение одного и того же или концептуально-родственных конструктов\nзначимая корреляция между тестами\n\n\n\n4.3.8.1.7 Дискриминантная валидность\n\nстепень, в которой тест не измеряет тот конструкт, для измерения которого он не предназначен\nотсутствие значимой корреляции между тестовыми показателями, отражающими концептуально независимые свойства\nчастный случай — отсутствие корреляции с переменными приводящими к фальсификации или мотивационным искажениям результата\n\nнапример, социальная желательность\n\n\n\n\n4.3.8.1.8 Критериальная валидность\n\nотражает соответствие результатов тестирования определенным значениям критериальной переменной или вероятности критериального события\nнезависимые от результатов теста непосредственные меры исследуемого качества\n\nуровень достижения в чем-либо\nстепень развития способности\nвыраженность определенного свойства личности\nпоказатели социально- или производственно-значимых результатов деятельности\n\n\nТекущая (конкурентная)\n\nкритериальное событие происходит сейчас, в момент исследования\n\nРетроспективная\n\nкритериальное событие уже произошло\n\nПрогностическая\n\nкритериальное событие будет потом\nнужен квазиэксперимент\n\n\n\n4.3.8.1.9 Концессуальная валидность\n\nустановлении связи (корреляции) тестовых данных с данными, полученными от внешних экспертов\nэксперты хорошо знакомы с тестируемыми\n\n\n\n4.3.8.1.10 Очевидная валидность (face validity)\n\nнасколько сам тест и его задания кажутся респондентам подходящими для цели тестирования\nвысокая очевидная валидность присуща также кейс-тестам.\nочень часто не совпадает с научной концепцией валидности\nвысокая очевидная валидность часто является весьма желательной\nфактор, побуждающий респондента к сотрудничеству, серьезному и ответственному отношению к выполнению заданий и к восприятию результатов оценки\n\n\n\n4.3.8.1.11 Содержательная валидность\n\nстепень соответствия содержания заданий теста той реальной деятельности, в которой проявляется измеряемое психическое свойство\n\nучебные тесты, тесты профессиональных достижений\n\nмного разнородных, факторов — личностные особенности, знания, умения и навыки, специальные способности — нужна адекватная модель тестируемой деятельности\n\nподбор заданий, охватывающие главные аспекты изучаемого феномена в правильной пропорции к реальной деятельности в целом\n\nавторское обоснование пригодности теста в самом содержании тестовых заданий\n\n\n\n4.3.8.1.12 Факторная валидность\n\nподтверждения теоретической структуры конструкта, разработанной в ходе операционализации, эмпирическими данными, собранными в ходе количественной апробации методики\nпроводится с помощью [конфирматорного] факторного анализа\n\n\n\n\n\nBarrett, P. 2005. “What If There Were No Psychometrics?: Constructs, Complexity, and Measurement.” Journal of Personality Assessment 85 (2): 134–40.\n\n\nMessick, S. 1993. “Foundations of Validity: Meaning and Consequences in Psychological Assessment.” Ets Research Report Series 1993 (2): i–18.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>L4 // Теория измерений</span>"
    ]
  },
  {
    "objectID": "l5.html#введение-в-математическую-статистику",
    "href": "l5.html#введение-в-математическую-статистику",
    "title": "5  L5 // Введение в статистику. Случайный эксперимент и случайные величины",
    "section": "5.1 Введение в математическую статистику",
    "text": "5.1 Введение в математическую статистику\nСтатистика — это междисциплинарная область знаний, а также практической деятельности, изучающая массовые явления, а также прицнипи и методы работы с данными, характеризующими эти явления.\nМассовые явления затрагивают огромные массы людей. Огромность масс, конечно, различна. Скажем, базовые перцептивные закономерности, связанные с тем, как устроена зрительная система, охватывают всех людей. Уровень удовлетворенности жизнью россиян охватывает только население России. Городские блага москвичей — только для жителей Москвы. Учебная мотивация студентов департамента психологии НИУ ВШЭ — это только про людей с психологических бакалариата и магистратур НИУ ВШЭ.\n\n5.1.1 Генеральная совокупность\nГенеральная совокупность (population) — множество всех [существующих] исследуемых объектов и сведений о них.\nОбъем генеральной совокупности (\\(N\\)) — число единиц, образующих генеральную совокупность.\nГенеральная совокупность недоступна для изучения в полном объеме, так как \\(N\\) имеет порядок сотен, тысяч или даже миллионов… Поэтому в рамках исследований мы всегда работаем с выборкой.\n\n\n5.1.2 Выборка\nВыборка, или выборочная совокупность (sample) — множество объектов генеральной совокупности, объемом \\(n\\) (\\(n \\ll N\\)).\n\n5.1.2.1 Репрезентативность выборки\nСтатистика даёт нам теоретический и математический аппарат, который позволяет делать выводы о генеральной совокупности по выборке. Однако если мы криво собрали данные, то никакая математика нас не спасет от некорректных выводов.\n\nGarbage in, garbage out.\n\nРепрезентативность — степень соответствия характеристик выборки характеристикам генеральной совокупности.\nНапример, мы хотим исследовать связь учебной мотивации и академической успеваемости бакалавров психологии. Если мы соберем данные только со своих однокурсников, будет нехорошо, так как в нашу выборку не попали (1) другие курсы психологического бакалавриата нашего вуза, (2) бакалавры-психологи других вузов Москвы и (3) бакалавры-психологи вузов других городов России.\nИ так работает всегда.\nНу, почти. Есть соблазн проводить исследования на студентах-психологах, потому что они достаточно близко и их можно загнать на эксперименты за баллы. Более-менее сносно это может работать на каких-то базовых когнитивных феноменах из восприятия и памяти. Обычно у нас нет оснований предполагать, что восприятие и память работают по-разному у людей разного возраста и разных социальных страт. Но вот уже с мышлением возникают проблемы.\nПочему выборка должна быть репрезентативной?\nПотому что если мы делаем нормально, то хотим обобщать результаты нашего исследования, полученные на выборке, на генеральную совокупность. Если выборка нерепрезентативна, то мы не можем этого сделать. Зачем в таком случае проводить исследование — решительно неясно.\n\n\n5.1.2.2 Как набрать репрезентативную выборку\n\nОсознать, кто наша генеральная совокупность, так как для каждой генеральной совокупности репрезентативная выборка будет своя.\nПонять, есть ли какая-то группировка, важная для нашего исследования, в нашей генеральной совокупности — социальная страта, специальность образования, сфера работы, пол / гендер, возрастные группы, регион проживания, семейное положение, что-либо ещё.\nРассчитать достаточный объём выборки. Это не самая простая задача и о ней мы будем говорить отдельно. Пока отметим в назывном порядке, что на объем выборки будут влиять дизайн исследования, изменчивость признака, уровень значимости и размер эффекта [что бы это ни значило].\nОбеспечить случайное попадание респондентов в выборку.\n\nЗдесь надо остановиться подробнее. Если у нас есть ресурсы набрать много человеков в выборку (скажем, раза в 2–3 больше, чем достаточный объем выборки), то можно просто случайным образом откуда-то доставать людей — и всё будет хорошо. Закон больших чисел и центральная предельная теорема говорят, что наша выборка будет репрезентативной. Пока примем это как данность, позже поговорим об это подробнее.\n\n\nДля интересующихся\n\nЗакон больших чисел\n\nС увеличением числа случайных величин их среднее арифметическое стремится к среднему арифметическому математических ожиданий и перестает быть случайным. Общий смысл закона больших чисел — совместное действие большого числа случайных факторов приводит к результату, почти не зависящему от случая.\n\nТаким образом, закон больших чисел гарантирует устойчивость для средних значений некоторых случайных событий при достаточно длинной серии экспериментов.\nЦентральная предельная теорема\n\nРаспределение случайной величины, которая получена в результате сложения большого числа независимых случайных величин (ни одно из которых не доминирует, не вносит в сумму определяющего вклада и имеет дисперсию значительно меньше по сравнению с дисперсией суммы) имеет распределение, близкое к нормальному.\n\nИз ЦПТ следует, что ошибки выборки также подчиняются нормальному распределению.\n\n\nЕсли мы всё же не можем набрать много человеков, то надо набрать выборку достаточного объема и проверить репрезентативна ли она — отражает ли выборка те группировки объектов, которые есть в генеральной совокупности.\nИдеальная выборка — это когда каждый человек имеет равную вероятность попасть в число респонтентов / испытуемых. Полностью случайный отбор трудно достижим — это очень дорого и логистически сложно — но к нему нужно стремиться. Сам метод сбора данных может деформировать выборку (например, онлайн опросы отсекают пенсионеров), поэтому думать о сборе данных необходимо уже на этапе планирования исследования.\n\n\n5.1.2.3 Способы формирования репрезентативной выборки\n\nЗдесь представлены три классических способа формирования выборки. В конкретном исследовании мы можем использовать и какие-либо другие способы формирования выборки, однако нам нужно будет обосновать, почему с нашем случае тот или иной способ позволяет собрать репрезентативную выборку.\n\n\n5.1.2.3.1 Простая случайная выборка (simple random sample)\n\nЭлементы генеральной совокупности случайным образом попадают в выборку\nС увеличением объема простая случайная выборка будет все больше напоминать генеральную совокупность по своим характеристикам.\n\nПредставим, что на этой картинке изображена вся генеральная совокупность:\n\n\n\n\n\n\n\n\n\nЕсли мы наберем простую случайную выборку из этой генеральной совокупности, она будет выглядеть так (черные точки):\n\n\n\n\n\n\n\n\n\nКак можно заметить, в выборку попали объекты из всех частей нашей генеральной совокупности — говорит о том, что выборка репрезентативна.\n\n\n5.1.2.3.2 Стратифицированная выборка (stratified sample)\n\nГенеральная совокупность разбивается на несколько обособленных различных по своей природе групп (страт). Например, по полу или уровню образования\nИз каждой группы случайным образом выбираются несколько объектов, которые попадают в выборку.\n\nПусть в нашей генеральной совокупности есть четыре страты:\n\n\n\n\n\n\n\n\n\nТогда мы можем разделить её на четыре «генеральных совокупности» соответственно:\n\n\n\n\n\n\n\n\n\nИз каждой такой «генеральной совокупности» будем извлекать случайную выборку:\n\n\n\n\n\n\n\n\n\n\n\n5.1.2.3.3 Групповая выборка (cluster sample)\n\nГенеральная совокупность разбивается на несколько обособленных, но одинаковых групп (кластеров). Например, население города группируется по району проживания\nВыбираются случайным образом несколько групп\nИз каждой группы случайным образом выбираются несколько объектов, которые попадают в выборку.\n\nПусть мы разделили нашу генеральную совокупность на 8 кластеров:\n\n\n\n\n\n\n\n\n\nКластеры у нас примерно одинаковые по характеристикам между собой — по крайне мере, мы так предполагаем. Выберем случайно четыре кластера, которые примут участие в исследовании:\n\n\n\n\n\n\n\n\n\nТеперь из этих кластеров наберем выборку (допустим, по 20 наблюдений из кластера) случайным образом (объекты, попавшие в итоговую выборку отмечены черным контуром, не попавшие — серым):\n\n\n\n\n\n\n\n\n\nТакой подход к формированию выборки позволяет экономить драгоценные ресурсы при проведении исследования.\n\nПодробнее про разные способы сбора выборки можно почитать тут.\n\n\n\n\n\n5.1.3 Характетистики объектов выборки и генеральной совокупности\nОбъекты генеральной совокупности обладают определенными признаками, которые мы и хотели бы изучать. Признаки количественно выражены в определенных показателях. Например,\n\n\n\n\n\n\n\nПризнак\nПоказатель\n\n\n\n\nРабочая память\nОбъем рабочей памяти\n\n\nНейротизм\nУровень нейротизма по BFI\n\n\nДоход\nСовокупный годовой доход после уплаты налогов\n\n\nКогнитивная нагрузка\nУровень когнитивной нагрузки по ЭЭГ-коррелятам\n\n\nДоверие к ИИ\nУровень доверия к ИИ по опроснику TAIA\n\n\nИндивидуализм/коллективизм\nИндекс индивидуализма/коллективизма по модели Хофстеде\n\n\n\nПризнаки могут быть очень разными и измеряться могут с помощью разных показателей.\nНезависимо от того, как измеряется признак, генеральная совокупность характеризуется параметром.\nПараметр (\\(\\theta\\)) — относительно постоянная [от одной совокупности к другой] величина, харакретизующая генеральную совокупность по некоторому показателю.\nНу, то есть в принципе существует средний уровень нейротизма по BFI студента-психолога или индекс индивидуализма/коллективизма для конкретной культуры. Проблема в том, что величина параметра, который мы изучаем, неизвестна. И никогда не будет известна.\nНо почему?\n\nМы не можем изучать всю генеральную совокупность — слишком много объектов\nНаши измерения всегда содержат ошибку — мы даже длину линейкой точно не можем измерить, что уж о психологических измерениях говорить\n\nПоэтому величину параметра мы можем только предсказать с определённой статистической точностью. Измеряя что-либо на выборке, мы получаем выборочную характеристику, или оценку (\\(\\hat \\theta\\)) — эмпирический (измеримый) аналог параметра.\n\n\n5.1.4 Характеристики статистических данных\nИтак, теперь задумаемся о том, что мы собираем на выборке некоторые данные. К каким их особенностям приведут все моменты, описанные выше?\n\nМы не можем работать с генеральной совокупностью, поэтому набираем выборку — выборки между собой имеют право различаться\nКаждый респондент или испытуемый обладает своими особенностями — мы не знаем, что мы получим в результате конкретного измерения на конкретном изучаемом объекте1\nЛюбое наше измерение содержит ошибку — ни один измерительный инструмент не является совершенным\n\nВсё это приводит нас к двум ключевым характеристикам статистически данных — неопределенности и вариативности.\n\nНеопределённость нам говорит о том, что мы не знаем, что именно мы получим в результате наших измерений для конкретной выборки.\n\nОтсюда чуть позже возникнут случайные величины.\n\nВариативность означает, что наши данные будут различатся от выборки к выборке и от респондента к респонденту\n\nОтсюда возникнут статистические критерии, в которых эта характеристика данных будет учтена в ходе тестирования гипотез.\n\n\n\n\n5.1.5 Зачем нужна статистика?\nМы в какой-то малоприятной ситуации… Мы пытаемся измерить то, что в определенном смысле невозможно измерить, при этом достаточно точно, чтобы потом это можно было сравнивать или строить какие-то модели. Задача выглядит заведомо провальной…\nОднако именно в этот момент на помощь нам приходит статистика. Не в гордом одиночестве, конечно. Она проводит с собой теорию измерений, психометрику, теорию обнаружения сигнала и др. Всё это работает в нашей психологической науке в комлексе.\nСтатистика даёт нам теоретический и математический инструментарий, чтобы мы могли делать какие-либо выводы по нашим собранным данным. К сожалению, как бы нам не хотелось, мы не можем делать выводы по сырым данным, потому что измерения по выборке не отражают вот прям ровно то, что есть в генеральной совокупности. Нам их надо определенным образом обсчитать, чтобы наши выводы были корректными. Этим и занимается статистика.\nВозможно, это звучит достаточно абстрактно, но я хочу, чтобы на данном моменте вы поймали некоторое интуитивное понимание того, зачем нужна статистика. Далее это обрастёт содержанием и уложится, я надеюсь, в достаточно стройную систему.\nИтог — статистика помогает нам делать выводы о нашей генеральной совокупности по выборке.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L5 // Введение в статистику. Случайный эксперимент и случайные величины</span>"
    ]
  },
  {
    "objectID": "l5.html#случайный-эксперимент",
    "href": "l5.html#случайный-эксперимент",
    "title": "5  L5 // Введение в статистику. Случайный эксперимент и случайные величины",
    "section": "5.2 Случайный эксперимент",
    "text": "5.2 Случайный эксперимент\nОтвлечемся немного на любимый объект статистиков — игральный кубик.\n\n\n\n\n\n\nБросание игрального кубика — это случайный эксперимент.\nВыпавшее число — это случайная величина.\n\nТеперь более строго.\nСлучайный эксперимент — это математическая модель некоторого реального эксперимента, результат которого невозможно точно предсказать.\nКлючевой момент в случайном эксперименте — это то, что его результат невозможно точно предсказать, то есть какой стороной упадёт кубик заранее неизвестно.\nЭто не соотносится с экспериментом как методом исследования. Эксперимент как метод исследования включает в себя сложную процедуту экспериментального воздействия, контроля систематических и несистематических смешений, манипуляции с независимыми переменными и фиксирование зависимых переменных и т.д. Случайный эксперимент же является частью эксперимента.\nПосмотрим на примеры из психологического поля:\n\nответ респондента на пункт (айтем) опросника — это случайный эксперимент\n\nнеизвестно, какой балл выберет респондент\n\nклик на стимул на экране в эксперименте на зрительный поиск — это случайный эксперимент\n\nзаранее неизвестно, когда точно испытуемый кликнет по стимулу\n\nзапись ЭЭГ-активности в конкретный момент времени — это случайный эксперимент\n\nнеизвестно, что мы зафиксируем в конкретный момент\n\nи т.д.\n\nОтсюда мы делаем важный вывод: любой акт измерения — это [с точки зрения статистики] случайный эксперимент.\n\n5.2.1 Модель случайного эксперимента\nНа модель случайного эксперимента накладывается ряд требований:\n\nадекватность описания реального эксперимента (в нашем случае, акта (момента) измерения)\nопределение совокупности наблюдаемых результатов случайного эксперимента (при фиксированных начальных данных)\nпринципиальная возможность осуществления эксперимента со случайным исходом сколько угодно большое количество раз (при фиксированных начальных данных)\nстохастическая устойчивость относительной частоты для любого наблюдаемого результата\n\nДля того, чтобы это всё понять, нужно ввести некоторые концепты.\n\n\n5.2.2 Элементарные исходы и события\nВ случайном эксперименте возможны различные исходы, называемые элементарными событиями (\\(\\omega_i\\)). Например, в случае упомянутого выше игрального кубика при его бросании возможны шесть элементарных событий (исходов):\n\n\\(\\omega_1\\) — выпала грань с одной точкой\n\\(\\omega_2\\) — выпала грань с двумя точками\n\\(\\omega_3\\) — выпала грань с тремя точками\n\\(\\omega_4\\) — выпала грань с четырьмя точками\n\\(\\omega_5\\) — выпала грань с пятью точками\n\\(\\omega_6\\) — выпала грань с шестью точками\n\nМножество всех элементарных событий называется пространством элементарных событий (\\(\\Omega\\)) случайного эксперимента.\nВ случае игрального кубика можно записать так:\n\\[\n\\Omega = \\{\\omega_1, \\omega_2, \\omega_3, \\omega_4, \\omega_5, \\omega_6\\}\n\\]\nВ общем случае, когда возможны \\(n\\) случайный исходов случайного эксперимента, пространство элементарных событий будет выглядеть так:\n\\[\n\\Omega = \\{\\omega_1, \\omega_2, \\dots, \\omega_{n-1}, \\omega_n\\}\n\\]\nАналогично, в случае, когда случайным экспериментом будет ответ респодента на пункт опросника по пятибалльной шкале Ликерта, пространство элементарных событий будет выглядеть так:\n\\[\n\\Omega = \\{\\omega_1, \\omega_2, \\omega_3, \\omega_4, \\omega_5\\},\n\\]\nгде\n\n\\(\\omega_1\\) — дан ответ «1» / «не согласен»\n\\(\\omega_2\\) — дан ответ «2» / «скорее, не согласен»\n\\(\\omega_3\\) — дан ответ «3» / «ни то, ни другое»\n\\(\\omega_4\\) — дан ответ «4» / «скорее, согласен»\n\\(\\omega_5\\) — дан ответ «5» / «согласен»\n\nВ пространстве элементарных событий определяются случайные события — любое подмножество множества элементарных событий. Например, для игрального кубика\n\nслучайное событие «выпало четное число очков» соответствует множеству \\(A_{\\text{even}} = \\{\\omega_2, \\omega_4, \\omega_6\\}\\)\nслучайное событие «выпало нечетное число очков» соответствует множеству \\(A_{\\text{odd}} = \\{\\omega_1, \\omega_3, \\omega_5\\}\\)\nслучайное событие «выпала грань с тремя точками» соответствует множеству \\(A_3 = \\{\\omega_3\\}\\)\nслучайное событие «не выпало ни одной грани» соответствует множеству \\(A_0 = \\varnothing\\) — такое событие называется невозможным\nслучайное событие «выпала любая грань» соответствует множеству \\(A_{\\text{any}} = \\Omega = \\{\\omega_1, \\omega_2, \\omega_3, \\omega_4, \\omega_5, \\omega_6\\}\\) — такое событие называется достоверным\n\nОбратите внимание, что \\(A_{\\text{even}} \\subset \\Omega\\), \\(A_{\\text{odd}} \\subset \\Omega\\), \\(A_3 \\subset \\Omega\\), \\(A_0 \\subset \\Omega\\) и \\(A_{\\text{any}} \\subset \\Omega\\), что как раз и утверждает определение случайного события.\nВсё множество случайных событий \\(A_i\\) обозначается2 \\(\\mathcal A\\).\n\nЕсли пространство элементарных событий конечно или счетно, то оно называется дискретным.\nЕсли пространство элементарных событий недискретно и элементарными исходами являются числа, то оно называется непрерывным.\n\n\n\n5.2.3 Вероятность\nОкей, мы ввели пространство элементарных событий \\(\\Omega\\) и множество случайных событий \\(\\mathcal A\\). Однако этого оказывается недостаточно, чтобы работать с результатами случайного эксперимента. Так как исход случайного эксперимента невозможно точно предсказать, необходимо ввести меру, которая будет описывать возможность наступления того или иного события. Такая мера называется вероятностью.\nВероятность (\\(\\mathbb{P}\\))— относительная мера возможности наступления некоторого события в результате случайного эксперимента.\n\n5.2.3.1 Классическая вероятность\nВернемся вновь к игральному кубику. Напомним себе, что мы определили пространство элементарных событий для случайного эксперимента «бросание игрального кубика» следующим образом:\n\\[\n\\Omega = \\{\\omega_1, \\omega_2, \\omega_3, \\omega_4, \\omega_5, \\omega_6\\}\n\\]\nЗададим следующие ограничения, чтобы было удобнее работать с чиселками:\n\nвероятность достоверного события должна равняться единице — \\(\\mathbb{P}(\\Omega) = 1\\)\nвероятность невозможного события должна равняться нулю — \\(\\mathbb{P}(A_0) = 0\\)\n\nПредполагая, что кубик честный, то есть выпадение каждой грани равновозможно, можно определить вероятность выпадения каждой грани как\n\\[\n\\mathbb{P}(\\omega_i) = \\frac{1}{n},\n\\]\nгде \\(n\\) — количество всех возможных элементарных исходов случайного эксперимента. Получается, вероятность выпадения каждой грани — \\(\\frac{1}{6}\\).\nАналогично можно определить вероятность любого случаного события \\(A_i\\):\n\\[\n\\mathbb{P}(A_i) = \\frac{n_i}{n},\n\\]\nгде \\(n_i\\) — количество элементарных исходов, составляюших событие \\(A_i\\), а \\(n\\) — количество всех возможных элементарных исходов случайного эксперимента. Получается, что вероятность выпадения четного числа очков \\(\\mathbb{P}(A_\\text{even}) = \\frac{3}{6} = \\frac{1}{2}\\), что достаточно логично.\nТакой подход к вероятности называется классической вероятностью. Возвращаясь к требованиям модели случайного эксперимента, можно заключить, что такая модель (1) адекватно описывает реальный эксперимент, так как предсказанные вероятности согласуются с наблюдениями, (2) определяет совокупность наблюдаемых результатов случайного эксперимента (\\(\\Omega\\) и \\(\\mathcal A\\)) и допускает принципиальную возможность осуществления случайного эксперимента сколь угодно большое количество раз.\nТройку \\((\\Omega, \\mathcal A, \\mathbb{P})\\) называются вероятностным пространством.\n\n\n5.2.3.2 Статистическая вероятность\nС игральным кубиком классическая вероятность прекрасно работает. Однако задумается о следующем случае: какова вероятность встретить динозавра на улице?\nВоспользуемся классическим подходом к вероятности. Зададим пространство элементарных событий, которое будет выглядеть так: \\(\\Omega = \\{\\omega_1, \\omega_2\\}\\), где \\(\\omega_1\\) — встреча с динозавром случилась, а \\(\\omega_2\\) — встреча с динозавром не случилась.\nПолучается, что вероятность встретить динозавра на улице равняется\n\\[\n\\mathbb{P}(\\omega_1) = \\frac{1}{n} = \\frac{1}{2}\n\\]\nТо есть, каждый второй день мы по пути на работу должны сталкиваться с каким-нибудь жителем триаса. Однако наши наблюдательные данные говорят, что этого не происходит. Получается, такая модель неадекватна реальности. В чем тут дело?\nКлассическая вероятность исходит из допущения равновозможности наступления любого элементарного исхода. Данное допущение в случае с динозавром нарушается. Чтобы корректно определить вероятность в данном случае, необходимо использовать статистическую вероятность.\nФормально статистическая вероятность определяется как предел частоты наблюдений некоторого события при стремлении количества наблюдений к бесконечности [при их независимости (наблюдения не влияют друг на друга) и однороности (условия наблюдений одинаковы)]:\n\\[\n\\mathbb{P}(A) = \\lim_{N \\rightarrow \\infty} \\frac{n}{N},\n\\]\nгде \\(N\\) — количество наблюдений, а \\(n\\) — количество наступлений события \\(A\\).\nТо есть, чтобы оценить вероятность некотрого события, надо многократно повторить случайный эксперимент (провести большое количество наблюдений), посчитать, в скольки наблюдениях наступило интересующее нас событие, и поделить одно на другое. И чем больше наблюдений мы проведем, тем более точну оценку вероятности мы получим.\nТакой подход дает адекватное описание происходящему в реальном мире — вероятность встретить динозавра оказывается равной (крайне близкой) к нулю.\nОтметим, что этот подход работает и с игральным кубиком: совершив много бросков кубика мы сможем выяснить, что вероятность выпадения каждой грани — \\(\\frac{1}{6}\\), при условии, что кубик честный (что является допущением классической вероятности).\nМы будем опираться на статистический подход, поскольку в реальной исследовательской работе мы обычно имеем дело именно с неравновозможными событиями.\n\n\n5.2.3.3 Геометрическая вероятность\nПосмотрим еще на один подход к определению вероятности. Поставим себе весьма абстрактную, но веселую задачу.\nВозьмем квадрат, в который вписан круг:\n\n\n\n\n\n\n\n\n\nБудем бросать в этот квадрат точки случайным образом:\n\n\n\n\n\n\n\n\n\nВопрос: какова вероятность, что случайно брошенная точка попадет в круг (событие \\(A\\))?\nМы можем воспользоваться статистическим подходом к вероятности, набросать побольше точек и посчитать, сколько из них попало в пределы круга:\n\n\n\n\n\n\n\n\n\nИз визуализации видно, что в конечном итоге при очень большом количестве бросаний точек они заполнят всю площадь квадрата, а значит, и всю площадь круга, поэтому вероятность попадания случайно прошенной точки в круг равняется отношению площади круга к площади квадрата, то есть:\n\\[\n\\mathbb{P}(A) = \\lim_{N \\rightarrow \\infty} \\frac{n}{N} = \\frac{S_\\text{circle}}{S_\\text{square}}\n\\]\nМожно расписать точнее, если вспомнить геометрические формулы:\n\\[\n\\mathbb{P}(A) = \\frac{S_\\text{circle}}{S_\\text{square}} = \\frac{\\pi r^2}{a^2} = \\frac{\\pi \\big(\\frac{1}{2}a\\big)^2}{a^2} = \\frac{1}{4}\\pi \\approx 0.785\n\\]\nТакое подход к определению вероятности называется геометрической вероятностью.\nОтметим, что плоский случай можно обобщить и до объемного. В случае куба и шара вероятность будет равна\n\\[\n\\mathbb{P}(A) = \\frac{V_\\text{circle}}{V_\\text{square}} = \\frac{\\frac{4}{3}\\pi r^3}{a^3} = \\frac{1}{6} \\pi \\approx 0.523\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L5 // Введение в статистику. Случайный эксперимент и случайные величины</span>"
    ]
  },
  {
    "objectID": "l5.html#случайные-величины",
    "href": "l5.html#случайные-величины",
    "title": "5  L5 // Введение в статистику. Случайный эксперимент и случайные величины",
    "section": "5.3 Случайные величины",
    "text": "5.3 Случайные величины\nКогда мы первый раз обратились к игральному кубику,мы сказали, что бросание игрального кубика — это случайный эксперимент, а выпавшее число — это случайная величина. Со случайным экспериментом разобрались, приступим к случайным величинам.\nСлучайная величина — это некоторая переменная, значения которой представляют собой численные исходы некоторого случайного эксперимента.\nВ частности, исход бросания кубика — выпавшее число, исход фиксации времени реакции в эксперименте — количество миллисекунд (число), исход измерения псиометрического конструкта — суммарный балл по опроснику (число) и т.д.\nСами исходы случайного эксперимента числами быть в общем случае не обязаны, поэтому формально случайную величину \\(\\xi\\) определяют как функцию \\(y = \\xi(\\omega)\\), или \\(\\xi: \\Omega \\rightarrow \\mathbb{R}\\), на вероятностном пространстве \\((\\Omega, \\mathcal A, \\mathbb{P})\\), которая сопоставляет исходам случайного эксперимента некоторые числа.\nПоскольку случайная величина это численное выражения исходов случайного эксперимента, а сами исходы осуществляются с определенными вероятностями, чтобы мочь работать со случайной величиной, необходимо задать эту самую случайную величину, то есть описать её вероятностные свойства. Эти мы далее и займемся.\nВ зависимости от того, какое пространство элементарных событий было в случайном эксперименте — дискретное или непрерывное — случайные величины также могут быть дискретными или непрерывными.\n\n5.3.1 Дискретные случайные величины\nСлучайная величина является дискретной, если множество её значений конечно или счётно. Это позволяет задать случайную величину с помощью функции вероятности (probability mass function, PMF).\n\nМножеством определения (domain) этой функции будет множество значений случайной величины\nМножеством её значений (range) будет отрезок \\([0, 1]\\)\nСами значения будут определять вероятность, с которой происходит реализация определенного элементарного исхода соответствующего случайного эксперимента\n\nПосмотрим на примере игрального кубика. Множество определения функции вероятности известно — \\(\\{1, 2, 3, 4, 5, 6\\}\\). Каждое значение выпадает с вероятностью \\(\\frac{1}{2}\\), поэтому множество значений также известно. Можно построить график функции вероятности для этой случайной величины \\(X\\):\n\n\n\n\n\n\n\n\n\nПоскольку случайная величина дискретная, то и функция у нас получается дискретная, что выражается в том, что она отображена точками. Задав таким способом случайную величину мы полностью определяем её математическое поведение и можем с ней работать. Более того, мы можем записать её следующим образом:\n\\[\nf(x) = \\mathbb{P}(X = x),\n\\]\nгде \\(f(x)\\) — функция вероятности, \\(X\\) — случайная величина, \\(x\\) — конкретное значение случайной величины.\nИз того, как задана эта функция, вытекают её следующие свойства:\n\n\\(\\forall i \\in \\mathbb{N}\\, f(x_i) \\geq 0\\)\n\\(\\sum_{i=1}^\\infty f(x_i) = 1\\)\n\nДругим способом задания случайной величины является функция распределения (cumulative distribution function, CFD). Эта функция задается следующим образом:\n\\[\nF(x) = \\mathbb{P}(X &lt; x),\n\\] где \\(F(x)\\) — функция распределения, \\(X\\) — случайная величина, \\(x\\) — конкретное значение случайной величины. То есть значение функции распределения для данного значения случайной величины определяет вероятность того, что случайная величина примет такое или меньшее значение.\nФункция распределения может быть определена через функцию вероятности следующим образом:\n\\[\nF(x) = \\sum_{x' \\leq x} f(x')\n\\]\nОпираясь на это определение, можно изобразить график для случая игрального кубика:\n\n\n\n\n\n\n\n\n\nЭта функция также является дискретной.\n\n\n5.3.2 Непрерывные случайные величины\nСлучайная величина является непрерывной, если множество её значений обладает мощностью континуума.\nЕсли с дискретными случайными величинами все достаточно просто, то при работе с непрерывными мы сталкиваемся с определенной проблемой.\nЗададимся вновь абстрактной, но интересной задачей: будем набирать числа из отрезка от нуля до единицы \\([0, 1]\\). Возникает очень простой вопрос — с какой вероятностью мы сможем вытащить, скажем, число 0.5? или 0.341? или любое другое? Считаем, что все числа нам могут попасться равновероятно.\nНесмотря на допущение равновероятности, классическая вероятность нам здесь не поможет. При попытке определить вероятность как \\(\\frac{1}{n}\\), мы выясним, что \\(n\\) у нас бесконечно3. Получается, что\n\\[\n\\mathbb{P}(X = x) = \\frac{1}{n} = \\frac{1}{\\infty} = 0\n\\]\nИли если записать это более аккуратно:\n\\[\n\\mathbb{P}(X = x) = \\lim_{n \\rightarrow \\infty} \\frac{1}{n} = 0\n\\]\nСтатистическая вероятность нам тоже не поможет. Да, мы можем нагенерировать сколько угодно случайных чисел от 0 до 1, однако все они будут уникальными — то есть нам все равно придется делить на очень большое \\(n\\), которое будет стремиться к бесконечности при увеличении количества сгенерированных чисел, и мы получим тот же самый результат.\n— Получается, что вероятность вытащить конкретное значение равна нулю?  — Да.  — Значит ли это, что мы не сможем извлечь из отрезка от 0 до 1 ни одного числа?  — Нет.\nДействительно, для непрерывных случайных величин справедливо утверждение:\n\\[\n\\mathbb{P}(X = x) = 0,\n\\]\nгде \\(X\\) — непрерывная случайная величина, а \\(x\\) — её конкретное значение. То есть действительно,\n\nвероятность того, что непрерывная случайная величина примет своё конкретное значение, равна нулю.\n\nОднако это утверждение стоит понимать следующим образом: непрерывная случайная величина, конечно же, будет принимать какие-то конкретные значения в реализациях случайного эксперимента, однако если мы попытаемся орагнизовать случайный эксперимент так, чтобы она приняла значение \\(x\\), мы не сможем этого сделать — значение, которое мы с в итоге получим будет \\(x \\pm \\varepsilon, \\, \\varepsilon &gt; 0\\). Это значит что мы можем попадать рядом с требуемым значением, сколь угодно близко, но в него никогда не попадем.\nЧто же делать в этой ситуации? Ведь нам очень надо работать с непрерывнями случайными величинами — в частности потому, что значения статистических критериев являются непрерывными случайными величинами. Чтобы обойти это нулёво-вероятностную неприятность, вводится понятие плотности вероятности.\n\n5.3.2.1 Плотность вероятности\nПлотность вероятности — это весьма трудно перевариваемый математический концепт. Попробуем провести аналогию между дискретными и непрервными случайными величинами, чтобы попытаться облечь это в какое-то осязаемое содержание.\nМы видели функцию вероятности для игрального кубика — напомним себе, как она выглядит:\n\n\n\n\n\n\n\n\n\nЗначения на гранях кубика выпадают равновероятно, поэтому мы видим «прямую» из точек. Если мы допускаем, что точки из нашего отрезка извлекаются также равновероятно, то мы хотели бы увидеть что-то такое:\n\n\n\n\n\n\n\n\n\nПо оси \\(x\\) у нас идут значения случайной величины — от 0 до 1. За этими пределами чисел мы не набираем, поэтому линия туда не продолжается. Внутри этого отрезка у нас прямая линия, которая выражает идею равновероятного выбора любой точки из отрезка. Вопрос только один: что по оси \\(y\\)???\nЧтобы это понять, дайате сравним два графика ниже:\n\n\n\n\n\n\n\n\n\nОни похожи, только на верхнем отображена функция вероятности для дискретной величины. а на нижнем что-то аналогичное для непрерывной. Если на верхнем по оси \\(y\\) явно идет вероятность, то на нижнем какая-то другая величина. Однако и по верхнему, и по нижнему графику мы можем сказать, что значения около 15 встречаются чаще, чем значения около, например, 20. Значения же около 2–3 встречаются так же редко, как значения около 29–30.\nОкей, мы не можем работать в случае непрерывных случайных величин с конкретными значениями, но если мы выделим интервалы около интересующих нас значений, то вероятности попадания случайной величины в них мы уже можем посчитать. Возьмем значения 15 и 20 и немного отступим от них вправо — получатся интервалы \\([15, 15 + \\Delta x]\\) и \\([20, 20 + \\Delta x]\\):\n\n\n\n\n\n\n\n\n\nГрафик нам визуально подсказывает, что значения из интервала \\([15, 15 + \\Delta x]\\) встречаются чаще, чем значения из интервала \\([20, 20 + \\Delta x]\\) — это мы выяснили выше. Но можем ли мы на визуализации этих интервалов найти вероятность? Да, это будут закрашенные области под графиком. Идея здесь похожа на геометрическую вероятность, которую мы обсуждали ранее — чем больше площадь под графиком, тем больше вероятность попасть в заданный интервал.\nДействительно, такой способ отображения вероятности согласуется с тем, что вы получили аналитически — если мы сократим \\(\\Delta x\\) до нуля, то и площадь сократится до нуля, что будет означать нулевую вероятность принятия случайной величиной своего конкретного значения.\nХорошо, мы нашли вероятность. Но что же отображено на оси \\(y\\)? Это и есть та самая плотность вероятности.\nФормально плотность вероятности случайной величины \\(\\xi\\) — это числовая фукнция \\(f(x)\\), отношение \\(\\dfrac{f(x_1)}{f(x_2)}\\) значений которой в точках \\(x_1\\) и \\(x_2\\) задаёт отношение вероятностей попадания случайной величины \\(\\xi\\) в интервалы \\([x_1, x_1 + \\Delta x]\\) и \\([x_2, x_2 + \\Delta x]\\) при \\(\\Delta x \\rightarrow 0\\). Получается, что эта функция действительно отражает то, что определенные значения встречаются чаще, чем другие. График, который мы рассматривали выше, называется графиком функции плотности вероятности (probability density function, PDF).\nТаким образом, введение концепта плотности вероятности для непрерывных случайных величин позволяет визуально изучать их аналогично дискретным случайным величинам. Отличия проявляются в аналитической работе с вероятностью. Поскольку вероятность того, что непрерывная случайная величина примет своё конктреное значение, равняется нулю, мы не можем работать с вероятностями отдельных значений. Однако можем работать с вероятностями интервалов значений непрерывной случайной величины. То есть, вероятность попадания значения случайной величины в интервал \\([a, b]\\) определяется как интеграл функции плоности вероятности:\n\\[\n\\mathbb{P}(a \\leq X \\leq b) = \\int_a^b f(x) dx\n\\]\nПоскольку площадь под графиком функции плотности теперь определяет вероятность, необходимо потребовать, чтобы вся площадь под графиком была равна единице — вероятность того, что случайная величина примет хотя бы какое-то из своих значений:\n\\[\n\\int_{-\\infty}^{+\\infty} f(x) dx = 1\n\\]\nТак и задается функция плотности вероятности.\nИз функции плотности вероятности можно построить функцию распределения непрерывной случайной величины (cumulative distribution function, CDF) — это будет первообразная от функции плотности:\n\\[\nF(x) = \\int_{-\\infty}^x f(t) dt\n\\]\n\n\n\n5.3.3 Нормальное распределение\nСамое популярное распределение из всех распределений случайных величин — это нормальное распределение. Во-первых, потому что оно в принципе часто встречается в природе, а во-вторых, потому что из него получаются другие распределения. Познакомимся с ним подробнее.\nЭто непрерывное распределение, которое задается двумя параметрами — математическим ожиданием и дисперсией. Если некоторая случайная величина починяется нормальному распределению, это записывают следующим образом:\n\\[\nX \\thicksim \\mathcal N (\\mu, \\sigma^2),\n\\]\nгде \\(X\\) — случайная величина, \\(\\mathcal N\\) — обозначение нормального распределения, \\(\\mu\\) — математическое ожидание, \\(\\sigma^2\\) — дисперсия.\nАналитически нормальное распределение задается следующей функцией плотности:\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\tfrac{(x - \\mu)^2}{2\\sigma^2}},\n\\]\n\\(x \\in \\mathbb{R}, \\, \\mu \\in \\mathbb{R}, \\, \\sigma \\in \\mathbb{R}_{&gt;0}\\).\nГрафически в зависимости от параметров \\(\\mu\\) и \\(\\sigma^2\\) может выглядеть по-разному, но это всегда хорошо знакомый всем «колокол»:\n\n\n\n\n\n\n\n\n\nМатематическое ожидание \\mu задает положение середины «колокола» на оси \\(x\\), а дисперсия \\(\\sigma^2\\) — ширину колокола.\nПомимо математического ожидания и дисперсии, нормальное распределение характеризуется также коэффициентом асимметрии и коэффициентом эксцесса. Первый показывает, насколько симметрично распределение относительно математического ожидания, а второй — насколько оно сжато по бокам или, наоборот, растянуто вдоль оси \\(x\\).\n\nПоскольку любое нормальное распределение симметрично относительно математического ожидания, то коэффициент асимметрии любого нормального распределения равен нулю.\nКоэффициент эксцесса зависит от дисперсии\n\nдля дисперсии, равной единице, коэффициент эксцесса равен нулю\nесли дисперсия меньше 1 — значит, пик распределения высокий — то коэффициент эксцесса положительный\nесли дисперсия больше 1 — значит, пик распределения гладкий — то коэффициент эксцесса отрицательный\n\n\nШирину «колокола» распределения можно описывать не только через дисперсию, но и через старндартное отклонение \\(\\sigma = \\sqrt{\\sigma^2}\\). Использование стандартного отклонения позволяет определить верояности попадания значений случайной величины в определенные диапазоны:\n\n\n\n\n\n\n\n\n\nКонкретно с этими вероятностями мы работаем реже — полезнее оказываются следующие:\n\n\\(\\mathbb{P}(X \\in (\\mu - \\sigma, \\mu + \\sigma)) = 0.682\\)\n\\(\\mathbb{P}(X \\in (\\mu - 2\\sigma, \\mu + 2\\sigma)) = 0.956\\)\n\\(\\mathbb{P}(X \\in (\\mu - 3\\sigma, \\mu + 3\\sigma)) = 0.998\\)\n\nТо есть\n\nв пределах одного стандартного отклонения от среднего значения лежит почти 70% значений — это очень частотные значения\nв пределах двух стандартных отклонений от среднего значения лежит 95% значений — бо́льшая часть выборки\nв пределах трех стандартных отклонений от среднего значения лежит практически 100% выборки — то есть вся выборка",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L5 // Введение в статистику. Случайный эксперимент и случайные величины</span>"
    ]
  },
  {
    "objectID": "l6.html#оценивание-параметров",
    "href": "l6.html#оценивание-параметров",
    "title": "6  L6 // Оценивание параметров в практике статистического анализа. Тестирование статистических гипотез",
    "section": "6.1 Оценивание параметров",
    "text": "6.1 Оценивание параметров\nНапомним себе весьма малоприятную ситуацию, в которой мы находимся, когда решаем провести некоторое исследование. Мы заинтересованы в изучении генеральной совокупности. Объекты интересующей нас генеральной совокупности обладают определенными признаками, которые мы, собственно, хотели бы изучать. Признаки количественно выражены в определенных показателях.\nПризнаки могут быть очень разными и измеряться могут с помощью разных показателей. Независимо от того, как измеряется признак, генеральная совокупность характеризуется параметром.\nПараметр (\\(\\theta\\)) — относительно постоянная [от одной совокупности к другой] величина, харакретизующая генеральную совокупность по некоторому показателю.\nПроблема в том, что величина параметра, который мы изучаем, неизвестна. И никогда не будет известна. Потому что\n\nмы никогда не можем изучать всю генеральную совокупность, так как она содержит слишком много объектов\nнаши измерения всегда содержат ошибку, из-за чего мы ничего не можем измерить точно\n\nНам остаётся работать только с выборочной совокупность (выборкой) и опираться на статистические данные, которые мы собираем на ней. Измеряя что-либо на выборке, мы получаем выборочную характеристику, или оценку (\\(\\hat \\theta\\)) — эмпирический (измеримый) аналог параметра.\nВыборка извлекается из генеральной совокупности случайным образом, поэтому что там именно — с точки зрения данных — в нашей выборке будет нам также неизвестно. Отсюда происходят два ключевых свойства статистических данных — неопредлённость и вариативность.\nНеопределённость нам говорит о том, что мы не знаем, что именно мы получим в результате наших измерений для конкретной выборки. В том числе потому, что мы работаем на просторах случайных величин.\nВариативность означает, что наши данные будут различатся ещё и от респондента к респонденту. И между выборками тоже. Здесь и ошибка измерения, и различные смешения и ещё куча всего.\nВ итоге что мы имеем: так как нам не доступны истинные значения параметров, придётся использовать оценки этих параметров. Возникает вопрос: как нам получить эти оценки? и какими свойствами они должны обладать, чтобы хорошо отражать параметры генеральной совокупности?\n\n6.1.1 Точечные оценки\nПусть у нас есть некоторый параметр генеральной совокупности \\(\\theta\\). Его аналогом на выборочной совокупности является его точечная оценка \\(\\hat \\theta\\). Точечная она, потому что представляет собой некоторое одно число. Таким образом, это наиболее компактный способ составить представление о значении параметра. По своей сути она, на самом деле, является функцией — по факту, случайной величиной —- от результатов наблюдений:\n\\[\n\\hat \\theta = \\hat \\theta (\\mathbf{x}), \\; \\mathbf{x} = \\pmatrix{ x_1 & x_2 & \\dots & x_n}\n\\]\nЭто всё замечательно, но что это значит для нас как для практиков? Значение оценки зависит от наблюдений, поэтому на разных выборках мы будем получать разные значения оценки. Возьмем для примера такой параметр как среднее значение. Пусть мы изучаем интеллект — это наш признак — который мы измеряем как коэффициент IQ — это наш показатель (\\(X\\)). Известно, что в генеральной совокупности этот признак распределен нормально с математическим ожиданием 100 и стандартным отклонением 15, то есть \\(X \\thicksim \\mathcal N(100, 225)\\):\n\n\n\n\n\n\n\n\n\nТут нам, конечно, повезло, потому что мы знаем, как устроена шкала IQ1, поэтому мы знаем значение нашего параметра — \\(\\mu = 100\\). В общем случае, конечно, значение параметра, как мы отмечали выше, неизвестно.\nТеперь попробуем наизвлекать выборок человек по 50 и посчитать оценки среднего (выборочные средние) \\(\\hat \\mu\\) на них:\n\n\n\n\n\n\n\n\n\nНаблюдаем, что иногда мы при подсчёте оценке параметра попадаем близко к истинному его значению, иногда промахиваемся. Собственно, как раз об этом неопределённость и вариация.\n\n6.1.1.1 Метод моментов\nЧтобы получить точечные оценки параметров, используются разные методы. Метод зависит от того, какой параметр мы хотим оценить, а также с какой моделью мы сейчас работаем. Сейчас мы познакомимся с самым простым — методом моментов.\nСлово «момент» обычно вызывает странные ощущения — какой момент? момент чего? что в этот момент случается? Тут надо отпустить привычное понимание слово «момент» как некоторого момента времени и принять тот факт, что «момент случайной величины» — а именно о нём мы говорим — это просто характеристика распределения случайной величины. То есть математическое ожидание — это момент распределения случайной величины, дисперсия — это момент распределения случайной величины.\nВ методе моментов есть три этапа:\n\nустанавливается связь между оцениваемым параметром и моментом распределения случайной величины\n\n\\[\n\\quad \\theta = \\xi(\\mu_k),\n\\]\nгде \\(\\mu_k\\) — это момент случайной величины.\n\nнаходятся выборочные моменты\n\n\\[\n\\hat \\theta = \\xi(\\mu_k^*)\n\\]\n\nистинный момент заменяется на выборочный — получается оценка.\n\nВернемся к IQ. Эквивален среднего значения в случае генерально совокупности является математическое ожидание, поэтому значение параметра \\(\\mu\\) определяется как\n\\[\n\\mu = \\mathbb{E}X\n\\]\nВыборочным аналогом математического ожидания является выборочное среднее:\n\\[\n\\hat \\mu = \\frac{1}{n} \\sum_{i=1}^n x_i = \\bar x\n\\]\nИ это, собственно, всё. Если вы хотя бы раз анализировали данные, вы имплицитно пользовались этим знанием. Просто, скорее всего, не задумывались, что это так работает.\n\n\n\n6.1.2 Свойства точечных оценок\nТак как точечные оценки всё же оценки, мы можем и промахнуться мимо истинного среднего — это мы наблюдали на гистограмме. Поэтому нам надо предъявить определённые требования к точечным оценкам, которые будут отражать «хорошесть» точечной оценки. Таких требования три: несмещённость, состоятельность и эффективность.\n\n\n\n\n\n\nСвойства матемаческого ожидания\n\n\n\nПри обсуждении свойств точечных оценок нам потребуются два свойства математического ожидания:\n\nМатематическое ожидание суммы независимых случайных величины равно сумме их математических ожиданий\n\n\\[\n\\mathbb{E}(X_1 + X_2) = \\mathbb{E}X_1 + \\mathbb{E}X_2\n\\]\n\nКонстанту можно выносить за знак математического ожидания\n\n\\[\n\\mathbb{E}(cX) = c \\mathbb{E}X\n\\]\n\n\n\n6.1.2.1 Несмещенность\nНесмещённость выражает следующую идею: когда мы постоянно используем выборочную оценку нашего параметра на выборках некоторого объема, мы в среднем не ошибаемся в оценке параметра.\nТо есть, конечно, при каждой конкретной оценке нашего параметра на отдельной выборке мы будем совершать ошибку, однако в среднем при многократном повторении измерений и получения оценки мы будем попадать точно в цель — в параметр генеральной совокупности.\n\\[\n\\forall n \\; \\mathbb{E} \\hat \\theta = \\theta\n\\] где \\(n\\) — объём выборок.\nВыше мы рассматривали выборочное среднее как оценку математического ожидания генеральной совокупности. Давайте проверим, является ли такая оценка несмещнной. Для этого нам надо проверить, что выполняется следующее соотношение:\n\\[\n\\mathbb{E}(\\bar x) = \\mu\n\\]\nПусть у нас есть \\(n\\) выборок, на которых измерена переменная \\(X\\) — \\(X_1\\), \\(X_2\\), \\(\\dots\\), \\(X_n\\). Эти выборки пришли из одной генеральной совокупности, то есть \\(X_1, X_2, \\dots ,X_n \\overset{\\text{i.i.d}}{\\thicksim} (\\mu, \\sigma^2)\\). Запись \\(\\text{i.i.d.}\\) означает «независимые одинаково распределенные» (independent identically distributed).\nТогда получается следующее:\n\\[\n\\mathbb{E}(\\bar X) = \\mathbb{E}\\Big( \\frac{1}{n} (X_1 + X_2 + \\dots + X_n) \\Big) = \\frac{1}{n} \\Big( \\mathbb{E}(X_1) + \\mathbb{E}(X_2) + \\dots + \\mathbb{E}(X_n) \\Big)\n\\]\nПоскольку все \\(X_i\\) пришли из одного и того же распределения \\((\\mu, \\sigma^2)\\), то \\(\\forall i \\, \\mathbb{E}(x_i) = \\mu\\). Тогда\n\\[\n\\mathbb{E}(\\bar X) = \\frac{1}{n} \\cdot n \\cdot \\mu = \\mu\n\\]\nА это ровно то, что утверждается в качестве несмещенности. Таким образом, среднее является несмещенной оценкой математического ожидания.\nКроме среднего, у нас есть еще дисперсия, и мы её тоже всегда оцениванием в ходе анализа данных. На практике мы говорили, что дисперсия случайной величины определяется как \\(\\text{var}(X) = \\mathbb{E}(X^2) - \\big( \\mathbb{E}X\\big)^2\\). Эта формула удобна для расчетов, однако дисперсию можно определить и иначе:\n\\[\n\\text{var}(X) = \\mathbb{E}(X - \\mathbb{E}X)^2 = \\frac{\\sum_{i=1}^n(\\mu - x_i)^2}{n}\n\\]\nДве формулы, кстати, эквивалентны друг другу и одна выводится из другой:\n\\[\n\\begin{split}\n\\text{var}(x) &= \\mathbb{E}\\big( (X - \\mathbb{E}X )^2 \\big) = \\mathbb{E}\\big( X^2 - 2 X \\mathbb{E}X + (\\mathbb{E}X)^2 \\big) = \\\\\n& = \\mathbb{E}(X^2) - 2 \\mathbb{E}X \\mathbb{E}X + (\\mathbb{E}X)^2 = \\mathbb{E}(X^2) - 2 (\\mathbb{E}X^2) + (\\mathbb{E}X)^2 = \\\\\n& = \\mathbb{E}(X^2) - (\\mathbb{E}X^2)\n\\end{split}\n\\]\nИтак, в качестве оценки дисперсии, кажется, можно использоваться \\(\\text{var}(X) = \\frac{\\sum_{i=1}^n(\\mu - x_i)^2}{n}\\), однако из столкновения с реальностью мы знаем, что в знаменателе формулы, которую мы реально используем стоит \\(n-1\\). Почему?\nПроверим оценку дисперсии на несмещенность. Нам нужно показать, что \\(\\mathbb{E}(\\hat \\sigma^2) = \\sigma^2\\), тогда мы сможем сказать, что оценка является несмещенной. По формуле получается, что\n\\[\n\\begin{split}\n\\mathbb{E}(\\hat \\sigma^2) & = \\mathbb{E}\\Big( \\mathbb{E}(X^2) - (\\mathbb{E}X)^2 \\Big) = \\\\\n& = \\mathbb{E}\\Big( \\overline{X^2} - \\bar X^2\\Big) = \\mathbb{E}(\\overline{X^2}) - \\mathbb{E}(\\bar X^2)\n\\end{split}\n\\]\nРассмотрим сначала \\(\\mathbb{E}(\\overline{X^2})\\):\n\\[\n\\mathbb{E}(\\overline{X^2}) = \\mathbb{E}\\Big( \\frac{X_1^2 + X_2^2 + \\dots + X_n^2}{n} \\Big) = \\frac{1}{n} \\Big( \\mathbb{E}X_1^2 + \\mathbb{E}X_2^2 + \\dots + \\mathbb{E}X_n^2\\Big)\n\\]\nТак как все наблюдения приходят из одного и того же распределения, то все математические ожидания будут равны, поэтому:\n\\[\n\\mathbb{E}(\\overline{X^2}) = \\frac{1}{n} \\cdot n \\cdot \\mathbb{E}(X_i^2) = \\mathbb{E}(X_i^2)\n\\]\nТеперь \\(\\mathbb{E}(\\bar X^2)\\):\n\\[\n\\begin{split}\n\\mathbb{E}(\\bar X^2) &= \\mathbb{E}\\Big( \\frac{X_1 + X_2 + \\dots + X_n}{n} \\Big)^2 = \\\\\n& = \\frac{1}{n^2} \\mathbb{E}(X_1 + X_2 + \\dots + X_n)^2 = \\\\\n& = \\frac{1}{n^2} \\mathbb{E}(X_1^2 + X_2^2 + \\dots X_n^2 + 2X_1X_2 + \\dots + 2X_{n-1}X_n) = \\\\\n& = \\frac{1}{n^2} \\mathbb{E}\\Big( (X_1^2 + X_2^2 + \\dots X_n^2) + (2X_1X_2 + \\dots + 2X_{n-1}X_n) \\Big)\n\\end{split}\n\\]\nВнутри скобок получается два слагаемых: если с \\(X_i^2\\) все понятно — выше мы уже с ним сталкивались, то со вторым надо разбираться, а именно, подсчитать, сколько попарных произведений случайных величин у нас будет. Их будет \\(C_n^2 = \\frac{n(n-1)}{2}\\). Поэтому если мы будем раскрывать скобки, то получим слоедующее:\n\\[\n\\begin{split}\n\\mathbb{E}(\\bar X^2) & = \\frac{1}{n^2} \\cdot n \\cdot \\mathbb{E}(X_i^2) + \\frac{1}{n^2} \\cdot \\frac{n(n-1)}{2} \\cdot 2 \\mathbb{E}(X_iX_j) = \\\\\n& = \\frac{1}{n} \\mathbb{E}(X_i^2) + \\frac{n-1}{n} (\\mathbb{E}X_i)^2\n\\end{split}\n\\]\nТеперь соберем две части вместе:\n\\[\n\\begin{split}\n\\mathbb{E}(\\hat \\sigma^2) & = \\mathbb{E}(\\overline{X^2}) - \\mathbb{E}(\\bar X^2) = \\\\\n& = \\mathbb{E}(X_i^2) - \\frac{1}{n} \\mathbb{E}(X_i^2) - \\frac{n-1}{n} (\\mathbb{E}X_i)^2 = \\\\\n& = \\frac{n}{n} \\mathbb{E}(X_i^2) - \\frac{1}{n} \\mathbb{E}(X_i^2) - \\frac{n-1}{n} (\\mathbb{E}X_i)^2 = \\\\\n& = \\frac{n-1}{n} \\Big ( \\mathbb{E}(X_i^2) - (\\mathbb{E}X_i)^2 \\Big) = \\\\\n& = \\frac{n-1}{n} \\sigma^2\n\\end{split}\n\\]\nПолучается, что математическое ожидание нашей оценки оказывается равно не самому значению интересующего нас параметра, а значению параметра, умноженному на некоторое число \\(\\frac{n-1}{b}\\), то есть оценка является смещенной. Именно поэтому для расчета дисперсии на выборке используется выборочная, или исправленная, дисперсия.\nКак она исправляется? Если у нас оценка дисперсии отличается от значения параметра в \\(\\frac{n-1}{n}\\) раз, то надо домножить оценку на \\(\\frac{n}{n-1}\\):\n\\[\ns^2 = \\frac{n}{n-1} \\cdot \\hat \\sigma^2 = \\frac{n}{n-1} \\cdot \\frac{1}{n} \\sum (x_i - \\bar x)^2 = \\frac{1}{n-1} \\sum (x_i - \\bar x)^2\n\\]\nИ вот мы получили знакомую нам формулу для расчета выборочной дисперсии. Такая оценка является несмещенной.\nИногда несмещенность от оценки бывает потребовать сложно, тогда можно ограничиться ассимптотической несмещенностью:\n\\[\n(\\mathbb{E}\\hat \\theta - \\theta) \\underset{n \\rightarrow \\infty}{\\rightarrow} 0,\n\\]\nгде \\((\\mathbb{E}\\hat \\theta - \\theta)\\) — смещение. Ассимптотическая несмещенность требует, чтобы математическое ожидание нашей оценки приближалось к значению параметра с ростом объема выборки.\n\n\n6.1.2.2 Состоятельность\nМатематически состоятельность определяется следующим образом:\n\\[\n\\lim_{n \\rightarrow \\infty} \\mathrm{P}(|\\hat \\theta - \\theta| &lt; \\varepsilon) = 1, \\, \\varepsilon &gt; 0\n\\]\nСодержательно эта запись нам говорит следующее, что при неограниченном росте мощности выборки наша оценка стремится к истинному значению параметра. Или, проще, с ростом выборки значение нашей оценки все реже выпадает из некоторого достаточно узкого интервала \\((\\theta - \\varepsilon, \\theta + \\varepsilon)\\). Может быть, такая формулировка не совсем точна математически, но позволяет представить, что происходит.\nДавайте посмотрим на это на картинке. Нже изображено поведение состоятеной оценки с ростом выборки:\n\n\n\n\n\n\n\n\n\nА так ведет себя несостоятельная оценка:\n\n\n\n\n\n\n\n\n\n\n\n6.1.2.3 Эффективность\nЭффективность точечной оценки определяется достаточно просто. Так как оценка параметра — это случайная величина, но у неё есть дисперсия. Чтобы оценка была эффективна, её дисперсия должна быть минимальной:\n\\[\n\\sigma^2_{\\hat \\theta} = \\min\n\\]\nОпять же попробуем посмотреть на это на картинке:\n\n\n\n\n\n\n\n\n\n\n\n\n6.1.3 Интервальные оценки\nКроме самого значения оценки, необходимо определить качество этой оценки, иначе говоря — её точность. Для этого используется такая величина как надёжность:\n\\[\n\\gamma = \\mathbb{P}(\\theta_\\min &lt; \\theta &lt; \\theta_\\max)\n\\]\nТакая форма оценки называется интервальной оценкой параметра, так как мы указываем интервал, в котором находится истинное значение с определённой вероятностью.\nТакая форма оценки даёт исчерпывающую информацию о параметре: мы знаем (1) интервал, в котором находится значение параметра генеральной совокупности, а также (2) надёжность, с которой выбранный интервал накрывает это значение.\nЗначение надежности \\(\\gamma\\) может быть выбрано произвольно, но обычно оно близко к единице. Однако необходимо помнить, что чем выше надёжность, тем шире границы интервальной оценки.\n\n6.1.3.1 Стандартная ошибка\nДля того, что получить интервальную оценку нашего параметра, нам нужно изучить, как ведет себя наша выборочная оценка в случае, когда мы много раз извлекаем выборку из генеральной совокупности. Рассмотрим на примере среднего значения.\nПусть мы в том же примере с IQ, который обсуждали выше. Мы знаем, что распределение параметра в генеральной совокупности такое:\n\n\n\n\n\n\n\n\n\nВновь извлечем несколько выборок из нашей генеральной совокупности:\n\n\n\n\n\n\n\n\n\nМы уже отмечали, что на отдельной выборке мы будем получать оценку среднего, которая будет отличаться от значения параметра в генеральной совокупности. Однако если мы извлечем много выборок — скажем, 1000 выборок по 100 наблюдений — посчитаем на каждой среднее и построим распределение выборочных средних, то получим нечто такое:\n\n\n\n\n\n\n\n\n\nНаши средние будут как-то распределены, при это среднее средних будет оказываться очень близко с значению нашего параметра. При этом данное распределение крайне похоже на нормальное и может быть описано как \\(\\mathcal N(\\overline{\\bar x}, \\sigma_{\\bar x}^2)\\), то есть как и любое нормальное распределение получившееся распределение будет описываться некоторой дисперсией. Стандартное отклонение этого распределения называется стандартной ошибкой среднего (standard error of mean):\n\\[\n\\text{se}(\\bar x) = \\sqrt{\\sigma^2_{\\overline x}} = \\sigma_{\\overline x}\n\\]\nВ нашем случае оно будет равно 1.43.\nСтандартная ошибка среднего является одной из интервальных оценок среднего значения. Однако выше мы сказали, что для интервальной оценки нам надо указать надежность, то есть вероятность, с которой значение изучаемого параметра находится в интервале, задаваемом интервальной оценкой. Можем ли мы это сделать в случае стандартной ошибки? Да, поскольку мы значем как устроено нормальное распределение. Так как стандартная ошибка является стандартным отклонением распределения выборочных средних, а в пределах отдного стандартного отклонения от среднего лежит 68.2% значений нормально распределенной случайной величины, то мы можем записать:\n\\[\n\\begin{split}\n0.682 & = \\mathbb{P}(\\overline{\\bar x}-\\sigma^2_{\\overline x} &lt; \\mu &lt; \\overline{\\bar x}+\\sigma^2_{\\overline x}) \\\\\n& = \\mathbb{P}(98.57 &lt; \\mu &lt; 101.43)\n\\end{split}\n\\]\nТо, что мы проделали выше легитимизировано цетральной предельной теоремой. Визуализацию можно найти здесь.\nЭто конечно хорошо, но мы же не можем каждый раз извлекать по 1000 выборок, чтобы рассчитать стандартную ошибку среднего. Нам необходим способ её расчета по одной выборки, чтобы мы могли производить интервальную оценку нашего параметра в одном отдельной исследовании. Такой способ подсчета есть, и он выглядит так:\n\\[\n\\text{se}_X = \\frac{\\text{sd}_X}{\\sqrt{n}} = \\frac{\\hat \\sigma^2_X}{\\sqrt{n}}\n\\]\nВообще-то такое равенство совершенно неочевидно, поэтому попробуем понять, почему оно справедливо.\n\n\n\n\n\n\nСвойства дисперсии\n\n\n\nДля того, чтобы вывести эту формулу нам потребуются два свойства дисперсии:\n\nДисперсия суммы двух независимых случайных величины равна сумме их дисперсий\n\n\\[\n\\text{var}(X + Y) = \\text{var}(X) + \\text{var}(Y)\n\\]\n\nКонстанта выносится из-под знака дисперсии с возведением в квадрат:\n\n\\[\n\\text{var}(aX) = a^2 \\text{var}(X)\n\\]\n\n\nТак как наблюдения извлекаются из независимых одинаково распределенных величин (independent identically distributed, iid), то они независимы. Получается что дисперсия распределения выбборочных средних будет равна:\n\\[\n\\text{var}\\bar X_i = \\text{var}\\Big( \\frac{1}{n} \\sum X_i \\Big)\n\\]\nПо свойствам дисперсии:\n\\[\n\\begin{split}\n\\text{var}\\bar X_i & = \\text{var}\\Big( \\frac{1}{n} \\sum X_i \\Big) \\\\\n& = \\frac{1}{n^2} \\sum \\text{var}(X_i) = \\frac{1}{n^2} \\sum \\sigma^2 = \\frac{n}{n^2} \\sigma^2 = \\frac{\\sigma^2}{n}\n\\end{split}\n\\]\nТак как стандартная ошибка это старндартное отклонение распределения выборочных средних, то:\n\\[\n\\text{se}_X = \\sqrt{ \\text{var}\\Big( \\frac{1}{n} \\sum X_i \\Big)} = \\sqrt{\\frac{\\sigma^2}{n}} = \\frac{\\sigma}{\\sqrt{n}}\n\\]\nВот и получается формула для расчета стандартной ошибки на одной выборке.\n\n\n6.1.3.2 Доверительный интервал\nДругим вариантом интервальной оценки является доверительный интервал (confidence interval). На практике он используется гораздо чаще стандартной ошибки, но оказывается тесно с ней связан. Итак, ещё раз:\n\\[\n\\mathrm{P}(\\theta_\\min &lt; \\theta &lt; \\theta_\\max) = \\gamma, \\; \\gamma \\rightarrow 1\n\\]\n\\(theta_\\min\\) и \\(\\theta_\\max\\) — границы доверительного интервала, \\(\\gamma\\) — доверительная вероятность. На практике её значение чаще всего принимается равным \\(0.95\\).\nЧто нам нужно, чтобы определить границы, в которых значение параметра лежит с вероятностью \\(0.95\\), если мы значем, про наша оценка распределена нормально?\nМы знаем особое нормальное распределение \\(z \\thicksim \\mathcal N(0, 1)\\), которое называется стандартным нормальным распределением. Можно рассчитать границы следующего интервала:\n\n\n\n\n\n\n\n\n\nОказывается, что \\(z\\)-значение с вероятностью 0.95 попадается в интервал \\([z_{\\min}, z_{\\max}] = [-1.96, 1.96]\\). Зная эти точки и воспользовавшись принципом стандартизации, мы можем записать следующее:\n\\[\n\\mathbb{P}\\Big( \\bar x - z_\\min \\text{se}_X &lt; \\mu &lt;\n\\bar x + z_\\max \\text{se}_X \\Big) = \\gamma\n\\]\nИли конкретнее:\n\\[\n\\mathbb{P}\\Big( \\bar x - -1.96 \\text{se}_X &lt; \\mu &lt;\n\\bar x + 1.96 \\text{se}_X \\Big) = 0.95\n\\]\nТо есть, зная стандартную ошибку мы можем рассчитать доверительный интервал. В общем случае, любой — 95%, 90%, 99% и др. — подставляя разные \\(z_\\min\\) и \\(z_\\max\\).\n\n\n6.1.3.3 Интерпретация доверительного интервала\nС учетом расчета нам очень хочется сказать, что границы доверительного инетрвала задают диапазон, в котором значение нашего параметра — математического ожидания в генеральной совокупности — лежат с вероятностью 0.95. Однако это не верно!\nЗдесь в игру вступает фреквентистский подход (см. далее), в котором мы с вами живем. Корректная интерпретация границ доверительного интервала звуит так:\n\nЕсли мы будет бесконечно извлекать новые выборки из генеральной совокупности, рассчитывать на них средние и 95% доверительные интервалы к ним, то генеральное среднее попадёт в границы 95% таких доверительных интервалов.\n\nТо есть, если мы извлечем 100 выборок, посчитаем на каждой из них среднее и построим 95% доверительный интервал к каждому из 100 средних, то 95 доверительных интервалов из 100 будут содержать генеральное среднее — а 5 интервалов содержать его не будут.\nВизуализацию этого можно наблюдать здесь.\nРеальная вероятность, что значение параметра генеральной совокупности попадет в пределы конкретного доверительного, рассчитанного в данном исследовании, оказывается меньше — около 84.3%.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>L6 // Оценивание параметров в практике статистического анализа. Тестирование статистических гипотез</span>"
    ]
  },
  {
    "objectID": "l6.html#тестирование-гипотез",
    "href": "l6.html#тестирование-гипотез",
    "title": "6  L6 // Оценивание параметров в практике статистического анализа. Тестирование статистических гипотез",
    "section": "6.2 Тестирование гипотез",
    "text": "6.2 Тестирование гипотез\nВ ходе статистического анализа мы, главным образом, заняты тем, что тестируем статистические гипотезы. Ведь на какого рода вопросы мы отвечаем с помощью анализа?\n\nРазличаются ли группы между собой?\nЗначимо ли влияние какого-либо фактора? → Различаются ли группы между собой?\nХороша ли та модель, которую мы построили? → Отличается ли она от нулевой модели?\n\nИ так далее. Так или иначе, всё сводится в тому, что мы ищем какие-то различия. Но силу того, что у нас неопределённость и вариация в данных, мы просто так «в лоб» сказать о различиях по оценкам параметров не можем. Приходится тестировать статистические гипотезы.\n\n6.2.1 Нулевая и альтернативная гипотезы\nЧто такое гипотеза и какие они вообще бывают?\n\nГипотеза (\\(H\\)) — это предположение, которое подлежит проверке на основе результатов наблюдений.\nГипотезы бывают:\n\nтеоретические — про конструкты\nэмпирические — про переменные\nстатистические — про параметры [генеральной совокупности] и данные\n\n\nСтатистические гипотезы бывают простыми и сложными:\n\nПростая гипотеза — это такое предположение, которое включает в себя какое-либо однозначно определеяемое утверждение. Например, истинная величина параметра соответствует некоторому строго заданному значению: \\(H : \\theta = \\theta_0\\). Другой вариант — две генеральные совокупности имеют одно и то же значение одной и той же характеристики: \\(H : \\theta_1 = \\theta_2\\).\nСложная гипотеза предполагает множественность вариантов для параметра, которые укладываются в рамки проверяемого предположения. Например, \\(H : \\theta &gt; \\theta_0\\) или \\(H : \\theta_1 \\neq \\theta_2\\).\n\nВ рамках самого хода тестирования гипотез существует проверяемая (нулевая) гипотеза (\\(H_0\\)). Её обычно стараются предельно упростить, поэтому она формулируется как простая гипотеза. В противовес ей выдвигается альтернативная гипотеза (\\(H_1\\)), которая будет иметь вид сложной гипотезы.\nДля проверки гипотезы необходимы две вещи:\n\nрезультаты наблюдений и\nкритерий.\n\nРезультаты наблюдений, полученные на выборке, сами по себе, как правило, не используются. Однако на их основе рассчитываются выборочные статистики (показатели), которые непосредственно участвуют в проверке гипотезы.\n\n\n6.2.2 Подходы к тестированию статистических гипотез\n\n6.2.2.1 Фреквентистский подход\nФреквентистский подход, широко распространенный в тестировании статистических гипотез, задается следующим вопросом:\n\nКакова вероятность получить такие данные, если допустить, что нулевая гипотеза верна?\n\nТо есть в этой логике мы собираем какие-то данные в ходе исследования, формулируем некоторую статистическую гипотезу \\(H_0\\) об отсутствии закономерности в генеральной совокупности, и далее определяем, насколько вероятно было бы получить вот эти данные, которые у нас сейчас есть, в случае, когда закономерности нет.\nДалее, если эта вероятность мала, то мы делаем вывод, что в генеральной совокупности закономерность всё-такие есть — поэтому мы получили данные, которые не характерны для случая, когда верна нулевая гипотеза. Если же мы получаем, что такая вероятность велика, то мы остаемся с нулевой гипотезой, которую не удалось отклонить.\nЭтот подход требует возможности много раз повторять наше исследование, чтобы проверять, действительно ли мы из ра-а в раз будем получать схожие результаты — то есть, фреквентистский подход хорошо работает в долгосрочной перспективе и именно когда у нас есть возможность повторять наше исследование, в которых мы тестируем одни и те же гипотезы, мы можем получать достаточно однозначные выводы об изучаемых закономерностях.\n\n\n6.2.2.2 Байесовский подход\nЕсли мы внимательно вдумаемся в тот вопрос, которым задается фреквентистская статистика, мы поймем, что он достаточно тупой: нам вообще-то не интересует вероятность получить вот эти данные, если нулевая гипотеза верна. Почему? Потому что мы эти данные уже получили. Гораздо более интересным является вопрос о том,\n\nнасколько вероятная справедливость нулевой или альтернативной гипотезы при условии, что мы получили такие данные.\n\nЭтим вопросом задается байесовская статистика, которая постепенно набирает свое влияние среди исследователей. Сложно сказать, какой из двух подходов лучше — у каждого есть свои плюсы, у каждого есть свои минусы, и каждый обладает своими особенностями. В целом, они хорошо дополняют друг друга, и одни и те же данные могут быть проанализированы как с помощью байесовского, так и с помощью фреквентистского подхода.\nОднако в нашем курсе мы будем идти классическим путем и рассматривать тестирование статистических гипотез во фреквентистском подходе.\n\n\n\n6.2.3 Возможные результаты проверки гипотез\nВ результате проверки статистических гипотез могут возникнуть четыре ситуации.\nМы изучаем в исследовании какую-либо закономерность, которая в реальном мире может существовать, а может и не существовать. В силу неопределённости и вариативности наших данных мы может либо обнаружить интересующую нас закономерность, либо не обнаружить.\nВ качестве нулевой гипотезы мы выдвигаем предположение о том, что закономерность отсутствует — так мы упрощаем нашу нулевую гипотезу. Пусть \\(H_0\\) обозначает, что предположение, которое мы проверяем справедливо, а \\(H_1\\) — не справедливо. На основании данных мы можем либо не отклонить наше предположение (\\(\\hat H_0\\)), либо отклонить (\\(\\hat H_1\\)).\nТогда имеем следующую ситуацию:\n\n\n\n\n\\(H_0\\)\n\\(H_1\\)\n\n\n\n\n\\(\\hat H_0\\)\n✓\nОшибка II рода\n\n\n\\(\\hat H_1\\)\nОшибка I рода\n✓\n\n\n\n\nОшибка I рода возникает, когда в генеральной совокупности искомой закономерности нет, но мы в силу случайных флуктуаций в данных её нашли.\nОшибка II рода возникает, когда в генеральной совокупности искомая закономерность есть, но мы в силу каких-либо причин её не нашли.\n\nОшибки — это нехорошо, они нас не устраивают. Надо каким-то образом их контролировать.\n\nОшибка I рода контролируется достаточно просто. Так как мы нашли закономерность, которую искали, мы можем посчитать вероятность, с которой потенциально ошиблись. А собственно контролировать ошибку мы будем с помощью уровня значимости \\(\\alpha\\), который выбирается до начала процедуры тестирования гипотезы. Он и задает вероятность, с который мы позволяем себе ошибиться — отклонить нулевую гипотезу, при условии, что она верна.\nОшибку II рода контролировать сложнее, так как мы не нашли закономерность, которую искали. Нам нужна какая-то метрика, которая позволит сказать, что мы сделали всё возможное для того, чтобы обнаружить искомую закономерность. Вероятность ошибки II рода обозначается \\(\\beta\\) — тогда вероятность того, что мы не совершили ошибку II рода будет \\(1 - \\beta\\). Эта величина называется статистической мощностью, и она связана с размером эффекта и объемом выборки. Статистическую мощность рассчитывают до проведения статистического анализа — она помогает определить требуемый объема выборки.\n\nСоберем все обозначения в единую табличку2:\n\n\n\n\n\n\n\n\n\n\\(H_0\\)\n\\(H_1\\)\n\n\n\n\n\\(\\hat H_0\\)\n\\(\\mathrm P (\\hat H_0 | H_0)\\)\n\\(\\mathrm P (\\hat H_0 | H_1) = \\beta\\)\n\n\n\\(\\hat H_1\\)\n\\(\\mathrm P (\\hat H_1 | H_0) = \\alpha\\)\n\\(\\mathrm P (\\hat H_1 | H_1) = 1 - \\beta\\)\n\n\n\nУровень значимости \\(\\alpha\\) выбирается близким к нулю — всем знакомо конвенциональное значение \\(0.05\\). Вообще \\(\\alpha\\) можно выбрать сколь угодно малым, однако при выборе уровня значимости руководствуются принципом разумной достаточности, так как если устремить \\(\\alpha\\) к нулю, то устремиться к нулю и вероятность отклонения нулевой гипотезы.\n\\[\n\\mathrm P (\\hat H_1) = \\mathrm P (\\hat H_1 | H_0) \\cdot \\mathrm P (H_0) = \\alpha \\cdot \\mathrm P(H_0)\n\\]\nДостаточной статистической мощностью считается \\(0.8\\). Аналогично, устремляя мощность к единице (\\((1 - \\beta) \\rightarrow 1 \\Rightarrow \\beta \\rightarrow 0\\)), мы устремляем вероятность не отклонения нулевой гипотезы к нулю:\n\\[\n\\mathrm P (\\hat H_0) = \\mathrm P (\\hat H_0 | H_1) \\cdot \\mathrm P (H_1) = \\beta \\cdot \\mathrm P (H_1)\n\\]\n\n6.2.3.1 Асимметрия статистического вывода\nВыше мы сказали, что для проверки гипотезы нужны две вещи:\n\nрезультаты наблюдений и\nкритерий.\n\nС результатами наблюдений более-менее очевидно.\nКритерий — это правило, согласно которому гипотезу либо принимают, либо отклоняют. Однако перед тем как проверять гипотезу, её так-то нужно сформулировать, и сделать это правильно, поскольку от формулировки гипотезы зависит интерпретация результатов проверки и дальнейшее использование полученной информации.\nИспользуемая статистика сама по себе является [непрерывной] случайной величиной, а значит может быть построено её распределение. Критерий будет разделять это распределение на непересекающиеся области. В результате чего возникает критическая область — область отклонения гипотезы. Дополнением к ней является область неотклонения гипотезы.\nКритическая область может быть односторонней (при \\(H_1:\\theta &gt; \\theta_0\\) или \\(H_1: \\theta &lt; \\theta_0\\)) и двусторонней (при \\(H_1:\\theta \\neq \\theta_0\\)). «Размер» критической области определяется уровнем значимости.\nСтатистический вывод — заключение о том, получили ли мы подтверждение альтернативной гипотезы — по структуре представляет собой импликацию и звучит так:\n\nЕсли значение статистики критерия попало в критическую область, то у нас есть основания отклонить нулевую гипотезу в пользу альтернативной\n\nК чему приводит факт, что это утверждение является имликацией:\n\nЕсли значение нашей статистики, которое мы рассчитали на выборке, попало в критическую область, то мы говорим о том, что нулевая гипотеза отклоняется.\nЕсли значение нашей статистики, которое мы рассчитали на выборке, не попало в критическую область, то мы не получаем оснований для того, чтобы отклонить нулевую гипотезу. Однако мы также не получаем оснований, чтобы её «принять». Мы остаёмся в некотором неведении: мы не нашли различий, а есть они там или нет — хто ж их знает… Итого, мы не можем сделать никакого вывода.\n\nВ этом и заключается асимметрия статистического вывода. Как раз для того, чтобы с ней как-то жить, мы работаем со статистической мощностью.\n\n\n6.2.3.2 Связь ошибки первого и второго рода\nНеобходимо также помнить, что ошибки первого и второго рода связаны между собой так, что\n\\[\n\\alpha \\rightarrow 0 \\Rightarrow \\beta \\rightarrow 1\n\\]\n\\[\n\\begin{split}\n\\beta \\cdot \\mathrm P (H_1) & = \\mathrm P (\\hat H_0) = \\mathrm P (\\hat H_0 | H_0) \\cdot \\mathrm P (H_0) \\Rightarrow \\\\\n\\beta & = \\frac{1}{\\mathrm P (H_1)} \\cdot \\mathrm P (H_0) \\cdot \\mathrm P(\\hat H_0 | H_0) \\\\\n\\beta & = \\frac{1}{\\mathrm P (H_1)} \\cdot \\big (1 - \\mathrm P (H_1 | H_0)\\big) = \\frac{1}{\\mathrm P (H_1)} \\cdot \\mathrm P (H_0) \\cdot (1 - \\alpha)\n\\end{split}\n\\]\n\n\n\n6.2.4 Агоритм тестирования статистических гипотез\nДля тестирования гипотез есть два сценария: первый и тот, которым мы будем пользоваться. Первый вариант чуть более классический, второй — более гибкий.\nСценарий номер раз\n\nФормулировка гипотезы\nВыбор статистического критерия\nВыбор уровня значимости \\(\\alpha\\)\nПостроение закона распредления статистики критерия при условии, что нулевая гипотеза верна\nОпределение границ критической области\nРасчёт выборочной статистики\nОпределение, попадает ли наблюдемое значение статистики в критическую область и вынесение решения\n\nСценарий номер два\n\nФормулировка гипотезы\nВыбор статистического критерия\nВыбор уровня значимости \\(\\alpha\\)\nПостроение закона распредлеения статистики критерия при условии, что нулевая гипотеза верна\nРасчёт выборочной статистики\nРасчёт достигнутого уровня значимости p-value\nСопоставление \\(\\alpha\\) и p-value и вынесение решения\n\nПочему второй вариант более гибкий? Представим, что мы захотели понизить уровень значимости с \\(0.05\\) до \\(0.01\\) — такие уровни значимости всречаются, например, в медицине. Если мы идем по первому сценарию, то нам надо заново пересчитать критические значения и вновь проанализировать, попадает ли наблюдаемое значение в критическую область. Если мы адепты второго сценария, то нам надо только выполнить одно новое сравнение нашего p-value с новым уровнем значимости.\n\n\n6.2.5 Размер эффекта и статистическая мощность\nОшибку второго рода контролировать сложнее, чем ошибку первого рода, так как мы не обнаруживаем закономерность. Собственно, ошибка второго рода соответствует ситуации, когда мы не обнаружили закономерность при условии, что закономерность в генеральной совокупности присутствует. На эту вероятность влияет «размер» той закономерности, её «сила», в генеральной совокупности. Это велична называется размером эффекта. Численным выражением силы взаимосвязи в генеральной совокупности является размер эффекта (effect size).\nИз-за того, что в случае ошибки второго рода мы не можем работать с её вероятностью \\(\\beta\\) — опят же, так как мы не обнаруживаем в этом случае закономерность — мы работает с вероятностью \\(1-\\beta = \\mathbb{P}(\\hat H_1|H_1)\\). Это вероятность, с которой мы обнаружим закономерность при условии, что в генеральной совокупности закономерность есть. Эта величина называется статистическая мощность (statistical power) исследования.\nСтатистическая мощность зависит от размера эффекта и объема выборки. Вопрос: как размер эффекта, статистическая мощность и объем выборки соотносятся между собой?\n\nЧем больше размер эффекта, тем меньшую по объему выборку нам необходимо набрать, чтобы достигнуть требуемой статистической мощности.\nЧем больше выборка, тем выше статистическая мощность исследования.\n\n\nПосмотреть, как все эти штуки друг с другом соотносятся можно тут.\n\n\n\n6.2.6 Ложноположительный вывод\n\n6.2.6.1 Проблема множественных сравнений\nИтак, мы сравниваем попарно все группы наблюдений между собой. В каждом сравнении мы фиксируем вероятность ошибки первого рода с помощью уровня значимости на уровне \\(0.05\\). А какова будет вероятность ошибки, если мы проводим несколько сравнений?\nСчитаем, что наши сравнения независимы, поэтому вероятности будут перемножаться1. Если верояность ошибиться в одном сравнении равна \\(\\alpha\\), то вероятность сделать правильный вывод — \\(1 - \\alpha\\). Тогда вероятность сделать правильный вывод в \\(m\\) сравнениях — \\((1 - \\alpha)^m\\). Отсюда мы можем вывести вероятность ошибиться хотя бы в одном сравнении:\n\\[\n\\mathbb{P}^′ = 1 - (1 - \\alpha)^m\n\\]\nПусть у нас есть три группы, которые нам надо сравнить друг с другом — получается необходимо провести три сравнения. Итого вероятность ошибиться получается:\n\\[\n\\mathbb{P}^′ = 1 - (1 - 0.05)^3 \\approx 0.143\n\\]\nЗначительно больше, чем \\(0.05\\), что нехорошо. И дальше только хуже. Поэтому нам надо либо корректировать уровень значимости, либо использовать мощные методы типа дисперсионного анализа.\n\n\n\n\n\n\n\n\n\n\n6.2.6.1.1 Корректировка уровня значимости\nКорректировать уровень значимости можно по-разному. Например, можно разделить \\(\\alpha\\) на количество попарных сравнений — такой способ называется поправкой Бонферрони (Bonferroni):\n\\[\n\\alpha’ = \\frac{\\alpha}{n},\n\\]\nгде \\(n\\) — число попарных сравнений.\nПоправка Бонферрони считается самой консервативной поправкой — она достаточно сильно уменьшает уровень значимости, и мы можем не поймать искомую закономерность, то есть совершить ошибку второго рода2. Поэтому придумали более либеральные поправки, например, поправку Холма (Холма–Бонферрони, Holm) или поправку Тьюки (Tukey’s HSD test).\nНа практике в силу того, что в статистических пакетах мы работаем с p-value, корректируется именно его значение.\nПо достаточно незамысловатой логике Здесь: вариант для поправки Бонферрони.\n\\[\np &lt; \\frac{\\alpha}{n} \\Rightarrow np &lt; \\alpha\n\\]\nТаким образом, мы просто сравниваем уже скорретированное p-value, которое нам считает программа, с тем же самым \\(\\alpha = 0.05\\). Жизнь становится значительно проще и приятнее.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>L6 // Оценивание параметров в практике статистического анализа. Тестирование статистических гипотез</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Источники",
    "section": "",
    "text": "Barrett, P. 2005. “What If There Were No Psychometrics?:\nConstructs, Complexity, and Measurement.” Journal of\nPersonality Assessment 85 (2): 134–40.\n\n\nMessick, S. 1993. “Foundations of Validity: Meaning and\nConsequences in Psychological Assessment.” Ets Research\nReport Series 1993 (2): i–18.\n\n\nЗегет, В. 1985. Элементарная Логика. Москва: Высшая школа.",
    "crumbs": [
      "Источники"
    ]
  },
  {
    "objectID": "l7.html#andan-descriptives",
    "href": "l7.html#andan-descriptives",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "7.1 Описательные статистики",
    "text": "7.1 Описательные статистики\n\n7.1.1 Виды статистики\nСтатистика [как набор методов и инструментов] делится на два вида — описательная статистика и статистика вывода.\n\nОписательная статистика (descriptive statistics1) занимается обработкой статистических данных, их наглядным представлением, и собственно описанием через некоторые характеристики.\n\nЭти характеристики, количественно описывающие особенности имеющихся данных, называются описательными статистиками (descriptive statistics2).\nЗадача описательной статистики — ёмко описать имеющиеся данные и составить на основе этих описаний общее представление о них, а также обнаружить особенности, которые могут повлиять на дальнейший анализ.\n\nСтатистика вывода (inferential statistics) занимается поиском ответов на содержательные вопросы, которые мы задаем данным в ходе их анализа в рамках научных и практических исследований.\n\nСостоит из двух компонентов — тестирования статистических гипотез и статистических методов.\n\n\n\n\n\n\n\n\nЗамечание о машинном обучении\n\n\n\nВ названии курса упомянуто «машинное обучение». Иногда его причисляют к статистике, иногда рассматривают отдельно. На самом же деле, статистические методы лежат где-то между статистикой вывода и машинным обучением.\nПочему?\nДело в том, что на статистические методы можно смотреть по-разному.\n\nЕсли нашей задачей является поиск ответов на исследовательские вопросы о закономерностях, о связи каких-либо факторов или влиянии переменных друг на друга, то мы будем смотреть на статистические модели с точки зрения статистики вывода. Это позволит нам находить ответы на интересующие нас вопросы — причем не важно, говорим мы о научных исследованиях или об исследованиях в индустрии.\nЕсли перед нами стоит задача хорошо предсказывать одни переменные на основании значений других — например, выдавать рекомендации на Яндекс.Музыке или в Яндекс.Лавке — то мы будем смотреть на те же статистические модели с точки зрения машинного обучения.\n\nТо есть, модели в анализе данных и машинном обучении одни и те же, но то, какую модель мы назовем хорошей и как мы эту «хорошесть» определим, будет отличаться в зависимости от задачи — исследовательская или предиктивная — которая перед нами стоит.\n\n\n\n\n7.1.2 Меры центральной тенденции\nИтак, мы хотим описать наши данные. Точнее, распределения переменных, которые у нас в данных есть. Хотим сделать это просто и ёмко. Насколько просто и ёмко? Ну, допустим максимально — одним числом. Для этого неплохо подойдет значение переменной, которое лежит в центре распределения.\nКак мы будем искать, что там в центре распределения? Зависит от шкалы, в которой измерена конкретная переменная.\n\n\n\nШкала\nМера центральной тенденции\n\n\n\n\nНоминальная\nМода\n\n\nПорядковая\nМедиана\n\n\nИнтервальная\nСреднее арифметическое\n\n\nАбсолютная\nСреднее арифметическое, геометрическое и др.\n\n\n\nОднако есть некоторые нюансы.\n\n7.1.2.1 Мода\nСамый простой вариант найти центральную тенденцию — это определить наиболее часто встречающееся значение переменной. Это значение называется модой (mode).\n\nОпределение 7.1 Мода [дискретной переменной] — наиболее часто встречающееся значение данной переменной.\n\nНапример, у нас есть следующий ряд наблюдений по какой-то переменной:\n\\[\n\\begin{bmatrix}\n1 & 3 & 4 & 6 & 3 & 2 & 3 & 3 & 2 & 4 & 1\n\\end{bmatrix}\n\\]\nЕсли мы посчитаем, сколько раз встретилась каждое значение переменной и составим таблицу частот, то получим следующее:\n\\[\n\\begin{matrix}\n\\text{Значение} & 1 & 2 & 3 & 4 & 6 \\\\\n\\text{Частота}  & 2 & 2 & 4 & 2 & 1\n\\end{matrix}\n\\]\nОчевидно, что \\(3\\) встречается чаще других значений — это и есть мода.\nПонятно, что если на нашей шкале нет чисел, а есть текстовые лейблы, это ничего не меняет. Пусть у нас есть переменная с кодами аэропортов:\n\\[\n\\begin{bmatrix}\n\\text{DME} & \\text{LED} & \\text{IST} & \\text{AER} & \\text{IST} &\\text{SVO} & \\text{LED} & \\text{VKO} & \\text{LED} & \\text{IST} & \\text{IST} & \\text{VKO} & \\text{AER} & \\text{DME}\n\\end{bmatrix}\n\\]\n\\[\n\\begin{matrix}\n\\text{Значение} & \\text{DME} & \\text{LED} & \\text{IST} & \\text{AER} & \\text{SVO} & \\text{VKO}\\\\\n\\text{Частота}  & 2 & 3 & 4 & 2 & 1 & 2\n\\end{matrix}\n\\]\nМода — \\(\\text{IST}\\) (Международный аэропорт Стамбула, İstanbul Havalimanı).\nТак мы действуем в случае с эмпирическим распределением. Если нам известна функция вероятности переменной (probability mass function, PMF), то мы можем определить моду, основываясь на ней:\n\nОпределение 7.2 Мода [дискретной переменной] — это значение переменной, при котором её функция вероятности принимает своё максимальное значение.\n\n\\[\n\\text{mode}(X) = \\arg \\max(\\text{PMF}(X)) = \\arg \\max_{x_i}(\\mathbb{P}(X = x_i)),\n\\tag{7.1}\\]\nгде \\(X\\) — дискретная случайная величина, \\(x_i\\) — значение этой случайной величины.\n\n\n\n\n\n\n\n\nРисунок 7.1: Определение моды с помощью функции вероятности\n\n\n\n\n\nОкей, мы видим, что мода отлично считается на дискретных переменных. А как же быть с непрерывными?\nНапомним себе, что вероятность того, что непрерывная случайная величина примет своё конкретное значение, равна нулю. Из этого следует, что все значения непрерывной случайное величины уникальны — каждое повторяется только один раз. Получается, что строить частотную таблицу бессмысленно…\nПо этой причине для непрерывных переменных моду не считают.\n\n7.1.2.1.1 Мода для непрерывной переменной\nДа, это так. Действительно, посчитать моду для непрерывной переменной способом, аналогичным тому, что мы увидели выше, не получится. Однако математиков это не остановило.\nЕсли мы посмотрим на график плотности вероятности (probability density function, PDF), который является аналогом PMF для дискретных переменных, мы увидим, что какие-то значения встречаются чаще, а какие-то реже. Что в общем-то логично. Напомним себе, как это выглядит, например, для любимого [стандартного] нормального распределения:\n\n\n\n\n\n\n\n\nРисунок 7.2: Частоты интервалов значений непрерывной случайной величины на функции плотности распределения\n\n\n\n\n\nТо есть, самые часто встречающиеся значения — это пик распределения. Там и должна быть мода. Визуально это выглядит достаточно справедливо.\nМатематики так и решили:\n\nОпределение 7.3 Мода [непрерывной переменной] — это значение переменной, при котором её функция плотности вероятности достигает локального3 максимума.\n\n\\[\n\\text{mode}(X) = \\arg \\max(\\text{PDF}(X)) = \\arg \\max_{x \\in S}f(x),\n\\tag{7.2}\\]\nгдe \\(X\\) — непрерывная случайная величина, \\(x\\) — значение этой случайной величины, \\(S\\) — имеющаяся выборка значений переменной.\n\n\n\n\n\n\n\n\nРисунок 7.3: Положение моды на функции плотности [стандартного] нормального распределения\n\n\n\n\n\nХотя моду для непрерывной переменной вычислить можно, обычно этого не делают, так как достаточно других мер центральной тенденции для описания распределения.\n\n\n\n7.1.2.2 Унимодальные и полимодальные распределения\nНормальное распределение, как и ряд других — биномиальное, отрицательное биномиальное, пуассоновское — относятся к унимодальным. Такие распределения имеют только одну моду (см. Рисунок 7.4, Рисунок 7.5, Рисунок 7.6).\n\n\n\n\n\n\n\n\nРисунок 7.4: Нормальное распределение (μ = 2$, σ = 0.5). Пунктирной линией обозначено положение моды.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 7.5: Биномиальное распределение (n = 50, p = 0.3). Пунктирной линией обозначено положение моды.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 7.6: Распределение Пуассона (λ = 5.5). Пунктирной линией обозначено положение моды.\n\n\n\n\n\nЭто теоретические распределения. С эмпирическими распределениями дело обстоит так же, хотя они обычно менее гладенькие и красивые (см. Рисунок 7.7 и Рисунок 7.8).\n\n\n\n\n\n\n\n\nРисунок 7.7: Эмпирическое распределение, сгенерированное из нормального распределения (μ = 8, σ = 4, n = 100). set.seed(314). Пунктирной линией обозначено положение моды.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 7.8: Эмпирическое распределение, сгенерированное из логнормального распределения (μ = 1.1, σ = 1.39, n = 30). set.seed(314). Пунктирной линией обозначено положение моды.\n\n\n\n\n\nОднако на практике возможны и другие ситуации. Например, такие (Рисунок 7.9, Рисунок 7.10):\n\n\n\n\n\n\n\n\nРисунок 7.9: Бимодальное распределение. Сгенерировано из двух нормальных распределений (μ1 = 1.5, σ1 = 0.4, n1 = 80; μ2 = 4, σ2 = 0.5, n2 = 40). set.seed(65). Пунктирными линиями обозначены положения мод.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 7.10: Полимодальное распределение. Сгенерировано из двух нормальных распределений (μ1 = 1.5, σ1 = 0.3, n1 = 80; μ2 = 3.4, σ2 = 0.5, n2 = 40) и бета-распределения (α = 2, β = 4, n = 50). set.seed(65). Пунктирными линиями обозначены положения мод.\n\n\n\n\n\nВ первом случае (Рисунок 7.9) мы видим два локальных максимума функции плотности вероятности — такое распределение называется бимодальным. Во втором случае (Рисунок 7.10) функция плотности вероятности имеет три локальных максимума — такое распределение называется полимодальным. Бимональное распределение является частным случаем полимодального распределения.\nВ прицнипе, пиков может быть и больше, однако при работе с реальными данными чаще всего мы сталкиваемся с бимодальными распределениями.\nЧто это значит и что с этим делать?\nБимодальное распределение сигнализирует нам о гетерогенности выборки. Если мы видим два выделяющихся пика, стоит подумать о том, что наша выборка неоднородна и в ней выделяются две подвыборки. Посмотрим на структуру выборки из примера выше (Рисунок 7.11):\n\n\n\n\n\n\n\n\nРисунок 7.11: Структура бимодального распределения из Рисунок 7.9. Для удобства сопоставления графиков плотностей вероятности по оси ординат отложены частоты.\n\n\n\n\n\nДействительно, наше распределение состоит из двух других распределений, у каждого из которого есть своя мода — поэтому итоговое распределение получается бимодальным. Конечно, сейчас нам это очень удобно показать, потому что мы знаем, как это распределение генерировалось. Когда же у нас есть реальные данные и мы там наблюдаем такого «верблюда», бывает достаточно сложно сказать, что «пошло не так».\nСамо по себе распределение не даст нам ответ на вопрос, почему оно бимодальное — чтобы выяснить причины такого поведения переменной нам потребуются другие данные. Обычно у вас в данных есть «соцдем» — пол, возраст, сфера и место работы, уровень обрвазования и др. Попробуйте построить распределение с разбиением исследуемой бимодальной переменной по переменным «соцдема». Это, к сожалению, не является рецептом успеха, поскольку причина гетерогенности выборки может и не содержаться в ваших данных, но такое изучение данных станет хорошим показателем того, что вы не просто «забили» на странное распределение своей переменной, а поисследователи возможные его причины.\nЕсли вам удалось найти причины гетерогенности выборки — допустим, у вас выделяются подвыборки «бакалавры» и «магистры» — стоит подумать о том, как обойтись с этой переменной в планируемом анализе, так как игнорировать её, по-видимому, нельзя, поскольку она влияет на вариатиность данных.\n\n\n\n\n\n\nСоцдем лишним не бывает\n\n\n\nНа этапе планирования исследования подумайте о том, чем могут отличаться ваши респонденты или испытуемые между собой, помимо индивидуальных различий.\n\nЕсли в эксперименте используете задачу мысленного вращения (mental rotation), вполне возможно, испытуемые, работающие в сфере 3D-моделирования или дизайна интерьеров, могут сформировать подвыборку.\nВ случае HR-исследования, где фиксируется доход респондента, необходимо записать город, в котором он проживает и/или работает.\nПри изучении удовлетворенности городским пространством важными пунктами станут беременность, наличие/отсутствие детей, наличие/отсутствие автомобиля и др.\n\nИ так далее. Примеров для каждого случая можно подобрать много.\nСтоит ли, скажем, в первом случае сразу исключить из выборки 3D-моделлеров? Зависит. От количества времени и денег на проведение исследования. Однако как минимум эту информацию надо зафиксировать в данных. А решить, исключать ли этих респондентов из выборки или нет, можно и позже. Главно об этом написать в отчете/статье, когда будете описывать предобработку данных.\n\n\n\n\n7.1.2.3 Медиана\nДля номинальной шкалы мода — это единственно возможная мера центральной тенденции, потому что на этой шкале отсутствует порядок элементов. На других шкалах наблюдения уже можно сортировать по возрастнию или убыванию, поскольку начиная с ранговой (порядковой) шкалы на всех них определена операция сравнения на «больше-меньше».\nВозьмем тот же ряд наблюдений, что и в предыдущем разделе:\n\\[\n\\begin{bmatrix}\n1 & 3 & 4 & 6 & 3 & 2 & 3 & 3 & 2 & 4 & 1\n\\end{bmatrix}\n\\]\nОтсортируем наблюдения по возрастанию:\n\\[\n\\begin{bmatrix}\n1  & 1 & 2 & 2 & 3 & 3 & 3 & 3 & 4 & 4 & 6\n\\end{bmatrix}\n\\]\nНаша задача — определить центральную тенденцию. Давайте посмотрим, что оказалось в середине отсортированного ряда:\n\\[\n\\begin{bmatrix}\n1 & 1 & 2 & 2 & 3 & \\mathbf{3} & 3 & 3 & 4 & 4 & 6\n\\end{bmatrix}\n\\]\nЭто медиана. В данном случае она равна \\(3\\).\n\nОпределение 7.4 Медиана (median) — это значение, которое располагается на середине отсортированного ряда значений переменной.\n\nМедиана делит все наблюдения переменной ровно пополам и половина наблюдений оказывается по одну сторону от медианы, а половина — по другую.\nЕсли число наблюдений нечётное, то всё ясно — в середине отсортированного ряда будет какое-то значение. А если число наблюдений чётное? Тогда мы попадаем между значениями.\nВозьмем для примера такой вектор наблюдений:\n\\[\n\\begin{bmatrix}\n14 & 10 & 9 & 16 & 30 & 3 & 25 & 8 & 18 & 7\n\\end{bmatrix}\n\\]\nОтсортируем:\n\\[\n\\begin{bmatrix}\n3 & 7 & 8 & 9 & 10 & 14 & 16 & 18 & 25 & 30\n\\end{bmatrix}\n\\]\nНайдем середину:\n\\[\n\\begin{bmatrix}\n3 & 7 & 8 & 9 & 10 & | & 14 & 16 & 18 & 25 & 30\n\\end{bmatrix}\n\\]\nВ таком случае в качестве медианы берется среднее между двумя срединными значениями:\n\\[\n\\text{median} = \\frac{10 + 14}{2} = 12\n\\]\nИтого, формализовать вычисление медианы можно следующим образом:\n\\[\n\\text{median}(X) = X(a) =\n\\cases{\nX\\left(\\frac{n+1}{2}\\right), & if  2 | n \\\\\n\\dfrac{X(\\frac{n}{2}) + X(\\frac{n}{2} + 1)}{2}, & otherwise\n}\n\\tag{7.3}\\]\nгде \\(X\\) — ряд наблюдений случайной величины, \\(n\\) — число наблюдений, \\(X(a)\\) — наблюдение с индексом \\(a\\) в отсортированном векторе \\(X\\).\nЕсли мы будем смотреть на медиану с позиции описания распределения, то она будет той самой линией, которая разделит площадь под графиком функции плотности вероятности пополам:\n\n\n\n\n\n\n\n\nРисунок 7.12: Медиана нормального распределения\n\n\n\n\n\nПри этом форма распределения не имеет значения — площадь под графиком всегда будет делиться пополам:\n\n\n\n\n\n\n\n\nРисунок 7.13: Медиана распределения с отрицательной асимметрией.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 7.14: Медиана распределения с положительной асимметрией.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 7.15: Медиана бимодального распределения.\n\n\n\n\n\n\n\n\n7.1.2.4 Среднее\nЕсли наша переменная измерена в самых мощных шкалах — интервальной или абсолютной — то нам доступна ещё одна мера центральной тенденции.\n\n7.1.2.4.1 Арифметическое среднее\nС этим существом все знакомы еще со школы. Арифметическое среднее (arithmetic mean, mean, average) считается так:\n\\[\n\\mathbb{M}_X = \\bar X = \\dfrac{\\sum_{i=1}^{n}x_i}{n},\n\\]\nгде \\(\\bar X\\) — среднее арифметическое, \\(x_i\\) — наблюдение в векторе \\(X\\), \\(n\\) — количество наблюдений.\nНу, то есть всё сложить и поделить на количество того, чего сложили. Изи.\n\n7.1.2.4.1.1 Свойства среднего арифметического\n\nЕсли к каждому значению распределения прибавить некоторое число (константу), то среднее увеличится на это же константу.\n\n\\[\n\\mathbb{M}_{X+c} = \\mathbb{M}_X + c\n\\]\nВот почему:\n\\[\n\\mathbb{M}_{X+c} = \\frac{\\sum_{i=1}^n (x_i + c)}{n} = \\frac{\\sum_{i=1}^n x_i + nc}{n} = \\frac{\\sum_{i=1}^n x_i}{n} + c = \\mathbb{M}_X + c\n\\]\nИначе говоря, распределение просто сдвинется. Например, если к каждому значению синего распределения прибавить \\(2\\), получится красное:\n\n\n\n\n\n\n\n\n\n\nЕсли каждое значение распределение умножить на некоторое число (константу), то среднее увеличится во столько же раз.\n\n\\[\n\\mathbb{M}_{X \\times c} = \\mathbb{M}_X \\times c\n\\]\nВот почему:\n\\[\n\\mathbb{M}_{X \\times c} = \\frac{\\sum_{i=1}^n (x_i \\times c)}{n} = \\frac{c \\times \\sum_{i=1}^n x_i}{n} = \\frac{\\sum_{i=1}^n x_i}{n} \\times c = \\mathbb{M}_X \\times c\n\\]\nНапример, здесь каждое значение синего распределения умножили на \\(3\\) и получили красное:\n\n\n\n\n\n\n\n\n\nТут, правда, явно что-то ещё произошло, но мы пока этого не знаем. Однако, отметит этот факт.\n\nСумма отклонений от среднего значения равна нулю.\n\n\\[\n\\sum_{i=1}^n(x_i - \\bar X) = 0\n\\]\nЭлегантное доказательство:\n\\[\n\\begin{split}\n\\sum_{i=1}^n(x_i - \\bar X) & = \\sum_{i=1}^n x_i - \\sum_{i=1}^n \\bar X = \\sum_{i=1}^n x_i - n \\bar X = \\\\\n& = \\sum_{i=1}^n x_i - n \\times \\frac{1}{n} \\sum_{i=1}^n x_i = \\sum_{i=1}^n x_i - \\sum_{i=1}^n x_i = 0\n\\end{split}\n\\]\nНо можно это осмыслить и более просто графически.\nОтклонение — это разность между средним и конкретным значением переменной. И, действительно, так как среднее находится в центре распределения, то часть значений лежит справа, а часть слева. Значит, будут как положительные, так и отрицательные отклонения — и их сумма в итоге будет равна нулю.\n\n\n\n\n\n\n\n\n\nСреднее арифметическое не одиноко — есть и другие. Встретяться они вам примерно нигде — то есть о-о-о-очень редко и, скорее всего, в каком-то изощрённом виде. Но упомянуть их, пожалуй, стоит.\n\n\n\n7.1.2.4.2 Геометрическое среднее\nРедко встречается в научных работах, но заради общего представления пусть будет. Поскольку оно считается через умножение, то может быть рассчитано только на абсолютной шкале.\n\\[\nG_{X} = \\sqrt[n]{\\prod_{i=1}^n x_i} = \\Big(\\prod_{i=1}^n x_i\\Big)^{\\tfrac{1}{n}}\n\\]\n\n\n7.1.2.4.3 Квадратичное среднее\n\nА вот это уже более полезная история. Мы с ним столкнёмся далее, правда под разными масками.\n\nКвадратичное среднее (quadratic mean, root mean square, RMS) — это квадратный корень из среднего квадрата наблюдений. Ничего не понятно, поэтому по порядку.\n\nесть наблюдение \\(x_i\\)\nзначит есть и его квадрат \\(x_i^2\\)\nмы умеем считать обычно среднее арифметическое, но ведь \\(x_i^2\\) — это тоже наблюдение, просто в квадрате, так?\nзначит можем посчитать среднее арифметическое квадратов наблюдений — средний квадрат\n\n\\[\n\\frac{\\sum_{i=1}^n x_i^2}{n}\n\\]\n\nнорм, а теперь извлечём из этого дела корень — получим то, что там надо\n\n\\[\nS_X = \\sqrt{\\frac{\\sum_{i=1}^n x_i^2}{n}}\n\\]\nPer se4 мы его вряд ли ещё когда-то увидим, но пару раз оно внезапно всплывет.\n\n\n7.1.2.4.4 Гармоническое среднее\n\nСуперэкзотичный покемон.\n\n\\[\nH_X = \\frac{n \\prod_{i=1}^n x_i}{\\sum_{i=1}^n (\\tfrac{1}{x} \\prod_{j=1}^n x_j)} = \\frac{n}{\\sum_{i=1}^n \\tfrac{1}{x_i}}\n\\]\n\n\n7.1.2.4.5 Взвешенное среднее\nЧасто бывает такая ситуация, что нас нужно посчитать среднее по каким-либо имеющимся параметрам, но одни параметры для нас важнее, чем другие. Например, мы хотим вычислить суммарный балл обучающегося за курс на основе ряда работ, выполненных в течение курса, однако мы понимаем, что тест из десяти вопросов с множественном выбором явно менее показателен, чем, например, аналитическое эссе или экзаменационная оценка. Что делать? Взвесить параметры!\nЧто значит взвесить? Умножить на некоторое число. На самом деле, любое. Пусть мы посчитали, что написать эссе в три абстрактных раза тяжелее, чем написать тест, а сдать экзамен в два раза тяжелее, чем написать эссе. Тогда мы можем присвоить баллу за тест вес \\(1\\), баллу за аналитическое эссе вес \\(3\\), а экзамену — вес \\(6\\). Тогда итоговая оценка за курс будет рассчитываться следующим образом:\n\\[\n\\text{final score } = 1 \\cdot \\text{test} + 3 \\cdot \\text{essay} + 6 \\cdot \\text{exam}\n\\]\nСуперкласс. Однако! Весьма вероятно, что в учебном заведении принята единая система оценки для всех видов работ (ну, скажем, некая абстрактная десятибалльная система в сферическом вакууме). Получается, если и за тест, и за эссе, и за экзамен у студента по 10 баллов, то суммарный балл 100, что, кажется, больше, чем 10. Чтобы вернуться к изначальным границам баллов, нужно моделить суммарный балл на сумму весов параметров:\n\\[\n\\text{final score } = \\frac{1 \\cdot \\text{test} + 3 \\cdot \\text{essay} + 6 \\cdot \\text{exam}}{1 + 3 + 6}\n\\]\nКайф! Собственно, это и есть взвешенное среднее. Коэффициенты, на которые мы умножаем значение парамернов, называются весами параметров. И в общем виде формула принимает следующий вид.\n\\[\n\\bar X = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i} = \\sum_{i=1}^n w_i' x_i,\n\\]\nгде \\(x_i\\) — значения конкретных параметров, \\(w_i\\) — веса конкретных параметров, \\(w_i'\\) — нормированные веса параметров.\nВторая часть формулы показывается нам, что можно облегчить себе вычислительную жизнь, если заранее нормировать веса, то есть разделить каждый коэффициент на сумму коэффициентов:\n\\[\nw_i' = \\frac{w_i}{\\sum_{i=1}^n w_i}\n\\]\nТогда сумма коэффициентов будет равна единице. Так чаще всего и поступают, так как тогда коэффициент будет представлять долю, которую весит данный параметр в суммарной оценке. Удобно, практично, красиво.\nВзвещенное среднее часто применяется именно во всякого рода ассессментах, и не только образовательных. Например, вы HR-аналитик и оцениваете персонал. Вы аналитически вычисляете веса коэффициентов (допустим, с помощью линейной регрессии), а далее на их основе высчитаете интегральный балл, по которому будете оценивать сотрудников. Это как один из индустриальных примеров.\nТакже оно применяется, когда в наших данных есть какая-то группировка (например, когорты), при этом группы неравномерны.\n\n\n\n7.1.2.5 Среднее vs медиана\nПомимо того, что среднее и медиана информативны сами по себе, полезно смотреть на их взаимное расположение.\n\nСравнивать будем моду, медиану и среднее [арифметическое].\n\nИтак, все три статистики — мода, медиана и среднее — описывают центральную тенденцию — некоторое значение изучаемой нами переменной, вокруг которого собираются другие значения. Но если их три и все они используются, значит между ними должны быть какие-то различия. Посмотрим, какие.\nВо-первых, моду невозможно посчитать для непрерывной переменной.\nТак как вероятность того, что непрерывная случайная величина принимает своё конкретное значение, равна нулю, каждое наблюдение в нашей выборке будет уникально — встретится ровно один раз. Вспомните [посмотрите] пример из предыдущей главы, где мы набирали числа из отрезка. Получается, что мода теряет свой смысл.\nВо-вторых, медиану нельзя посчитать на номинальной шкале.\nНа номинальной шкале нет отношения порядка между элементами. Помните, на ней нельзя сравнивать на больше-меньше. Поэтому невозможно отсортировать наблюдения, а значит, и найти медиану.\nВ-третьих, среднее тоже нельзя посчитать на номинальной шкале.\nВообще, конечно, да — нельзя, потому что на номинальной шкале не определена операция сложения, входящая в вычисление среднего. Однако если на номинальной шкале есть только две категории, которые закодированы 0 и 1, то посчитать среднее можно. Но что оно будет значить?\nИсходный математический смысл среднего явно утерян. Посмотрим на это по-другому: посчитать сумму единиц это всё равно, что посчитать количество единиц. То есть, если мы сложим все нули и единицы, то получим количество единиц среди всех наших наблюдений. А разделив количество единиц на количество наблюдений, мы получим долю единиц — то есть долю наблюдений с лейблом 1.\nВ-четвертых, для дискретной переменной значение среднего арифметического будет не особо осмысленно. Ну, скажем, странно сказать, что в аудитории в среднем стоят 15.86 столов или в российских семьях в среднем 1.5 ребенка. Конечно, в ряде случаев можно это как-то более-менее содержательно интерпретировать, но это требует усилий, а мы ленивые, поэтому лучше использовать медиану.\nИтого, делаем следующие выводы:\n\nдля номинальной шкалы пригодна только мода\nдля дискретных переменных подходят мода и медиана\n\nмода иногда лучше, так как точно всегда будет целым числом\n\nдля непрерывных переменных подходят медиана и среднее\n\nТеперь нам надо разобраться, как будут себя вести меры центральной тенденции в зависимости от формы распределения.\nНа симметричном распределении мода, медиана и среднее совпадают [или, по крайней мере, находятся очень близко друг к другу]. Здесь и далее: красная линия — среднее, синяя — медиана, зелёная — мода.\n\n\n\n\n\n\n\n\n\nНа асимметричном распределении мода [практически] в пике. Практически, потому что функция плотности вероятности [черная линия на графике] на всегда точно аппроксимирует (в данном случае то же, что и сглаживает) эмпирическое распределение. На картинке ниже мы видим, что на гистограмме мода — самый высокий столбик, что и показывает нам зелёная линия, которой обозначена мода. Однако при сглаживании гистограммы пик немного съехал, и мода оказалась не совсем в вершине графика функции плотности вероятности.\nВообще-то это нормально, потому что мода для непрерывной величины, которую мы и визуализируем с помощью графика плотности, либо не может быть посчитана вовсе, либо — если так получилось, и у нас все же есть повторяющиеся значения — не слишком хорошая мера центральной тенденции. В целом, и на симметричном распределении мода тоже может находиться немного в стороне от пика.\nНа асимметричном распределении медиана и среднее смещены в сторону хвоста. Среднее смещено сильнее медианы. Это связано с тем, что медиана зависит только от количества наблюдений, а среднее ещё и от самих значений. На картинке ниже пример для распределения с правосторонней асимметрии (потому что хвост справа) — среднее (красная линия) правее медианы (синяя линия).\n\n\n\n\n\n\n\n\n\nА это пример для распределения с левосторонней асимметрией (так как хвост слева) — среднее (красная линия) левее медианы (синяя линия).\n\n\n\n\n\n\n\n\n\nДля того, чтобы лучше разобраться с тем, как большие и малые значения влияют на моду и медиану посмотрим такой пример. Пусть у нас есть оценки за выпускную квалификационную работу. Например, такие:\n\n\n[1] 6 7 7 8 8\n\n\nПосчитаем медиану и среднее:\n\n\n[1] 7\n\n\n[1] 7.2\n\n\nСреднее \\(7.2\\) округлиться до \\(7\\), то есть можно считать, что среднее и медиана совпали. Ну, ок.\nНо в комиссии сидят два требовательных доктора наук, которые поставили оценки, сильно отличающиеся от остальных:\n\n\n[1] 6 7 7 8 8 3 4\n\n\nПосчитаем медиану и среднее теперь:\n\n\n[1] 7\n\n\n[1] 6.142857\n\n\nМедиана осталась на месте — всё ещё \\(7\\). А вот среднее \\(6.1\\) округлится до \\(6\\). Казалось бы, это немного, но в смысле оценок — это прилично, и может сильно повлиять на GPA.\nИтого, среднее более чувствительно к нетипичным значениям (очень большим или очень малым).\nЕсть ещё один интересный вариант распределений — бимодальные. Значит ли, что у этого распределения две моды? Не всегда. Посмотрим пример ниже:\n\n\n\n\n\n\n\n\n\nМы видим, что на графике есть два пика, однако строго математически мода одна (зеленая линия) — и она в более высоком пике. Это логично, ибо там самые часто встречающиеся значения.\nСо средним и медианой происходит примерно то же, что и в случае асимметричного распределения. Второй пик смещает к себе обе меры центральной тенденции, причем среднее вновь сильнее, чем медиану.\n\n\n\n7.1.3 Меры разброса\nИтак, мы разобрались с мерами центральной тенденции. Однако для описания распределения их оказвается недостаточно. Почему?\n\n7.1.3.1 Зачем нужны меры разброса\nПосмотрим на несколько распределений:\n\n\n\n\n\n\n\n\n\nМетодом пристального взгляда можно установить, что у всех распределений одинаковые средние:\n\n\n\n\n\n\n\n\n\nОднако мы видим, что значения по-разному группируются вокруг среднего. Как они группируются — плотно, как на третьем рисунке, или не особо, как на втором — можно описать с помощью мер разброса, или мер вариативности.\n\n\n7.1.3.2 Минимум, максимум, размах\nНачнем с самого простого. Как наиболее просто описать вариативность? Мы работаем с выборкой, а в выборке, как известно, ограниченное число наблюдений. А если оно ограниченое, значит среди них точно есть наибольшее — максимальное — и наименьшее — минимальное.\nДопустим, мы открыли ведомость по «Анатомии и физиологии ЦНС» некоторой академической группы и пронаблюли следующее:\n\n\n [1]  7  4  6  9 10  5  6  9  6  6  3  6  8  8  5 10  7  5  7  3  9  4  8  3  8\n[26]  4  6  8  7  5\n\n\nМы можем посчитать минимальное и максимальне значение по этому ряду наблюдений:\n\n\n[1] 3\n\n\n[1] 10\n\n\nПолучается, что оценки варьируются от \\(3\\) до \\(10\\). Ну, приемлемо. Разница между максимальным и минимальным значением называется размах (range):\n\\[\n\\mathrm{range}(X) = \\max(X) - \\min(X)\n\\]\nИ вот мы преисполнившиеся идёт описывать вариативность переменной с помощью размаха, но обнаруживаем в другой ведомости этой же группы (по «Введению в психологию») вот что:\n\n\n [1]  6  8  4  6  7  5  7 10  4  6  7  8  7  6  8 10  8  7  7  6  8  7  6  8  6\n[26]  3  8  6  6  4\n\n\nРазмах вроде как такой же:\n\n\n[1]  3 10\n\n\nЗначит ли это, что вариативность одинаковая?\nНарисуем.\n\n\n\n\n\n\n\n\n\nКажется, что вариативность различна. Распределение оценок по «Анатомии и физиологии ЦНС» более равномерное, в то время как оценки по «Введению в психологию» активнее группируются где-то в середине.\nШтош, размах хоть и дает нам некоторую информацию о вариативности, нам этого маловато. Будем искать другие меры разброса.\n\n\n7.1.3.3 Дисперсия\nХотя описание разброса переменных с помощью квантилей (в частности, квартилей) может дать нам много полезной информации, все же у них есть существенный недостаток: они никак не взаимодействуют с самими значениями нашей переменной.\nДействительно, мы делим нашу сортированную выборку на равные части, и смотрим, что в эти части попало. Но хотелось бы как-то учесть ещё и сами значения переменной в некотрой числовой мере разброса.\nНу, хорошо. Поступим следующим образом. Мы все ещё хотим узнать, как наши значения группируются вокруг среднего. В предыдущей главе мы уже видели, что наши наблюдения отклоняются от среднего значения — значит мы можем посчитать отклонение для каждого наблюдения:\n\\[\nd_i = \\bar X - x_i\n\\]\nОкей. Если мы сложим все отклоненияи и поделим на их количество (которое равно количеству наблюдений), то мы получим среднее отклонение, да?\n\\[\n\\bar d = \\frac{1}{n} \\sum_{i=1}^n \\bar X - x_i\n\\]\nДа. Однако есть одна проблема. Ранее мы выяснили, что сумма отклонений от среднего равна нулю, а значит и среднее отклонение также будет равно нулю.\nХорошо. Но отрицательные значения ведь можно победить! Есть два пути:\n\nМодуль. Преимущество первого в том, что размерность величины разброса остается той же, что и у измеряемой переменной.\nКвадрат. Преимущество второго в том, что сильные отклонения будут оказывать более сильное влияние на окончательное значение статистики, в то время как для первого малые и большие отклонения равноценны.\n\nВторой путь на практике оказывается полезнее, так как мы хотим, чтобы сильно отличающиеся наблюдения вносили вклад в меру разброса.\nВозведя отклонения в квадрат, получим формулу дисперсии (вариации, variation):\n\\[\n\\mathbb{D}_X = \\text{var}(X) = \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (\\bar X - x_i)^2\n\\]\nГениально.\nНе совсем. Формула, которую мы получили, пригодна для расчета дисперсии генеральной совокупности — на выборке же она будет давать смещенную оценку. Это мы выводили математически и проверяли на симуляциях в предыдущем блоке.\nЧтобы получить точную (несмещенную) оценку дисперсии по выборке, нам нужно исправить знаменатель дроби — вместо \\(n\\) использовать \\(n-1\\):\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^n (\\bar X - x_i)^2\n\\]\nНо почему?\n\n7.1.3.3.1 Степени свободы\nВо всём виновата выборка.\nВзглянем на формулу дисперсии: в неё входит среднее арифметическое. То есть для того, чтобы рассчитать дисперсию на выборке, сначала нам необходимо на этой же выборке рассчитать среднее. Тем самым, мы как бы «фиксируем» нашу выборку этим средним значением — у значений нашего распределения становится меньше свободы для варьирования. Теперь свободно варьироваться могут \\(n-1\\) наблюдение, так как последнее всегда будет возможно высчитать, исходя из среднего значения. По этой причине нам необходимо корректировать исходную формулу расчета дисперсии.\n\n\n\n7.1.3.4 Стандартное отклонение\nИ вот мы получили невероятное! У нас есть формула расчета меры разброса, которая позволяет учесть сами значения переменной! Ну не чудо ли!\nЧудо, конечно, однако есть некоторая проблема. Мы возводили отклонения в квадрат. Представим, что мы хотим посчитатить дисперсию роста студентов психфака. Пусть мы измеряли рост в метрах. Отклонения тоже будут в метрах (потому что среднее — это тоже метры, а если из метров вычитать метры, то мы получим метры). А при возведении метров в квадрат получаются метры в квадрате. Очевидно, что если мы модели квадратные метры на некоторое число (\\(n\\)), они все еще останутся метрами в квадрате.\nО, нет! А счастье было так близко, так возможно! Получается, мы не можем интерпретировать эту меру разброса? Не сможем даже нарисовать?\nДа, но это не очень большая беда. Для того, чтобы вернуться обратно к единицам измерения нашей переменной, нам всего лишь нужно извлечь корень из дисперсии:\n\\[\n\\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\bar X - x_i)^2}\n\\]\nМы получили величину, называемую стандартным отклонением (standard deviation). Чем она хороша? Тем, что её размерность совпадает с размерностью нашей переменной. Стандартное отклонение уже может быть достаточно интерпретабельно и хорошо визуализируемо.\nКстати, формула выше, которая что-то очень напоминает, — это стандартное отклонение генеральной совокупности, потому что под корнем стоит дисперсия генеральной совокупности.\nЧтобы посчитать стандартное отклонение по выборке, нам надо извлечь корень из выборочной дисперсии:\n\\[\ns = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (\\bar X - x_i)^2}\n\\]\n\n\n7.1.3.5 Свойства дисперсии и стандартного отклонения\n\nЕсли к каждому значению распределения прибавить некоторое число (константу), то дисперсия не изменится.\n\n\\[\n\\mathbb{D}_{X+c} = \\mathbb{D}_{X}\n\\]\nВот почему:\n\\[\n\\begin{split}\n\\mathbb{D}_{X+c} &= \\frac{\\sum_{i=1}^n \\big((\\bar X + c) - (x_i + c)\\big)^2}{n-1} = \\\\\n&= \\frac{\\sum_{i=1}^n \\big(\\bar X + c - x_i - c\\big)^2}{n-1} \\\\\n& = \\frac{\\sum_{i=1}^n \\big(\\bar X - x_i\\big)^2}{n-1} = \\mathbb{D}_X\n\\end{split}\n\\]\n\nЕсли каждое значение распределение умножить на некоторое число (константу), то дисперсия увеличится в \\(c^2\\) раз.\n\n\\[\n\\mathbb{D}_{X \\cdot c} = c^2\\mathbb{D}_{X}\n\\]\nВот почему:\n\\[\n\\mathbb{D}_{X \\cdot c} = \\frac{\\sum_{i=1}^n (c\\bar X - cx_i)^2}{n-1} = \\frac{\\sum_{i=1}^n c^2(\\bar X - x_i)^2}{n-1} = \\frac{c^2 \\sum_{i=1}^n (\\bar X - x_i)^2}{n-1} = c^2\\mathbb{D}_X\n\\]\n\nЕсли к каждому значению распределения прибавить некоторое число (константу), то стандартное отклонение не изменится.\n\n\\[\ns_{X+c} = s_X\n\\]\nЭто следует из свойства дисперсии:\n\\[\ns_{X+c} = \\sqrt{s^2_{x+c}} = \\sqrt{s^2_x} = s_x\n\\]\nКак мы уже видели, распределение просто сдвигается на константу. Например, если к каждому значению синего распределения прибавить \\(2\\), получится красное — разброс у обоих распределений одинаковый:\n\n\n\n\n\n\n\n\n\n\nЕсли каждое значение распределение умножить на некоторое число (константу), то стандартное отклонение увеличится во столько же раз.\n\n\\[\ns_{X \\cdot c} = c\\cdot s_X\n\\]\nЭто также следует из свойства дисперсии:\n\\[\ns_{X \\cdot c} = \\sqrt{s^2_{X \\cdot c}} = \\sqrt{s_X \\cdot c^2} = c \\cdot s_x\n\\]\nНапример, здесь каждое значение синего распределения умножили на \\(3\\) и получили красное — разброс также увеличился в три раза, поэтому распределение более плоское:\n\n\n\n\n\n\n\n\n\n\n\n\n7.1.4 Квантили\nВозьмем распределение суммарного балла по шкале «Доверие к техническим интеллектуальным системам». Выглядит оно как-то так:\n\n\n\n\n\n\n\n\n\nТеперь нам понадобится определение квантиля распределения.\nКвантиль — это значение переменной, которое не превышается с определенной вероятностью (обозначим её \\(p\\)). Иначе говоря, слева от значения квантиля лежит \\(p\\%\\) наблюдений.\nПосмотрим на картинки.\nСлева относительно квантиля-0.05 (\\(x_{0.05}\\)) лежит 5% наблюдений:\n\n\n\n\n\n\n\n\n\nСлева относительно квантиля-0.68 (\\(x_{0.68}\\)) лежит 68% наблюдений:\n\n\n\n\n\n\n\n\n\nСлева относительно квантиля-0.99 (\\(x_{0.99}\\)) лежит 99% наблюдений:\n\n\n\n\n\n\n\n\n\nИтак, мы поняли, а также приняли и осознали, что такое квантиль. Неясно только, как он нам поможет описать вариативность данных.\n\n7.1.4.1 Квартили\nДля этого нам пригодятся специально обученные квантили. Оказалось достаточно удобно поделить все наблюдение на четыре равные части — вот так:\n\n\n\n\n\n\n\n\n\nЗначения переменной, которые делят выборку на четыре равные части называются квартили. Получается, что\n\nслева от первого (нижнего) квартиля (\\(Q_1\\), \\(x_{0.25}\\)) лежит 25% наблюдений\nслева от второго (среднего) квартиля (\\(Q_2\\), \\(x_{0.50}\\)) лежит 50% наблюдений\n\nа значит и справа 50% — получается второй квартиль делит выборку пополам — это медиана\n\nслева от третьего (верхнего) квартиля (\\(Q_3\\), \\(x_{0.75}\\)) лежит 75% наблюдений\n\nЧетвертый квартиль не используется, потому что это максимальное значение — слева от него лежит 100% наблюдений.\nКстати, можно также отметить, что первый квартиль — это медиана нижней (меньшей) половины наблюдений, а третий — медиана верней (большей) половины наблюдений.\nВот такая вот прикольная история.\n\n\n7.1.4.2 Децили\nК слову, делить выборку можно не только на четверти — можно поделить, скажем, на 10 частей и получить децили. Так, слева от первого дециля (\\(x_{0.10}\\)) лежит 10% наблюдений, а слева от третьего (\\(x_{0.30}\\)) — 30%.\nДецили встречаются редко (в основном в психометрике), но знать о них полезно.\n\n\n7.1.4.3 Перцентили\nГораздо чаще встречаются перцентили — значения переменной, которые делят выборку на 100 равных частей. Например, так устроен ваш рейтинг. Только стоит помнить, что в рейтинге отсчет ведется от максимального среднего балла, поэтому если у вас нулевой перцентиль (\\(x_{0.00}\\)) по программе, значит выше вас в рейтинге никого нет. А если ваш перцентиль, скажем, 36-ой (\\(x_{0.36}\\)), то выше вас в рейтинге 36% ваших однокурсников, то есть вы все ещё в первой половине рейтинга, что очень неплохо!\n\n\n7.1.4.4 Интерквартильный размах\nИ — о, ура! — мы наконец-то добрались до того, ради чего тут собрались! Зная первый и третий квартили распределения, можно рассчитать интерквартильный (межквартильный) размах (interquartile range, IQR).\n\\[\n\\mathrm{IQR}(X) = Q_3(X) - Q_1(X)\n\\]\nИнтерквартильный размах — это разница между третьим и первым квартилем распределения. Эта величина описывает интервал значений признака, в котором лежит 50% наблюдений.\n\n\n\n\n\n\n\n\n\nВ данном случае он равен 40 — то есть 50% наблюдений лежит в пределах 40 единиц шкалы.\n\n\n7.1.4.5 Визуализация квартилей. Боксплот\nОтображать квартили на гистограмме, во-первых, совершенно неудобно, а во-вторых, не то чтобы график получается информативный. Для визуализации квартилей придумали специальный тип графика — ящик с усами, или боксплот (boxplot).\n\n\n\n\n\n\n\n\n\nПрикольная ерунда. Научимся его читать.\nЗначения переменной идут по вертикальной оси (оси ординат). По горизонтальной оси (оси абсцисс) здесь ничего не идет5. Жирная линия по середине ящика — медиана (второй квартиль). Нижняя граница ящика — первый квартиль, верхняя — третий. Получается, что границы ящика показывают нам значения, в пределах которых лежит половина наблюдений.\nНижний ус — первый квартиль минус полтора межквартильных размаха. Верхний ус — третий квартиль плюс полтора мехквартильных размаха.\n\n\n\n\n\n\n\n\n\n\n\n\nЗамечание\n\nЯщик может быть асимметричным — то есть верхняя его часть (расстояние между медианой и третьим квартилем) и нижняя его часть (расстояние между медианой и первым квартилем) могут быть разными. Это нам говорит об асимметричности распределения. Усы также могут быть неравными, если один из них упирается в максимум / минимум — тоже по причине асимметричности распределения.\n\n\nНу, допустим. А что тогда точки?\n\n7.1.4.5.1 Выбросы\nВообще справедливо было бы задаться вопросом, а зачем нам вообще усы на этом графике? И почему мы прибавляем полтора межквартильных размаха?\nЭто один из подходов к определению нехарактерных значений — выбросов. При исследовании данных мы часто задаемся вопросом, если ли в наших данных такие значения, которые сильно отличаются от распределения той или иной переменной. Но как определить это самое «сильно»?\nВот один из подходов. Будем считать, что значения, которые укладываются в интервал \\((Q_1 - 1.5 \\times \\mathrm{IQR}, \\, Q_3 + 1.5 \\times \\mathrm{IQR})\\), нас устраивают. Все что попадает в этот интервал — это «нормальные», типичные значения нашей переменной. Те же, которые будут находиться за пределами этого интервала, мы назовем нетипичными, аномальными значениями, или выбросами. Эти значения и будут отмечены точками на графике boxplot.\nЧто с ними делать? Во-первых, содержательной анализировать. Выбросы могут возникнуть по разным причинам. Может быть испытуемый отвлекся на прилетевшего в окно голубя, и у нас в данных появилось время реакции 200 секунд. Такие выбросы мы можем исключить из данных. А возможно в нашу выборку попали какие-то люди, которые, скажем, очень сильно или очень слабо доверяют искусственному интеллекту (как в примере на рисунке). Эти наблюдения необходимо дополнительно проанализировать — возможно, это представители специфических групп нашей генеральной совокупности (например, программисты-разработчики или люди пенсионного возраста). Анализ принесет нам дополнительную информацию, которую мы могли не учесть при планировании исследования. Крч, думать надо. И собирать побольше данных, чтобы можно было найти содержательную интерпретацию происходящему.\n\n\n\n\n7.1.5 Сравнение мер разброса\nКак и разные меры центральной тенденции, разные меры разброса по-своему хороши. Более того, они дружат с мерами центральной тенденции. Так, с медианой используется мехквартильных размах, а со средним арифметическим — стандартное отклонение.\nРазмах подходит для всего сразу. Его стоит рассчитать, чтобы составить самое первое представление в разбросе, о границах измерения изучаемого признака [на нашей выборке].\nСтоит также отметить, что все, что мы тут обсуждали, совершенно не годиться для номинативных переменных. Однако у них тоже есть вариативность. Согласитель, что выборка из Питера, Москвы, и Казани более вариативна, чем выборка из Москвы. Аналогом меры разброса для номинальной переменной можно назвать количество уникальных значений этой переменной.\n\n\n7.1.6 Асимметрия\nВыше мы видели, что распределения бывают асимметричными, и нам бы хотелось каким-то образом — желательно, числовым — эту асимметричность описывать. Для этого среди описательнрых статистик существует коэффициент асимметрии. Приведем его формулу, но запоминать не будем, потому что она в целом не особо нужна — всё-таки мы доверяем R считать всякие вычисления.\n\\[\n\\mathrm{skew}(X) = \\frac{\\frac{1}{n}\\sum_{i=1}^n (\\bar X - x_i)^3}{\\left(\\frac{1}{n-1} \\sum_{i=1}^n (\\bar X - x_i)^2 \\right)^{3/2}}\n\\]\nОзнакомимся прежде всего в интерпретацией значений коэффицинета асимметрии:\n\nположительный коэффициент асимметрии (positive skew) указывает на наличие длинного правого хвоста распределения, соответственно всё распределение будет скошено влево (то есть преобладают низкие значения)\nотрицательный коэфффициент асимметрии (negative skew) указывает на наличие длинного левого хвоста распределения, соответственно всё распределения будет скошено вправо (то есть преобладают высокие значения)\nзначения коэффициента асимметрии, близкие к нулю, говорят о симметричности распределения\n\nПосмотрим на картинках:\n\nсимметричное распределение, коэффициент асимметрии равен нулю:\n\n\n\n\n\n\n\n\n\n\n\nлевосторонняя асимметрия, коэффициент асимметрии отрицательный:\n\n\n\n\n\n\n\n\n\n\n\nправосторонняя асимметрия, коэффициент асимметрии положительный:\n\n\n\n\n\n\n\n\n\n\nСодержательная интерпретация асимметрии очень сильно зависит от исследовательской области — когда-то мы вполне ожидаем асимметричность (например, для времени реакции), а когда-то это может свидетельствовать о проблемах с выборкой или, в случае психометрики, о проблемах формулировок вопросов.\n\n\n7.1.7 Эксцесс\nПомимо симметричности эмпирического распределения нас часто интересует, насколько наше распределение растянулось по горизонтальной оси. Это определяется коэффициентом эксцесса. Вновь приведем формулу, но не будем на ней останавливаться.\n\\[\n\\mathrm{kurt}(X) = \\frac{\\frac{1}{n}\\sum_{i=1}^n (\\bar X - x_i)^4}{\\left(\\frac{1}{n-1} \\sum_{i=1}^n (\\bar X - x_i)^2 \\right)^{2}} - 3\n\\]\nОпять же остановимся, прежде всего, на интерпретации значений коэффициента эксцесса:\n\nнулевой коэффициент эксцесса обозначает такой же эксцесс, как у стандартного нормального распределения (то есть, «нормальный»)\nположительный коэффициент эксцесса обозначает, что распределение имеет более острую вершину (то есть у нас очень много средних значений, но тонкие «хвосты» — мало низких и высоких значений)\nотрицательный коэффициент эксцесса обозначает, что распределение имеет более пологую вершину (то есть у нас меньше средних значений и толстые «хвосты» — много низких и высоких значений)\n\nИ также посмотрим на картинки:\n\n[стандартное] нормальное распределение, коэффициент эксцесса равен нулю\n\n\n\n\n\n\n\n\n\n\n\nвысокий пик распределения, коэффициент эксцесса положительный\n\n\n\n\n\n\n\n\n\n\n\nнизкий пик распределения, коэффициент эксцесса отрицательный",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "l7.html#andan-descriptives-kinds-of-stats",
    "href": "l7.html#andan-descriptives-kinds-of-stats",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "7.2 Виды статистики",
    "text": "7.2 Виды статистики\nСтатистика [как набор методов и инструментов] делится на два вида — описательная статистика и статистика вывода.\n\nОписательная статистика (descriptive statistics1) занимается обработкой статистических данных, их наглядным представлением, и собственно описанием через некоторые характеристики.\n\nЭти характеристики, количественно описывающие особенности имеющихся данных, называются описательными статистиками (descriptive statistics2).\nЗадача описательной статистики — ёмко описать имеющиеся данные и составить на основе этих описаний общее представление о них, а также обнаружить особенности, которые могут повлиять на дальнейший анализ.\n\nСтатистика вывода (inferential statistics) занимается поиском ответов на содержательные вопросы, которые мы задаем данным в ходе их анализа в рамках научных и практических исследований.\n\nСостоит из двух компонентов — тестирования статистических гипотез и статистических методов.\n\n\n\n\n\n\n\n\nЗамечание о машинном обучении\n\n\n\nВ названии книги упомянуто «машинное обучение». Иногда его причисляют к статистике, иногда рассматривают отдельно. На самом же деле, статистические методы лежат где-то между статистикой вывода и машинным обучением.\nПочему?\nДело в том, что на статистические методы можно смотреть по-разному.\n\nЕсли нашей задачей является поиск ответов на исследовательские вопросы о закономерностях, о связи каких-либо факторов или влиянии переменных друг на друга, то мы будем смотреть на статистические модели с точки зрения статистики вывода. Это позволит нам находить ответы на интересующие нас вопросы — причем не важно, говорим мы о научных исследованиях или об исследованиях в индустрии.\nЕсли перед нами стоит задача хорошо предсказывать одни переменные на основании значений других — например, выдавать рекомендации на Яндекс.Музыке или в Яндекс.Лавке — то мы будем смотреть на те же статистические модели с точки зрения машинного обучения.\n\nТо есть, модели в анализе данных и машинном обучении одни и те же, но то, какую модель мы назовем хорошей и как мы эту «хорошесть» определим, будет отличаться в зависимости от задачи — исследовательская или предиктивная — которая перед нами стоит.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "l7.html#andan-descriptives-central-tendency",
    "href": "l7.html#andan-descriptives-central-tendency",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "7.3 Меры центральной тенденции",
    "text": "7.3 Меры центральной тенденции\nИтак, мы хотим описать наши данные. Точнее, распределения переменных, которые у нас в данных есть. Хотим сделать это просто и ёмко. Насколько просто и ёмко? Ну, допустим максимально — одним числом. Для этого неплохо подойдет значение переменной, которое лежит в центре распределения.\nКак мы будем искать, что там в центре распределения? Зависит от шкалы, в которой измерена конкретная переменная.\n\n\n\nШкала\nМера центральной тенденции\n\n\n\n\nНоминальная\nМода\n\n\nПорядковая\nМедиана\n\n\nИнтервальная\nСреднее арифметическое\n\n\nАбсолютная\nСреднее арифметическое, геометрическое и др.\n\n\n\nОднако есть некоторые нюансы.\n\n7.3.1 Мода\nСамый простой вариант найти центральную тенденцию — это определить наиболее часто встречающееся значение переменной. Это значение называется модой (mode).\n\nОпределение 7.1 Мода [дискретной переменной] — наиболее часто встречающееся значение данной переменной.\n\nНапример, у нас есть следующий ряд наблюдений по какой-то переменной:\n\\[\n\\begin{bmatrix}\n1 & 3 & 4 & 6 & 3 & 2 & 3 & 3 & 2 & 4 & 1\n\\end{bmatrix}\n\\]\nЕсли мы посчитаем, сколько раз встретилась каждое значение переменной и составим таблицу частот, то получим следующее:\n\\[\n\\begin{matrix}\n\\text{Значение} & 1 & 2 & 3 & 4 & 6 \\\\\n\\text{Частота}  & 2 & 2 & 4 & 2 & 1\n\\end{matrix}\n\\]\nОчевидно, что \\(3\\) встречается чаще других значений — это и есть мода.\nПонятно, что если на нашей шкале нет чисел, а есть текстовые лейблы, это ничего не меняет. Пусть у нас есть переменная с кодами аэропортов:\n\\[\n\\begin{bmatrix}\n\\text{DME} & \\text{LED} & \\text{IST} & \\text{AER} & \\text{IST} &\\text{SVO} & \\text{LED} & \\text{VKO} & \\text{LED} & \\text{IST} & \\text{IST} & \\text{VKO} & \\text{AER} & \\text{DME}\n\\end{bmatrix}\n\\]\n\\[\n\\begin{matrix}\n\\text{Значение} & \\text{DME} & \\text{LED} & \\text{IST} & \\text{AER} & \\text{SVO} & \\text{VKO}\\\\\n\\text{Частота}  & 2 & 3 & 4 & 2 & 1 & 2\n\\end{matrix}\n\\]\nМода — \\(\\text{IST}\\) (Международный аэропорт Стамбула, İstanbul Havalimanı).\nТак мы действуем в случае с эмпирическим распределением. Если нам известна функция вероятности переменной (probability mass function, PMF), то мы можем определить моду, основываясь на ней:\n\nОпределение 7.2 Мода [дискретной переменной] — это значение переменной, при котором её функция вероятности принимает своё максимальное значение.\n\n\\[\n\\text{mode}(X) = \\arg \\max(\\text{PMF}(X)) = \\arg \\max_{x_i}(\\mathbb{P}(X = x_i)),\n\\tag{7.1}\\]\nгде \\(X\\) — дискретная случайная величина, \\(x_i\\) — значение этой случайной величины.\n\n\n\n\n\n\n\n\nРисунок 7.1: Определение моды с помощью функции вероятности\n\n\n\n\n\nОкей, мы видим, что мода отлично считается на дискретных переменных. А как же быть с непрерывными?\nНапомним себе, что вероятность того, что непрерывная случайная величина примет своё конкретное значение, равна нулю. Из этого следует, что все значения непрерывной случайное величины уникальны — каждое повторяется только один раз. Получается, что строить частотную таблицу бессмысленно…\nПо этой причине для непрерывных переменных моду не считают.\n\n7.3.1.1 Мода для непрерывной переменной\nДа, это так. Действительно, посчитать моду для непрерывной переменной способом, аналогичным тому, что мы увидели выше, не получится. Однако математиков это не остановило.\nЕсли мы посмотрим на график плотности вероятности (probability density function, PDF), который является аналогом PMF для дискретных переменных, мы увидим, что какие-то значения встречаются чаще, а какие-то реже. Что в общем-то логично. Напомним себе, как это выглядит, например, для любимого [стандартного] нормального распределения:\n\n\n\n\n\n\n\n\nРисунок 7.2: Частоты интервалов значений непрерывной случайной величины на функции плотности распределения\n\n\n\n\n\nТо есть, самые часто встречающиеся значения — это пик распределения. Там и должна быть мода. Визуально это выглядит достаточно справедливо.\nМатематики так и решили:\n\nОпределение 7.3 Мода [непрерывной переменной] — это значение переменной, при котором её функция плотности вероятности достигает локального3 максимума.\n\n\\[\n\\text{mode}(X) = \\arg \\max(\\text{PDF}(X)) = \\arg \\max_{x \\in S}f(x),\n\\tag{7.2}\\]\nгдe \\(X\\) — непрерывная случайная величина, \\(x\\) — значение этой случайной величины, \\(S\\) — имеющаяся выборка значений переменной.\n\n\n\n\n\n\n\n\nРисунок 7.3: Положение моды на функции плотности [стандартного] нормального распределения\n\n\n\n\n\nХотя моду для непрерывной переменной вычислить можно, обычно этого не делают, так как достаточно других мер центральной тенденции для описания распределения.\n\n\n\n7.3.2 Унимодальные и полимодальные распределения\nНормальное распределение, как и ряд других — биномиальное, отрицательное биномиальное, пуассоновское — относятся к унимодальным. Такие распределения имеют только одну моду (см. Рисунок 7.4, Рисунок 7.5, Рисунок 7.6).\n\n\n\n\n\n\n\n\nРисунок 7.4: Нормальное распределение (μ = 2$, σ = 0.5). Пунктирной линией обозначено положение моды.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 7.5: Биномиальное распределение (n = 50, p = 0.3). Пунктирной линией обозначено положение моды.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 7.6: Распределение Пуассона (λ = 5.5). Пунктирной линией обозначено положение моды.\n\n\n\n\n\nЭто теоретические распределения. С эмпирическими распределениями дело обстоит так же, хотя они обычно менее гладенькие и красивые (см. Рисунок 7.7 и Рисунок 7.8).\n\n\n\n\n\n\n\n\nРисунок 7.7: Эмпирическое распределение, сгенерированное из нормального распределения (μ = 8, σ = 4, n = 100). set.seed(314). Пунктирной линией обозначено положение моды.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 7.8: Эмпирическое распределение, сгенерированное из логнормального распределения (μ = 1.1, σ = 1.39, n = 30). set.seed(314). Пунктирной линией обозначено положение моды.\n\n\n\n\n\nОднако на практике возможны и другие ситуации. Например, такие (Рисунок 7.9, Рисунок 7.10):\n\n\n\n\n\n\n\n\nРисунок 7.9: Бимодальное распределение. Сгенерировано из двух нормальных распределений (μ1 = 1.5, σ1 = 0.4, n1 = 80; μ2 = 4, σ2 = 0.5, n2 = 40). set.seed(65). Пунктирными линиями обозначены положения мод.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 7.10: Полимодальное распределение. Сгенерировано из двух нормальных распределений (μ1 = 1.5, σ1 = 0.3, n1 = 80; μ2 = 3.4, σ2 = 0.5, n2 = 40) и бета-распределения (α = 2, β = 4, n = 50). set.seed(65). Пунктирными линиями обозначены положения мод.\n\n\n\n\n\nВ первом случае (Рисунок 7.9) мы видим два локальных максимума функции плотности вероятности — такое распределение называется бимодальным. Во втором случае (Рисунок 7.10) функция плотности вероятности имеет три локальных максимума — такое распределение называется полимодальным. Бимональное распределение является частным случаем полимодального распределения.\nВ прицнипе, пиков может быть и больше, однако при работе с реальными данными чаще всего мы сталкиваемся с бимодальными распределениями.\nЧто это значит и что с этим делать?\nБимодальное распределение сигнализирует нам о гетерогенности выборки. Если мы видим два выделяющихся пика, стоит подумать о том, что наша выборка неоднородна и в ней выделяются две подвыборки. Посмотрим на структуру выборки из примера выше (Рисунок 7.11):\n\n\n\n\n\n\n\n\nРисунок 7.11: Структура бимодального распределения из Рисунок 7.9. Для удобства сопоставления графиков плотностей вероятности по оси ординат отложены частоты.\n\n\n\n\n\nДействительно, наше распределение состоит из двух других распределений, у каждого из которого есть своя мода — поэтому итоговое распределение получается бимодальным. Конечно, сейчас нам это очень удобно показать, потому что мы знаем, как это распределение генерировалось. Когда же у нас есть реальные данные и мы там наблюдаем такого «верблюда», бывает достаточно сложно сказать, что «пошло не так».\nСамо по себе распределение не даст нам ответ на вопрос, почему оно бимодальное — чтобы выяснить причины такого поведения переменной нам потребуются другие данные. Обычно у вас в данных есть «соцдем» — пол, возраст, сфера и место работы, уровень обрвазования и др. Попробуйте построить распределение с разбиением исследуемой бимодальной переменной по переменным «соцдема». Это, к сожалению, не является рецептом успеха, поскольку причина гетерогенности выборки может и не содержаться в ваших данных, но такое изучение данных станет хорошим показателем того, что вы не просто «забили» на странное распределение своей переменной, а поисследователи возможные его причины.\nЕсли вам удалось найти причины гетерогенности выборки — допустим, у вас выделяются подвыборки «бакалавры» и «магистры» — стоит подумать о том, как обойтись с этой переменной в планируемом анализе, так как игнорировать её, по-видимому, нельзя, поскольку она влияет на вариатиность данных.\n\n\n\n\n\n\nСоцдем лишним не бывает\n\n\n\nНа этапе планирования исследования подумайте о том, чем могут отличаться ваши респонденты или испытуемые между собой, помимо индивидуальных различий.\n\nЕсли в эксперименте используете задачу мысленного вращения (mental rotation), вполне возможно, испытуемые, работающие в сфере 3D-моделирования или дизайна интерьеров, могут сформировать подвыборку.\nВ случае HR-исследования, где фиксируется доход респондента, необходимо записать город, в котором он проживает и/или работает.\nПри изучении удовлетворенности городским пространством важными пунктами станут беременность, наличие/отсутствие детей, наличие/отсутствие автомобиля и др.\n\nИ так далее. Примеров для каждого случая можно подобрать много.\nСтоит ли, скажем, в первом случае сразу исключить из выборки 3D-моделлеров? Зависит. От количества времени и денег на проведение исследования. Однако как минимум эту информацию надо зафиксировать в данных. А решить, исключать ли этих респондентов из выборки или нет, можно и позже. Главно об этом написать в отчете/статье, когда будете описывать предобработку данных.\n\n\n\n\n7.3.3 Медиана\nДля номинальной шкалы мода — это единственно возможная мера центральной тенденции, потому что на этой шкале отсутствует порядок элементов. На других шкалах наблюдения уже можно сортировать по возрастнию или убыванию, поскольку начиная с ранговой (порядковой) шкалы на всех них определена операция сравнения на «больше-меньше».\nВозьмем тот же ряд наблюдений, что и в предыдущем разделе:\n\\[\n\\begin{bmatrix}\n1 & 3 & 4 & 6 & 3 & 2 & 3 & 3 & 2 & 4 & 1\n\\end{bmatrix}\n\\]\nОтсортируем наблюдения по возрастанию:\n\\[\n\\begin{bmatrix}\n1  & 1 & 2 & 2 & 3 & 3 & 3 & 3 & 4 & 4 & 6\n\\end{bmatrix}\n\\]\nНаша задача — определить центральную тенденцию. Давайте посмотрим, что оказалось в середине отсортированного ряда:\n\\[\n\\begin{bmatrix}\n1 & 1 & 2 & 2 & 3 & \\mathbf{3} & 3 & 3 & 4 & 4 & 6\n\\end{bmatrix}\n\\]\nЭто медиана. В данном случае она равна \\(3\\).\n\nОпределение 7.4 Медиана (median) — это значение, которое располагается на середине отсортированного ряда значений переменной.\n\nМедиана делит все наблюдения переменной ровно пополам и половина наблюдений оказывается по одну сторону от медианы, а половина — по другую.\nЕсли число наблюдений нечётное, то всё ясно — в середине отсортированного ряда будет какое-то значение. А если число наблюдений чётное? Тогда мы попадаем между значениями.\nВозьмем для примера такой вектор наблюдений:\n\\[\n\\begin{bmatrix}\n14 & 10 & 9 & 16 & 30 & 3 & 25 & 8 & 18 & 7\n\\end{bmatrix}\n\\]\nОтсортируем:\n\\[\n\\begin{bmatrix}\n3 & 7 & 8 & 9 & 10 & 14 & 16 & 18 & 25 & 30\n\\end{bmatrix}\n\\]\nНайдем середину:\n\\[\n\\begin{bmatrix}\n3 & 7 & 8 & 9 & 10 & | & 14 & 16 & 18 & 25 & 30\n\\end{bmatrix}\n\\]\nВ таком случае в качестве медианы берется среднее между двумя срединными значениями:\n\\[\n\\text{median} = \\frac{10 + 14}{2} = 12\n\\]\nИтого, формализовать вычисление медианы можно следующим образом:\n\\[\n\\text{median}(X) = X(a) =\n\\cases{\nX\\left(\\frac{n+1}{2}\\right), & if  2 | n \\\\\n\\dfrac{X(\\frac{n}{2}) + X(\\frac{n}{2} + 1)}{2}, & otherwise\n}\n\\tag{7.3}\\]\nгде \\(X\\) — ряд наблюдений случайной величины, \\(n\\) — число наблюдений, \\(X(a)\\) — наблюдение с индексом \\(a\\) в отсортированном векторе \\(X\\).\nЕсли мы будем смотреть на медиану с позиции описания распределения, то она будет той самой линией, которая разделит площадь под графиком функции плотности вероятности пополам:\n\n\n\n\n\n\n\n\nРисунок 7.12: Медиана нормального распределения\n\n\n\n\n\nПри этом форма распределения не имеет значения — площадь под графиком всегда будет делиться пополам:\n\n\n\n\n\n\n\n\nРисунок 7.13: Медиана распределения с отрицательной асимметрией.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 7.14: Медиана распределения с положительной асимметрией.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 7.15: Медиана бимодального распределения.\n\n\n\n\n\n\n\n\n7.3.4 Среднее\nЕсли наша переменная измерена в самых мощных шкалах — интервальной или абсолютной — то нам доступна ещё одна мера центральной тенденции.\n\n7.3.4.1 Арифметическое среднее\nС этим существом все знакомы еще со школы. Арифметическое среднее (arithmetic mean, mean, average) считается так:\n\\[\n\\mathbb{M}_X = \\bar X = \\dfrac{\\sum_{i=1}^{n}x_i}{n},\n\\]\nгде \\(\\bar X\\) — среднее арифметическое, \\(x_i\\) — наблюдение в векторе \\(X\\), \\(n\\) — количество наблюдений.\nНу, то есть всё сложить и поделить на количество того, чего сложили. Изи.\n\n7.3.4.1.1 Свойства среднего арифметического\n\nЕсли к каждому значению распределения прибавить некоторое число (константу), то среднее увеличится на это же константу.\n\n\\[\n\\mathbb{M}_{X+c} = \\mathbb{M}_X + c\n\\]\nВот почему:\n\\[\n\\mathbb{M}_{X+c} = \\frac{\\sum_{i=1}^n (x_i + c)}{n} = \\frac{\\sum_{i=1}^n x_i + nc}{n} = \\frac{\\sum_{i=1}^n x_i}{n} + c = \\mathbb{M}_X + c\n\\]\nИначе говоря, распределение просто сдвинется. Например, если к каждому значению синего распределения прибавить \\(2\\), получится красное:\n\n\n\n\n\n\n\n\n\n\nЕсли каждое значение распределение умножить на некоторое число (константу), то среднее увеличится во столько же раз.\n\n\\[\n\\mathbb{M}_{X \\times c} = \\mathbb{M}_X \\times c\n\\]\nВот почему:\n\\[\n\\mathbb{M}_{X \\times c} = \\frac{\\sum_{i=1}^n (x_i \\times c)}{n} = \\frac{c \\times \\sum_{i=1}^n x_i}{n} = \\frac{\\sum_{i=1}^n x_i}{n} \\times c = \\mathbb{M}_X \\times c\n\\]\nНапример, здесь каждое значение синего распределения умножили на \\(3\\) и получили красное:\n\n\n\n\n\n\n\n\n\nТут, правда, явно что-то ещё произошло, но мы пока этого не знаем. Однако, отметит этот факт.\n\nСумма отклонений от среднего значения равна нулю.\n\n\\[\n\\sum_{i=1}^n(x_i - \\bar X) = 0\n\\]\nЭлегантное доказательство:\n\\[\n\\begin{split}\n\\sum_{i=1}^n(x_i - \\bar X) & = \\sum_{i=1}^n x_i - \\sum_{i=1}^n \\bar X = \\sum_{i=1}^n x_i - n \\bar X = \\\\\n& = \\sum_{i=1}^n x_i - n \\times \\frac{1}{n} \\sum_{i=1}^n x_i = \\sum_{i=1}^n x_i - \\sum_{i=1}^n x_i = 0\n\\end{split}\n\\]\nНо можно это осмыслить и более просто графически.\nОтклонение — это разность между средним и конкретным значением переменной. И, действительно, так как среднее находится в центре распределения, то часть значений лежит справа, а часть слева. Значит, будут как положительные, так и отрицательные отклонения — и их сумма в итоге будет равна нулю.\n\n\n\n\n\n\n\n\n\nСреднее арифметическое не одиноко — есть и другие. Встретяться они вам примерно нигде — то есть о-о-о-очень редко и, скорее всего, в каком-то изощрённом виде. Но упомянуть их, пожалуй, стоит.\n\n\n\n7.3.4.2 Геометрическое среднее\nРедко встречается в научных работах, но заради общего представления пусть будет. Поскольку оно считается через умножение, то может быть рассчитано только на абсолютной шкале.\n\\[\nG_{X} = \\sqrt[n]{\\prod_{i=1}^n x_i} = \\Big(\\prod_{i=1}^n x_i\\Big)^{\\tfrac{1}{n}}\n\\]\n\n\n7.3.4.3 Квадратичное среднее\n\nА вот это уже более полезная история. Мы с ним столкнёмся далее, правда под разными масками.\n\nКвадратичное среднее (quadratic mean, root mean square, RMS) — это квадратный корень из среднего квадрата наблюдений. Ничего не понятно, поэтому по порядку.\n\nесть наблюдение \\(x_i\\)\nзначит есть и его квадрат \\(x_i^2\\)\nмы умеем считать обычно среднее арифметическое, но ведь \\(x_i^2\\) — это тоже наблюдение, просто в квадрате, так?\nзначит можем посчитать среднее арифметическое квадратов наблюдений — средний квадрат\n\n\\[\n\\frac{\\sum_{i=1}^n x_i^2}{n}\n\\]\n\nнорм, а теперь извлечём из этого дела корень — получим то, что там надо\n\n\\[\nS_X = \\sqrt{\\frac{\\sum_{i=1}^n x_i^2}{n}}\n\\]\nPer se4 мы его вряд ли ещё когда-то увидим, но пару раз оно внезапно всплывет.\n\n\n7.3.4.4 Гармоническое среднее\n\nСуперэкзотичный покемон.\n\n\\[\nH_X = \\frac{n \\prod_{i=1}^n x_i}{\\sum_{i=1}^n (\\tfrac{1}{x} \\prod_{j=1}^n x_j)} = \\frac{n}{\\sum_{i=1}^n \\tfrac{1}{x_i}}\n\\]\n\n\n7.3.4.5 Взвешенное среднее\nЧасто бывает такая ситуация, что нас нужно посчитать среднее по каким-либо имеющимся параметрам, но одни параметры для нас важнее, чем другие. Например, мы хотим вычислить суммарный балл обучающегося за курс на основе ряда работ, выполненных в течение курса, однако мы понимаем, что тест из десяти вопросов с множественном выбором явно менее показателен, чем, например, аналитическое эссе или экзаменационная оценка. Что делать? Взвесить параметры!\nЧто значит взвесить? Умножить на некоторое число. На самом деле, любое. Пусть мы посчитали, что написать эссе в три абстрактных раза тяжелее, чем написать тест, а сдать экзамен в два раза тяжелее, чем написать эссе. Тогда мы можем присвоить баллу за тест вес \\(1\\), баллу за аналитическое эссе вес \\(3\\), а экзамену — вес \\(6\\). Тогда итоговая оценка за курс будет рассчитываться следующим образом:\n\\[\n\\text{final score } = 1 \\cdot \\text{test} + 3 \\cdot \\text{essay} + 6 \\cdot \\text{exam}\n\\]\nСуперкласс. Однако! Весьма вероятно, что в учебном заведении принята единая система оценки для всех видов работ (ну, скажем, некая абстрактная десятибалльная система в сферическом вакууме). Получается, если и за тест, и за эссе, и за экзамен у студента по 10 баллов, то суммарный балл 100, что, кажется, больше, чем 10. Чтобы вернуться к изначальным границам баллов, нужно моделить суммарный балл на сумму весов параметров:\n\\[\n\\text{final score } = \\frac{1 \\cdot \\text{test} + 3 \\cdot \\text{essay} + 6 \\cdot \\text{exam}}{1 + 3 + 6}\n\\]\nКайф! Собственно, это и есть взвешенное среднее. Коэффициенты, на которые мы умножаем значение парамернов, называются весами параметров. И в общем виде формула принимает следующий вид.\n\\[\n\\bar X = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i} = \\sum_{i=1}^n w_i' x_i,\n\\]\nгде \\(x_i\\) — значения конкретных параметров, \\(w_i\\) — веса конкретных параметров, \\(w_i'\\) — нормированные веса параметров.\nВторая часть формулы показывается нам, что можно облегчить себе вычислительную жизнь, если заранее нормировать веса, то есть разделить каждый коэффициент на сумму коэффициентов:\n\\[\nw_i' = \\frac{w_i}{\\sum_{i=1}^n w_i}\n\\]\nТогда сумма коэффициентов будет равна единице. Так чаще всего и поступают, так как тогда коэффициент будет представлять долю, которую весит данный параметр в суммарной оценке. Удобно, практично, красиво.\nВзвещенное среднее часто применяется именно во всякого рода ассессментах, и не только образовательных. Например, вы HR-аналитик и оцениваете персонал. Вы аналитически вычисляете веса коэффициентов (допустим, с помощью линейной регрессии), а далее на их основе высчитаете интегральный балл, по которому будете оценивать сотрудников. Это как один из индустриальных примеров.\nТакже оно применяется, когда в наших данных есть какая-то группировка (например, когорты), при этом группы неравномерны.\n\n\n\n7.3.5 Среднее vs медиана\nПомимо того, что среднее и медиана информативны сами по себе, полезно смотреть на их взаимное расположение.\n\nСравнивать будем моду, медиану и среднее [арифметическое].\n\nИтак, все три статистики — мода, медиана и среднее — описывают центральную тенденцию — некоторое значение изучаемой нами переменной, вокруг которого собираются другие значения. Но если их три и все они используются, значит между ними должны быть какие-то различия. Посмотрим, какие.\nВо-первых, моду невозможно посчитать для непрерывной переменной.\nТак как вероятность того, что непрерывная случайная величина принимает своё конкретное значение, равна нулю, каждое наблюдение в нашей выборке будет уникально — встретится ровно один раз. Вспомните [посмотрите] пример из предыдущей главы, где мы набирали числа из отрезка. Получается, что мода теряет свой смысл.\nВо-вторых, медиану нельзя посчитать на номинальной шкале.\nНа номинальной шкале нет отношения порядка между элементами. Помните, на ней нельзя сравнивать на больше-меньше. Поэтому невозможно отсортировать наблюдения, а значит, и найти медиану.\nВ-третьих, среднее тоже нельзя посчитать на номинальной шкале.\nВообще, конечно, да — нельзя, потому что на номинальной шкале не определена операция сложения, входящая в вычисление среднего. Однако если на номинальной шкале есть только две категории, которые закодированы 0 и 1, то посчитать среднее можно. Но что оно будет значить?\nИсходный математический смысл среднего явно утерян. Посмотрим на это по-другому: посчитать сумму единиц это всё равно, что посчитать количество единиц. То есть, если мы сложим все нули и единицы, то получим количество единиц среди всех наших наблюдений. А разделив количество единиц на количество наблюдений, мы получим долю единиц — то есть долю наблюдений с лейблом 1.\nВ-четвертых, для дискретной переменной значение среднего арифметического будет не особо осмысленно. Ну, скажем, странно сказать, что в аудитории в среднем стоят 15.86 столов или в российских семьях в среднем 1.5 ребенка. Конечно, в ряде случаев можно это как-то более-менее содержательно интерпретировать, но это требует усилий, а мы ленивые, поэтому лучше использовать медиану.\nИтого, делаем следующие выводы:\n\nдля номинальной шкалы пригодна только мода\nдля дискретных переменных подходят мода и медиана\n\nмода иногда лучше, так как точно всегда будет целым числом\n\nдля непрерывных переменных подходят медиана и среднее\n\nТеперь нам надо разобраться, как будут себя вести меры центральной тенденции в зависимости от формы распределения.\nНа симметричном распределении мода, медиана и среднее совпадают [или, по крайней мере, находятся очень близко друг к другу]. Здесь и далее: красная линия — среднее, синяя — медиана, зелёная — мода.\n\n\n\n\n\n\n\n\n\nНа асимметричном распределении мода [практически] в пике. Практически, потому что функция плотности вероятности [черная линия на графике] на всегда точно аппроксимирует (в данном случае то же, что и сглаживает) эмпирическое распределение. На картинке ниже мы видим, что на гистограмме мода — самый высокий столбик, что и показывает нам зелёная линия, которой обозначена мода. Однако при сглаживании гистограммы пик немного съехал, и мода оказалась не совсем в вершине графика функции плотности вероятности.\nВообще-то это нормально, потому что мода для непрерывной величины, которую мы и визуализируем с помощью графика плотности, либо не может быть посчитана вовсе, либо — если так получилось, и у нас все же есть повторяющиеся значения — не слишком хорошая мера центральной тенденции. В целом, и на симметричном распределении мода тоже может находиться немного в стороне от пика.\nНа асимметричном распределении медиана и среднее смещены в сторону хвоста. Среднее смещено сильнее медианы. Это связано с тем, что медиана зависит только от количества наблюдений, а среднее ещё и от самих значений. На картинке ниже пример для распределения с правосторонней асимметрии (потому что хвост справа) — среднее (красная линия) правее медианы (синяя линия).\n\n\n\n\n\n\n\n\n\nА это пример для распределения с левосторонней асимметрией (так как хвост слева) — среднее (красная линия) левее медианы (синяя линия).\n\n\n\n\n\n\n\n\n\nДля того, чтобы лучше разобраться с тем, как большие и малые значения влияют на моду и медиану посмотрим такой пример. Пусть у нас есть оценки за выпускную квалификационную работу. Например, такие:\n\n\n[1] 6 7 7 8 8\n\n\nПосчитаем медиану и среднее:\n\n\n[1] 7\n\n\n[1] 7.2\n\n\nСреднее \\(7.2\\) округлиться до \\(7\\), то есть можно считать, что среднее и медиана совпали. Ну, ок.\nНо в комиссии сидят два требовательных доктора наук, которые поставили оценки, сильно отличающиеся от остальных:\n\n\n[1] 6 7 7 8 8 3 4\n\n\nПосчитаем медиану и среднее теперь:\n\n\n[1] 7\n\n\n[1] 6.142857\n\n\nМедиана осталась на месте — всё ещё \\(7\\). А вот среднее \\(6.1\\) округлится до \\(6\\). Казалось бы, это немного, но в смысле оценок — это прилично, и может сильно повлиять на GPA.\nИтого, среднее более чувствительно к нетипичным значениям (очень большим или очень малым).\nЕсть ещё один интересный вариант распределений — бимодальные. Значит ли, что у этого распределения две моды? Не всегда. Посмотрим пример ниже:\n\n\n\n\n\n\n\n\n\nМы видим, что на графике есть два пика, однако строго математически мода одна (зеленая линия) — и она в более высоком пике. Это логично, ибо там самые часто встречающиеся значения.\nСо средним и медианой происходит примерно то же, что и в случае асимметричного распределения. Второй пик смещает к себе обе меры центральной тенденции, причем среднее вновь сильнее, чем медиану.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "l7.html#andan-descriptives-variability",
    "href": "l7.html#andan-descriptives-variability",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "7.4 Меры разброса",
    "text": "7.4 Меры разброса\nИтак, мы разобрались с мерами центральной тенденции. Однако для описания распределения их оказвается недостаточно. Почему?\n\n7.4.1 Зачем нужны меры разброса\nПосмотрим на несколько распределений:\n\n\n\n\n\n\n\n\n\nМетодом пристального взгляда можно установить, что у всех распределений одинаковые средние:\n\n\n\n\n\n\n\n\n\nОднако мы видим, что значения по-разному группируются вокруг среднего. Как они группируются — плотно, как на третьем рисунке, или не особо, как на втором — можно описать с помощью мер разброса, или мер вариативности.\n\n\n7.4.2 Минимум, максимум, размах\nНачнем с самого простого. Как наиболее просто описать вариативность? Мы работаем с выборкой, а в выборке, как известно, ограниченное число наблюдений. А если оно ограниченое, значит среди них точно есть наибольшее — максимальное — и наименьшее — минимальное.\nДопустим, мы открыли ведомость по «Анатомии и физиологии ЦНС» некоторой академической группы и пронаблюли следующее:\n\n\n [1]  7  4  6  9 10  5  6  9  6  6  3  6  8  8  5 10  7  5  7  3  9  4  8  3  8\n[26]  4  6  8  7  5\n\n\nМы можем посчитать минимальное и максимальне значение по этому ряду наблюдений:\n\n\n[1] 3\n\n\n[1] 10\n\n\nПолучается, что оценки варьируются от \\(3\\) до \\(10\\). Ну, приемлемо. Разница между максимальным и минимальным значением называется размах (range):\n\\[\n\\mathrm{range}(X) = \\max(X) - \\min(X)\n\\]\nИ вот мы преисполнившиеся идёт описывать вариативность переменной с помощью размаха, но обнаруживаем в другой ведомости этой же группы (по «Введению в психологию») вот что:\n\n\n [1]  6  8  4  6  7  5  7 10  4  6  7  8  7  6  8 10  8  7  7  6  8  7  6  8  6\n[26]  3  8  6  6  4\n\n\nРазмах вроде как такой же:\n\n\n[1]  3 10\n\n\nЗначит ли это, что вариативность одинаковая?\nНарисуем.\n\n\n\n\n\n\n\n\n\nКажется, что вариативность различна. Распределение оценок по «Анатомии и физиологии ЦНС» более равномерное, в то время как оценки по «Введению в психологию» активнее группируются где-то в середине.\nШтош, размах хоть и дает нам некоторую информацию о вариативности, нам этого маловато. Будем искать другие меры разброса.\n\n\n7.4.3 Дисперсия\nХотя описание разброса переменных с помощью квантилей (в частности, квартилей) может дать нам много полезной информации, все же у них есть существенный недостаток: они никак не взаимодействуют с самими значениями нашей переменной.\nДействительно, мы делим нашу сортированную выборку на равные части, и смотрим, что в эти части попало. Но хотелось бы как-то учесть ещё и сами значения переменной в некотрой числовой мере разброса.\nНу, хорошо. Поступим следующим образом. Мы все ещё хотим узнать, как наши значения группируются вокруг среднего. В предыдущей главе мы уже видели, что наши наблюдения отклоняются от среднего значения — значит мы можем посчитать отклонение для каждого наблюдения:\n\\[\nd_i = \\bar X - x_i\n\\]\nОкей. Если мы сложим все отклоненияи и поделим на их количество (которое равно количеству наблюдений), то мы получим среднее отклонение, да?\n\\[\n\\bar d = \\frac{1}{n} \\sum_{i=1}^n \\bar X - x_i\n\\]\nДа. Однако есть одна проблема. Ранее мы выяснили, что сумма отклонений от среднего равна нулю, а значит и среднее отклонение также будет равно нулю.\nХорошо. Но отрицательные значения ведь можно победить! Есть два пути:\n\nМодуль. Преимущество первого в том, что размерность величины разброса остается той же, что и у измеряемой переменной.\nКвадрат. Преимущество второго в том, что сильные отклонения будут оказывать более сильное влияние на окончательное значение статистики, в то время как для первого малые и большие отклонения равноценны.\n\nВторой путь на практике оказывается полезнее, так как мы хотим, чтобы сильно отличающиеся наблюдения вносили вклад в меру разброса.\nВозведя отклонения в квадрат, получим формулу дисперсии (вариации, variation):\n\\[\n\\mathbb{D}_X = \\text{var}(X) = \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (\\bar x - x_i)^2\n\\]\nГениально.\nНе совсем. Формула, которую мы получили, пригодна для расчета дисперсии генеральной совокупности — на выборке же она будет давать смещенную оценку. Это мы выводили математически и проверяли на симуляциях в предыдущем блоке.\nЧтобы получить точную (несмещенную) оценку дисперсии по выборке, нам нужно исправить знаменатель дроби — вместо \\(n\\) использовать \\(n-1\\):\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^n (\\bar x - x_i)^2\n\\]\nНо почему?\n\n7.4.3.1 Степени свободы\nВо всём виновата выборка.\nВзглянем на формулу дисперсии: в неё входит среднее арифметическое. То есть для того, чтобы рассчитать дисперсию на выборке, сначала нам необходимо на этой же выборке рассчитать среднее. Тем самым, мы как бы «фиксируем» нашу выборку этим средним значением — у значений нашего распределения становится меньше свободы для варьирования. Теперь свободно варьироваться могут \\(n-1\\) наблюдение, так как последнее всегда будет возможно высчитать, исходя из среднего значения. По этой причине нам необходимо корректировать исходную формулу расчета дисперсии.\n\n\n\n7.4.4 Стандартное отклонение\nИ вот мы получили невероятное! У нас есть формула расчета меры разброса, которая позволяет учесть сами значения переменной! Ну не чудо ли!\nЧудо, конечно, однако есть некоторая проблема. Мы возводили отклонения в квадрат. Представим, что мы хотим посчитатить дисперсию роста студентов психфака. Пусть мы измеряли рост в метрах. Отклонения тоже будут в метрах (потому что среднее — это тоже метры, а если из метров вычитать метры, то мы получим метры). А при возведении метров в квадрат получаются метры в квадрате. Очевидно, что если мы модели квадратные метры на некоторое число (\\(n\\)), они все еще останутся метрами в квадрате.\nО, нет! А счастье было так близко, так возможно! Получается, мы не можем интерпретировать эту меру разброса? Не сможем даже нарисовать?\nДа, но это не очень большая беда. Для того, чтобы вернуться обратно к единицам измерения нашей переменной, нам всего лишь нужно извлечь корень из дисперсии:\n\\[\n\\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\bar X - x_i)^2}\n\\]\nМы получили величину, называемую стандартным отклонением (standard deviation). Чем она хороша? Тем, что её размерность совпадает с размерностью нашей переменной. Стандартное отклонение уже может быть достаточно интерпретабельно и хорошо визуализируемо.\nКстати, формула выше, которая что-то очень напоминает, — это стандартное отклонение генеральной совокупности, потому что под корнем стоит дисперсия генеральной совокупности.\nЧтобы посчитать стандартное отклонение по выборке, нам надо извлечь корень из выборочной дисперсии:\n\\[\ns = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (\\bar X - x_i)^2}\n\\]\n\n\n7.4.5 Свойства дисперсии и стандартного отклонения\n\nЕсли к каждому значению распределения прибавить некоторое число (константу), то дисперсия не изменится.\n\n\\[\n\\mathbb{D}_{X+c} = \\mathbb{D}_{X}\n\\]\nВот почему:\n\\[\n\\begin{split}\n\\mathbb{D}_{X+c} &= \\frac{\\sum_{i=1}^n \\big((\\bar X + c) - (x_i + c)\\big)^2}{n-1} = \\\\\n&= \\frac{\\sum_{i=1}^n \\big(\\bar X + c - x_i - c\\big)^2}{n-1} \\\\\n& = \\frac{\\sum_{i=1}^n \\big(\\bar X - x_i\\big)^2}{n-1} = \\mathbb{D}_X\n\\end{split}\n\\]\n\nЕсли каждое значение распределение умножить на некоторое число (константу), то дисперсия увеличится в \\(c^2\\) раз.\n\n\\[\n\\mathbb{D}_{X \\cdot c} = c^2\\mathbb{D}_{X}\n\\]\nВот почему:\n\\[\n\\mathbb{D}_{X \\cdot c} = \\frac{\\sum_{i=1}^n (c\\bar X - cx_i)^2}{n-1} = \\frac{\\sum_{i=1}^n c^2(\\bar X - x_i)^2}{n-1} = \\frac{c^2 \\sum_{i=1}^n (\\bar x - x_i)^2}{n-1} = c^2\\mathbb{D}_X\n\\]\n\nЕсли к каждому значению распределения прибавить некоторое число (константу), то стандартное отклонение не изменится.\n\n\\[\ns_{X+c} = s_X\n\\]\nЭто следует из свойства дисперсии:\n\\[\ns_{X+c} = \\sqrt{s^2_{x+c}} = \\sqrt{s^2_x} = s_x\n\\]\nКак мы уже видели, распределение просто сдвигается на константу. Например, если к каждому значению синего распределения прибавить \\(2\\), получится красное — разброс у обоих распределений одинаковый:\n\n\n\n\n\n\n\n\n\n\nЕсли каждое значение распределение умножить на некоторое число (константу), то стандартное отклонение увеличится во столько же раз.\n\n\\[\ns_{X \\cdot c} = c\\cdot s_X\n\\]\nЭто также следует из свойства дисперсии:\n\\[\ns_{X \\cdot c} = \\sqrt{s^2_{X \\cdot c}} = \\sqrt{s_X \\cdot c^2} = c \\cdot s_x\n\\]\nНапример, здесь каждое значение синего распределения умножили на \\(3\\) и получили красное — разброс также увеличился в три раза, поэтому распределение более плоское:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "l7.html#why_we_need_variation",
    "href": "l7.html#why_we_need_variation",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "7.5 Зачем нужны меры разброса",
    "text": "7.5 Зачем нужны меры разброса\nПосмотрим на несколько распределений:\n\n\n\n\n\n\n\n\n\nМетодом пристального взгляда можно установить, что у всех распределений одинаковые средние:\n\n\n\n\n\n\n\n\n\nОднако мы видим, что значения по-разному группируются вокруг среднего. Как они группируются — плотно, как на третьем рисунке, или не особо, как на втором — можно описать с помощью мер разброса, или мер вариативности.\n\n7.5.1 Основные характеристики статистических данных\nВообще если посмотреть на это более свысока, то необходимость описания разброса определяется тем, что статистические данные обладают двумя ключевыми особенностями — неопределенностью и вариативностью.\n\nНеопределённость нам говорит о том, что мы не знаем, что именно мы получим в результате наших измерений для конкретной выборки. В том числе потому, что мы работаем на просторах случайных величин.\nВариативность означает, что наши данные будут различатся ещё и от респондента к респонденту. И между выборками тоже. Здесь и ошибка измерения, и различные смешения и ещё куча всего.\n\nБолее того, вариативность настолько важна, что она входит в расчёт любого статистического критерия. Именно вариативность — а не центральная тенденция — позволяет нам сделать вывод о том, что наши выборки различаются (или нет).\n\n\n7.5.2 Минимум, максимум, размах\n\n\n\nНачнем с самого простого. Как наиболее просто описать вариативность? Мы работаем с выборкой, а в выборке, как известно, ограниченное число наблюдений. А если оно ограниченое, значит среди них точно есть наибольшее — максимальное — и наименьшее — минимальное.\nДопустим, мы открыли ведомость по «Анатомии и физиологии ЦНС» некоторой академической группы и пронаблюли следующее:\n\n\n [1]  7  4  6  9 10  5  6  9  6  6  3  6  8  8  5 10  7  5  7  3  9  4  8  3  8\n[26]  4  6  8  7  5\n\n\nМы можем посчитать минимальное и максимальне значение по этому ряду наблюдений:\n\n\n[1] 3\n\n\n[1] 10\n\n\nПолучается, что оценки варьируются от \\(3\\) до \\(10\\). Ну, приемлемо. Разница между максимальным и минимальным значением называется размах (range):\n\\[\n\\mathrm{range}(X) = \\max(X) - \\min(X)\n\\]\nПравда вот функция range в R вернёт не само значение размаха, а минимальное и максимальное значение. Ну, ладно.\n\n\n[1]  3 10\n\n\nИ вот мы преисполнившиеся идёт описывать вариативность переменной с помощью размаха, но обнаруживаем в другой ведомости этой же группы (по «Введению в психологию») вот что:\n\n\n [1]  6  8  4  6  7  5  7 10  4  6  7  8  7  6  8 10  8  7  7  6  8  7  6  8  6\n[26]  3  8  6  6  4\n\n\nРазмах вроде как такой же:\n\n\n[1]  3 10\n\n\nЗначит ли это, что вариативность одинаковая?\nНарисуем.\n\n\n\n\n\n\n\n\n\nКажется, что вариативность различна. Распределение оценок по «Анатомии и физиологии ЦНС» более равномерное, в то время как оценки по «Введению в психологию» активнее группируются где-то в середине.\nШтош, размах хоть и дает нам некоторую информацию о вариативности, нам этого маловато. Будем искать другие меры разброса.\n\n\n7.5.3 Среднее абсолютное отклонение\n\n\n\n\n7.5.3.1 Среднее абсолютное отклонение от среднего\n\n\n\n\n\n7.5.3.2 Среднее абсолютное отклонение от медианы\n\n\n\n\n\n7.5.3.3 Медианное абсолютное отклонение\n\n\n\n\n\n\n7.5.4 Дисперсия\n\n\n\nХотя описание разброса переменных с помощью квантилей (в частности, квартилей) может дать нам много полезной информации, все же у них есть существенный недостаток: они никак не взаимодействуют с самими значениями нашей переменной.\nДействительно, мы делим нашу сортированную выборку на равные части, и смотрим, что в эти части попало. Но хотелось бы как-то учесть ещё и сами значения переменной в некотрой числовой мере разброса.\nНу, хорошо. Поступим следующим образом. Мы все ещё хотим узнать, как наши значения группируются вокруг среднего. В предыдущей главе мы уже видели, что наши наблюдения отклоняются от среднего значения — значит мы можем посчитать отклонение для каждого наблюдения:\n\\[\n\\bar x - x_i\n\\]\nОкей. Если мы сложим все отклоненияи и поделим на их количество (которое равно количеству наблюдений), то мы получим среднее отклонение, да?\n\\[\n\\frac{1}{n} \\sum_{i=1}^n \\bar x - x_i\n\\]\nДа. Однако есть одна проблема. В прошлой главе мы выяснили, что сумма отклонений от среднего равна нулю, а значит и среднее отклонение также будет равно нулю.\nХорошо. Но отрицательные значения ведь можно победить! Есть два пути:\n\nМодуль. Преимущество первого в том, что размерность величины разброса остается той же, что и у измеряемой переменной.\nКвадрат. Преимущество второго в том, что сильные отклонения будут оказывать более сильное влияние на окончательное значение статистики, в то время как для первого малые и большие отклонения равноценны.\n\nВторой путь на практике оказывается полезнее, так как мы хотим, чтобы сильно отличающиеся наблюдения вносили вклад в меру разброса.\nВозведя отклонения в квадрат, получим формулу дисперсии (вариации, variation):\n\\[\nD(X) = \\mathrm{var}(X) = \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (\\bar x - x_i)^2\n\\]\nГениально.\nНе совсем. Формула, которую мы получили, пригодна для расчета дисперсии генеральной совокупности — на выборке же она будет давать неточную оценку.\nЧтобы получить точную (несмещенную) оценку дисперсии по выборке, нам нужно исправить знаменатель дроби — вместо \\(n\\) использовать \\(n-1\\):\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^n (\\bar x - x_i)^2\n\\]\nНо почему?\n\n7.5.4.1 Степени свободы\nВо всём виновата выборка.\nВзглянем на формулу дисперсии: в неё входит среднее арифметическое. То есть для того, чтобы рассчитать дисперсию на выборке, сначала нам необходимо на этой же выборке рассчитать среднее. Тем самым, мы как бы «фиксируем» нашу выборку этим средним значением — у значений нашего распределения становится меньше свободы для варьирования. Теперь свободно варьироваться могут \\(n-1\\) наблюдение, так как последнее всегда будет возможно высчитать, исходя из среднего значения. По этой причине нам необходимо корректировать исходную формулу расчета дисперсии.\nА что если не корректировать?\nМы стремимся к тому, чтобы наши расчеты на выборке достаточно точно [на столько, на сколько это возможно] отражали то, что происходит в генеральной совокупности. Математики-статистики выяснили, что та оценка, которая хорошо подходит для расчета дисперсии генеральной совокупности, при применении на выборке даёт смещенные оценки. То есть оценка выборочной дисперсии по формуле дисперсии для генеральной совокупности содержит в себе смещение — некоторую систематическую ошибку. Это нехорошо.\nК концепту степеней свободы мы ещё неоднократно вернемся. Сейчас хотелось бы, чтобы сформировалось какое-то минимальное более-менее освязаемое понимание того, почему они вообще нам нужны. Если на основе предыдущих абзацев раздела этого сделать не получилось, то давайте попробуеи воспользоваться следующим рассуждением.\nНа выборке происходят некоторые статистические преколы, которые несколько портят нам жизнь, и нам их неободимо учесть, чтобы адекватно оценивать то, что происходит в генеральной совокупности. В частности, нам необходимо учитывать количество степеней свободы, которое есть в нашей выборке. Для расчета выборочной дисперсии оно равно \\(n-1\\), так как мы для того, чтобы рассчитать дисперсию по выборке, нам сначала по той же самой выборке надо рассчитать ещё одну оценку — среднее арифметическое. Этот расчет заберет одну степень свободы у нашей выборки.\n\n\n7.5.4.2 Дисперсия генеральной совокупности\n\n\n\n\n\n7.5.4.3 Дисперсия выборки\n\n\n\n\n\n\n7.5.5 Стандартное отклонение\n\n\n\nИ вот мы получили невероятное! У нас есть формула расчета меры разброса, которая позволяет учесть сами значения переменной! Ну не чудо ли!\nЧудо, конечно, однако есть некоторая проблема. Мы возводили отклонения в квадрат. Представим, что мы хотим посчитатить дисперсию роста студентов психфака. Пусть мы измеряли рост в метрах. Отклонения тоже будут в метрах (потому что среднее — это тоже метры, а если из метров вычитать метры, то мы получим метры). А при возведении метров в квадрат получаются метры в квадрате. Очевидно, что если мы модели квадратные метры на некоторое число (\\(n\\)), они все еще останутся метрами в квадрате.\nО, нет! А счастье было так близко, так возможно! Получается, мы не можем интерпретировать эту меру разброса? Не сможем даже нарисовать?\nДа, но это не очень большая беда. Для того, чтобы вернуться обратно к единицам измерения нашей переменной, нам всего лишь нужно извлечь корень из дисперсии:\n\\[\n\\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\bar x - x_i)^2}\n\\]\nМы получили величину, называемую стандартным отклонением (standard deviation). Чем она хороша? Тем, что её размерность совпадает с размерностью нашей переменной. Стандартное отклонение уже может быть достаточно интерпретабельно и хорошо визуализируемо.\nКстати, формула выше, которая что-то очень напоминает, — это стандартное отклонение генеральной совокупности, потому что под корнем стоит дисперсия генеральной совокупности.\nЧтобы посчитать стандартное отклонение по выборке, нам надо извлечь корень из выборочной дисперсии:\n\\[\ns = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (\\bar x - x_i)^2}\n\\]\n\n\n7.5.6 Свойства дисперсии и стандартного отклонения\n\nЕсли к каждому значению распределения прибавить некоторое число (константу), то дисперсия не изменится.\n\n\\[\nD_{x+c} = D_{x}\n\\]\nВот почему:\n\\[\nD_{x+c} = \\frac{\\sum_{i=1}^n \\big((\\bar x + c) - (x_i + c)\\big)^2}{n-1} = \\frac{\\sum_{i=1}^n \\big(\\bar x + c - x_i - c\\big)^2}{n-1} = \\frac{\\sum_{i=1}^n \\big(\\bar x - x_i\\big)^2}{n-1} = D_x\n\\]\n\nЕсли каждое значение распределение умножить на некоторое число (константу), то дисперсия увеличится в \\(c^2\\) раз.\n\n\\[\nD_{x \\times c} = D_{x} \\times c^2\n\\]\nВот почему:\n\\[\nD_{x \\times c} = \\frac{\\sum_{i=1}^n (c\\bar x - cx_i)^2}{n-1} = \\frac{\\sum_{i=1}^n c^2(\\bar x - x_i)^2}{n-1} = \\frac{c^2 \\sum_{i=1}^n (\\bar x - x_i)^2}{n-1} = D_x \\times c^2\n\\]\n\nЕсли к каждому значению распределения прибавить некоторое число (константу), то стандартное отклонение не изменится.\n\n\\[\ns_{x+c} = s_x\n\\]\nЭто следует из свойства дисперсии:\n\\[\ns_{x+c} = \\sqrt{D_{x+c}} = \\sqrt{D_x} = s_x\n\\]\nКак мы уже видели, распределение просто сдвигается на константу. Например, если к каждому значению синего распределения прибавить \\(2\\), получится красное — разброс у обоих распределений одинаковый:\n\n\n\n\n\n\n\n\n\n\nЕсли каждое значение распределение умножить на некоторое число (константу), то стандартное отклонение увеличится во столько же раз.\n\n\\[\ns_{x \\times c} = s_x \\times c\n\\]\nЭто также следует из свойства дисперсии:\n\\[\ns_{x \\times c} = \\sqrt{D_{x \\times c}} = \\sqrt{D_x \\times c^2} = s_x \\times c\n\\]\nНапример, здесь каждое значение синего распределения умножили на \\(3\\) и получили красное — разброс также увеличился в три раза, поэтому распределение более плоское:\n\n\n\n\n\n\n\n\n\n\n\n7.5.7 Квантили",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "l7.html#quantiles",
    "href": "l7.html#quantiles",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "7.5 Квантили",
    "text": "7.5 Квантили\nВозьмем распределение суммарного балла по шкале «Доверие к техническим интеллектуальным системам». Выглядит оно как-то так:\n\n\n\n\n\n\n\n\n\nТеперь нам понадобится определение квантиля распределения.\nКвантиль — это значение переменной, которое не превышается с определенной вероятностью (обозначим её \\(p\\)). Иначе говоря, слева от значения квантиля лежит \\(p\\%\\) наблюдений.\nПосмотрим на картинки.\nСлева относительно квантиля-0.05 (\\(x_{0.05}\\)) лежит 5% наблюдений:\n\n\n\n\n\n\n\n\n\nСлева относительно квантиля-0.68 (\\(x_{0.68}\\)) лежит 68% наблюдений:\n\n\n\n\n\n\n\n\n\nСлева относительно квантиля-0.99 (\\(x_{0.99}\\)) лежит 99% наблюдений:\n\n\n\n\n\n\n\n\n\nИтак, мы поняли, а также приняли и осознали, что такое квантиль. Неясно только, как он нам поможет описать вариативность данных.\n\n7.5.1 Квартили\nДля этого нам пригодятся специально обученные квантили. Оказалось достаточно удобно поделить все наблюдение на четыре равные части — вот так:\n\n\n\n\n\n\n\n\n\nЗначения переменной, которые делят выборку на четыре равные части называются квартили. Получается, что\n\nслева от первого (нижнего) квартиля (\\(Q_1\\), \\(x_{0.25}\\)) лежит 25% наблюдений\nслева от второго (среднего) квартиля (\\(Q_2\\), \\(x_{0.50}\\)) лежит 50% наблюдений\n\nа значит и справа 50% — получается второй квартиль делит выборку пополам — это медиана\n\nслева от третьего (верхнего) квартиля (\\(Q_3\\), \\(x_{0.75}\\)) лежит 75% наблюдений\n\nЧетвертый квартиль не используется, потому что это максимальное значение — слева от него лежит 100% наблюдений.\nКстати, можно также отметить, что первый квартиль — это медиана нижней (меньшей) половины наблюдений, а третий — медиана верней (большей) половины наблюдений.\nВот такая вот прикольная история.\n\n\n7.5.2 Децили\nК слову, делить выборку можно не только на четверти — можно поделить, скажем, на 10 частей и получить децили. Так, слева от первого дециля (\\(x_{0.10}\\)) лежит 10% наблюдений, а слева от третьего (\\(x_{0.30}\\)) — 30%.\nДецили встречаются редко (в основном в психометрике), но знать о них полезно.\n\n\n7.5.3 Перцентили\nГораздо чаще встречаются перцентили — значения переменной, которые делят выборку на 100 равных частей. Например, так устроен ваш рейтинг. Только стоит помнить, что в рейтинге отсчет ведется от максимального среднего балла, поэтому если у вас нулевой перцентиль (\\(x_{0.00}\\)) по программе, значит выше вас в рейтинге никого нет. А если ваш перцентиль, скажем, 36-ой (\\(x_{0.36}\\)), то выше вас в рейтинге 36% ваших однокурсников, то есть вы все ещё в первой половине рейтинга, что очень неплохо!\n\n\n7.5.4 Интерквартильный размах\nИ — о, ура! — мы наконец-то добрались до того, ради чего тут собрались! Зная первый и третий квартили распределения, можно рассчитать интерквартильный (межквартильный) размах (interquartile range, IQR).\n\\[\n\\mathrm{IQR}(X) = Q_3(X) - Q_1(X)\n\\]\nИнтерквартильный размах — это разница между третьим и первым квартилем распределения. Эта величина описывает интервал значений признака, в котором лежит 50% наблюдений.\n\n\n\n\n\n\n\n\n\nВ данном случае он равен 40 — то есть 50% наблюдений лежит в пределах 40 единиц шкалы.\n\n\n7.5.5 Визуализация квартилей. Боксплот\nОтображать квартили на гистограмме, во-первых, совершенно неудобно, а во-вторых, не то чтобы график получается информативный. Для визуализации квартилей придумали специальный тип графика — ящик с усами, или боксплот (boxplot).\n\n\n\n\n\n\n\n\n\nПрикольная ерунда. Научимся его читать.\nЗначения переменной идут по вертикальной оси (оси ординат). По горизонтальной оси (оси абсцисс) здесь ничего не идет5. Жирная линия по середине ящика — медиана (второй квартиль). Нижняя граница ящика — первый квартиль, верхняя — третий. Получается, что границы ящика показывают нам значения, в пределах которых лежит половина наблюдений.\nНижний ус — первый квартиль минус полтора межквартильных размаха. Верхний ус — третий квартиль плюс полтора мехквартильных размаха.\n\n\n\n\n\n\n\n\n\n\n\n\nЗамечание\n\nЯщик может быть асимметричным — то есть верхняя его часть (расстояние между медианой и третьим квартилем) и нижняя его часть (расстояние между медианой и первым квартилем) могут быть разными. Это нам говорит об асимметричности распределения. Усы также могут быть неравными, если один из них упирается в максимум / минимум — тоже по причине асимметричности распределения.\n\n\nНу, допустим. А что тогда точки?\n\n7.5.5.1 Выбросы\nВообще справедливо было бы задаться вопросом, а зачем нам вообще усы на этом графике? И почему мы прибавляем полтора межквартильных размаха?\nЭто один из подходов к определению нехарактерных значений — выбросов. При исследовании данных мы часто задаемся вопросом, если ли в наших данных такие значения, которые сильно отличаются от распределения той или иной переменной. Но как определить это самое «сильно»?\nВот один из подходов. Будем считать, что значения, которые укладываются в интервал \\((Q_1 - 1.5 \\times \\mathrm{IQR}, \\, Q_3 + 1.5 \\times \\mathrm{IQR})\\), нас устраивают. Все что попадает в этот интервал — это «нормальные», типичные значения нашей переменной. Те же, которые будут находиться за пределами этого интервала, мы назовем нетипичными, аномальными значениями, или выбросами. Эти значения и будут отмечены точками на графике boxplot.\nЧто с ними делать? Во-первых, содержательной анализировать. Выбросы могут возникнуть по разным причинам. Может быть испытуемый отвлекся на прилетевшего в окно голубя, и у нас в данных появилось время реакции 200 секунд. Такие выбросы мы можем исключить из данных. А возможно в нашу выборку попали какие-то люди, которые, скажем, очень сильно или очень слабо доверяют искусственному интеллекту (как в примере на рисунке). Эти наблюдения необходимо дополнительно проанализировать — возможно, это представители специфических групп нашей генеральной совокупности (например, программисты-разработчики или люди пенсионного возраста). Анализ принесет нам дополнительную информацию, которую мы могли не учесть при планировании исследования. Крч, думать надо. И собирать побольше данных, чтобы можно было найти содержательную интерпретацию происходящему.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "l7.html#andan-descriptives_variation_comparison",
    "href": "l7.html#andan-descriptives_variation_comparison",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "7.6 Сравнение мер разброса",
    "text": "7.6 Сравнение мер разброса\nКак и разные меры центральной тенденции, разные меры разброса по-своему хороши. Более того, они дружат с мерами центральной тенденции. Так, с медианой используется мехквартильных размах, а со средним арифметическим — стандартное отклонение.\nРазмах подходит для всего сразу. Его стоит рассчитать, чтобы составить самое первое представление в разбросе, о границах измерения изучаемого признака [на нашей выборке].\nСтоит также отметить, что все, что мы тут обсуждали, совершенно не годиться для номинативных переменных. Однако у них тоже есть вариативность. Согласитель, что выборка из Питера, Москвы, и Казани более вариативна, чем выборка из Москвы. Аналогом меры разброса для номинальной переменной можно назвать количество уникальных значений этой переменной.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "l7.html#andan-descriptives-skewness",
    "href": "l7.html#andan-descriptives-skewness",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "7.7 Асимметрия",
    "text": "7.7 Асимметрия",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "l7.html#andan-descriptives-kurtosis",
    "href": "l7.html#andan-descriptives-kurtosis",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "7.8 Эксцесс",
    "text": "7.8 Эксцесс",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "l7.html#корреялционный-анализ",
    "href": "l7.html#корреялционный-анализ",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "7.2 Корреялционный анализ",
    "text": "7.2 Корреялционный анализ\nДо этого момента мы рассматривали только отдельные переменные и их характерики, однако в практике мы редко работаем только с одной переменной. Как правило, у нас есть многомерное пространство признаков, и нас интересуют взаимосвязи между ними.\n\n7.2.1 Ковариация\nМы хотим описать имеющиеся взаимосвязи как можно проще и опираясь на то, что у нас уже есть. Мы говорили, что дисперсия, или вариация, заключает в себе информацию об изменчивости признака. Если мы хотим исследовать взаимосвязь между признаками, то логично будет посмотреть, как изменяется один из признаков при изменении другого — иначе говоря, рассчитать совместную изменчивость признаков, или ко-вариацию (covariance).\nКак мы её будем считать? Подумаем графически. Расположим две переменные на осях и сопоставим каждому имеющемуся наблюдению точку на плоскости.\nКАРТИНКА\nОтметим средние значения по обеим переменным.\nКАРТИНКА\nЗаметим, что если наши наблюдения по переменной \\(x_1\\) отклоняются в большую сторону, то они отклоняются в большую сторону и по переменной \\(x_2\\). Аналогично, если они будут отклоняться в меньшую сторону по \\(x_1\\), то в меньшую же сторону они будут отклоняться и по \\(x_2\\).\nКАРТИНКА\nПолучается, мы можем на основании согласованности отклонений уже заключить о направлении связи. Произведение отклонений по обеим величинам будет положительно, если отклонения сонаправленны. Запишем это математически.\n\\[\n(\\bar x_1 - x_{i1}) (\\bar x_2 - x_{i2}) &gt; 0 \\Leftarrow \\big( (\\bar x_1 - x_{i1}) &gt; 0 \\wedge (\\bar x_2 - x_{i2}) &gt; 0 \\big) \\vee \\big( (\\bar x_1 - x_{i1}) &lt; 0 \\wedge (\\bar x_2 - x_{i2}) &lt; 0 \\big)\n\\]\nСоответственно, если отклонения будут направлены в разные стороны, из произведение будет отрицательным. Ну, осталось только понять, как совместные отклонения организованы в среднем — это и будет ковариацией двух величин:\n\\[\n\\mathrm{cov}(X_1, X_2) = \\frac{1}{n} \\sum_{i=1}^n (\\bar X_1 - x_{i1}) (\\bar X_2 - x_{i2})\n\\]\nВажно отметить, что ковариация улавливает только линейную составляющую взаимосвязи между признаками, поэтому если \\(\\mathrm{cov}(X_1,X_2) = 0\\), то мы можем сказать, что между переменными нет линейной взаимосвязи, однако это не значит, что между этими переменными нет никакой другой зависимости.\nКАРТИНКА\nУ ковариации есть два важных недостатка:\n\nэто размерная величина, поэтому её значение зависит от единиц измерения признаков,\nона зависит от дисперсий признаков, поэтому по её значению можно определить только направление связи (прямая или обратная), однако ничего нельзая сказать о силе связи.\n\nПоэтому нам нужно как-то модицифировать эту статистику, чтобы мы могли больше вытащить из её значения.\n\n\n7.2.2 Корреляция\nРаз ковариация зависит от дисперсии, то можно сделать некоторые математические преобразования, чтобы привести эмпирические распределения к какому-то одному виду — сделать так, чтобы они имели одинакое математическое ожидание (среднее) и одинаковую дисперсию. С этой задачей прекрасно справляется стандартизация. Напоминаю формулу:\n\\[\nx_i^* = \\frac{x_i - \\bar x}{s}\n\\]\nПосле такого преобразования математическое ожидание нашего распределения будет равно нуля, а стандартное отклонение — единице. Это избавит нас от влияния дисперсии на значение ковариации. Ковариация двух стандартно нормально распределенных величин называется корреляцией (correlation).\n\\[\n\\mathrm{cov}(X_1^*, X_2^*) = \\frac{1}{n-1} \\sum_{i=1}^n x_{i1}^* x_{i2}^* = \\mathrm{corr}(X_1, X_2),\n\\] где \\(X_1^*\\) и \\(X_2^*\\) — стандартизированные величины \\(X_1\\) и \\(X_2\\) соответственно.\nКорреляцию можно выразить через ковариацию:\n\\[\n\\mathrm{corr}(X_1, X_2) = \\frac{1}{n-1} \\sum_{i=1}^n \\Big( \\frac{\\bar x_1 - x_{i1}}{s_1} \\Big) \\Big( \\frac{\\bar x_2 - x_{i2}}{s_2} \\Big) =\n\\frac{1}{s_1 s_2} \\Big( \\frac{1}{n-1} \\sum_{i=1}^n (\\bar x_1 - x_{i1})(\\bar x_2 - x_{i2}) \\Big) = \\frac{\\mathrm{cov}(X_1, X_2)}{s_1 s_2}\n\\]\nЕсли внимательно всмотреться в формуле, то можно обнаружить, что корреляция это не что иное, как стандартизированное значение ковариации.\nКоэффициент корреляции имеет четкие пределы изменения: \\([-1; \\,1]\\). Крайнее левое значение говорит о том, что присутствует полная обратная линейная взаимосвязь, крайнее правое — что присутствует полная прямая линейная взаимосвязь. Как и ковариация, корреляция ловит только линейную составляющую связи, поэтому нулевое значение корреляци показывает, что между переменными отсутствует линейная взаимосвязь. Это всё еще не значит, что связи нет вовсе.\n\n7.2.2.1 Интерпретация коэффициента корреляции\nПреимущество корреляции над ковариацией в том, что она отражает не только направление, но и силу связи:\n\n\n\nЗначение коэффициента\nИнтерпретация\n\n\n\n\n\\(-1.0\\) – \\(-0.9\\)\nочень сильная обратная связь\n\n\n\\(-0.9\\) – \\(-0.7\\)\nсильная обратная связь\n\n\n\\(-0.7\\) – \\(-0.5\\)\nсредняя обратная связь\n\n\n\\(-0.5\\) – \\(-0.3\\)\nслабая обратная связь\n\n\n\\(-0.3\\) – \\(0.0\\)\nочень слабая обратная связь\n\n\n\\(0.0\\) – \\(0.3\\)\nочень слабая прямая связь\n\n\n\\(0.3\\) – \\(0.5\\)\nслабая прямая связь\n\n\n\\(0.5\\) – \\(0.7\\)\nсредняя прямая связь\n\n\n\\(0.7\\) – \\(0.9\\)\nсильная прямая связь\n\n\n\\(0.9\\) – \\(1.0\\)\nочень сильная прямая связь\n\n\n\n\n\n7.2.2.2 Тестирование статистической значимости коэффициента корреляции\nОценку коэффициента корреляции мы получаем методом моментов, заменяя истинный момент \\(\\rho_{ij}\\) выборочным \\(r_{ij}\\):\n\\[\n\\hat \\rho_{ij} = \\overline{\\big( (X_{ki} - \\bar X_i) (X_{kj} - \\bar X_j) \\big)} = r_{ij}\n\\]\nЕсли в генеральной совокупности связь между признаками отсутствует, то есть \\(\\rho_{ij} = 0\\), будет ли равен нулю \\(r_{ij}\\)? Можно с уверенностью сказать, что не будет, так как выборочный коэффициент корреляции — случайная величина. А мы помним, что вероятность принятия случайной величиной своего конкретного значения равна нулю.\nТогда необходимо протестировать статистическую гипотезу:\n\\[\n\\begin{split}\nH_0&: \\rho_{ij} = 0 \\; \\text{(линейной связи нет)} \\\\\nH_1&: \\rho_{ij} \\neq 0 \\; \\text{(наиболее частый вариант альтернативы)}\n\\end{split}\n\\]\nДля проверки нулевой гипотезы используется следующая статистика:\n\\[\nt = \\frac{r_{ij}}{\\sqrt{\\frac{1 - r^2_{ij}}{n-2}}} \\overset{H_0}{\\thicksim} t(\\nu = n-2)\n\\]\nВывод о статистической значимости коэффициента корреляции делается согласно алгоритму тестировния статистических гипотез.\n\n\n7.2.2.3 Размер эффекта в корреляционном анализе\nЕще одна статистика, которая нам необходима — это размер эффекта. Напомним себе, что размер эффекта — это численное выражение силы взаимосвязи между переменными в генеральной совокупности. Здесь нам необходимо призадуматься, и осознать, что вообще-то корреляция сама по себе выражает силу взаимосвязи между переменными. И, да, это правда — размером эффекта для коэффициента корреляции является сам коэффициент корреляции. Удобненько.\nТак, к сожалению, будет не всегда, но вот с корреляцией это так. Ну, и хорошо.\nРекомендации по интерпретации [абсолютного значения] коэффициента корреляции с точки зрения размера эффекта для социальных наук такие:\n\n\n\nЗначение коэффициента\nРазмер эффекта\n\n\n\n\n\\(0.1\\)\nМалый (small)\n\n\n\\(0.3\\)\nСредний (medium)\n\n\n\\(0.7\\)\nБольшой (large)\n\n\n\n\n\n7.2.2.4 Доверительный интервал для коэффициента корреляции\nС построением интервальной оценки коэффциента корреляции возникают некоторые сложности. Наша задача состоит в том, чтобы определить в каких границах будет лежать значение истинного коэффициента корреляции с заданной вероятностью:\n\\[\n\\mathbb{P}(\\rho_{ij,\\min} &lt; \\rho_{ij} &lt; \\rho_{ij,\\max}) = \\gamma\n\\]\nНам необходимо найти статистику, закон распределения корой известен, однако ранее упомянутся статистика не подходит, так как она имеет распределение Стьюдента, когда верна нулевая гипотеза об отсутствии связи. Если же мы строим интервальную оценку, нас интересует случай наличия связи.\nТакую статистику искали долго, и её удалось найти, когда ввели определённое преобразование выборочного критерия корреляции — z-преобразования Фишера:\n\\[\nz(r_{ij}) = \\frac{1}{2} \\ln \\frac{1 + r_{ij}}{1 - r_{ij}} \\thicksim \\mathcal{N}(\\bar z_{ij}, \\tfrac{1}{n-3}),\n\\] где \\(n\\) — объём выборки, а \\(\\bar z_{ij}\\) получается расчётом по указанной формуле после подставления точечной оценки коэффициента корреляции.\nТогда интервальная оценка для величины \\(z_{ij, \\mathrm{true}}\\) приобретает такой вид:\n\\[\n\\mathbb{P}\\Big( \\bar z_{ij} - t_\\gamma \\sqrt{\\tfrac{1}{n-3}} &lt; z_{ij, \\mathrm{true}} &lt; \\bar z_{ij} + t_\\gamma \\sqrt{\\tfrac{1}{n-3}}  \\Big) = \\gamma\n\\]\nДалее путём обратного преобразования получаются значения границ интервала \\((\\rho_{ij,\\min}, \\; \\rho_{ij,\\max})\\).\n\n\n\n7.2.3 Коэффициенты корреляции для разных шкал\nДла разных шкал разработаны разные коэффициенты корреляции. Оценки коэффициентов будут рассчитываться по-разному, но логика тестирования статистических гипотез остаётся одинаковой.\n\n\n\n\n\n\n\n\nПеременная \\(X\\)\nПеременная \\(Y\\)\nМера связи\n\n\n\n\nИнтервальная или отношений\nИнтервальная или отношений\nКоэффициент Пирсона\n\n\nРанговая, интервальная или отношений\nРанговая, интервальная или отношений\nКоэффициент Спирмена\n\n\nРанговая\nРанговая\nКоэффициент Кенделла",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "l7.html#частный-и-множественный-коэффициент-корреляции",
    "href": "l7.html#частный-и-множественный-коэффициент-корреляции",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "7.3 Частный и множественный коэффициент корреляции",
    "text": "7.3 Частный и множественный коэффициент корреляции\nЕсли у нас два признака, то с ними всё достаточно понятно. А если признаком много? Тогда у нас могут быть сложные взаимосвязи, и возможен такой случай, что некоторый признак оказывает связан как с одним, так и с другим из интересующих нас. Таким образом, мы можем наблюдать ложную корреляцию. Чтобы избавиться от влияния сторонних признаков, используюся частные коэффициенты корреляции.\nВ случае нескольких переменных удобно представить результаты вычисления коэффициентов корреляции в виде корреляционной матрицы, отображающей связи всех признаков со всеми:\n\\[\nR =\n\\begin{pmatrix}\n1 & r_{12} & \\dots & r_{1p} \\\\\nr_{12} & 1 & \\dots & r_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{p1} & r_{p2} & \\dots & 1\n\\end{pmatrix}\n\\]\nВ корреляционной матрице на главной диагонали стоят единицы, отражающай связь переменной в самой собой — разумеется, она будет абсолютно линейная.\nМатрица, как можно заметить, симметрична относительно главной диагонали, так как \\(r_{ij} = r_{ji}\\).\nИтак, возвращается к частному коэффициенту корреляции. Посмотрим пример для случая трех переменных:\n\\[\nR =\n\\begin{pmatrix}\n1 & r_{12} & r_{13} \\\\\nr_{21} & 1 & r_{23} \\\\\nr_{31} & r_{32} & 1\n\\end{pmatrix}\n\\]\n\\[\nr_{12,3} = \\frac{r_{12} - r_{13} \\cdot r_{23}}{\\sqrt{(1 - r^2_{23})(1-r^2_{13})}}\n\\]\n\\[\n\\begin{split}\nH_0&: \\rho_{12,3} = 0 \\\\\nH_1&: \\rho_{12,3} \\neq 0 \\\\\nt &= \\frac{r_{12,3} \\sqrt{n-3}}{\\sqrt{1 - r^2_{12,3}}} \\overset{H_0}{\\thicksim} t(\\nu = n-3)\n\\end{split}\n\\]\nХорошо, а если нас интересует связь одного признака с несколькими сразу? Тогда нам нужен множественный коэффициент корреляции. Он также вычисляется на основе корреляционной матрицы и определяется следующим образом. Пусть нас интересует связь первого признака со всеми остальными:\n\\[\nR_1 = \\sqrt{1 - \\frac{\\det R}{A_{11}}}\n\\]\nКвадрат множественного коэффициента корреляции называется коэффициентом детерминации6. Он показывает, во-первых, степень тесноты связи данного признака со всеми остальными, но, кроме того, ещё и долю дисперсии данного признака, определяемую вариацией все остальных признаков, включенных в данную корреляционную модель.\nМы подробнее его изучим в следуюшей теме, а также увидим, где нам его найти, чтобы не считать руками.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "l7.html#другие-корреляции",
    "href": "l7.html#другие-корреляции",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "7.4 Другие корреляции",
    "text": "7.4 Другие корреляции\nМожно коррелировать не только количественные и ранговые шкалы между собой, но и качественные тоже:\n\n\n\nПеременная \\(X\\)\nПеременная \\(Y\\)\nМера связи\n\n\n\n\nДихотомическая\nДихотомическая\n\\(\\phi\\)-коэффициент\n\n\nДихотомическая\nРанговая\nРангово-бисериальный коэффициент\n\n\nДихотомическая\nИнтервальная или отношений\nБисериальный коэффициент\n\n\n\n\n7.4.1 \\(\\phi\\)-коэффициент\nЭтот коэффициент позволяет рассчитать корреляцию между двумы дихотомическими шкалами. Он основан на расчёте статистики \\(\\chi^2\\).\n\n7.4.1.1 Критерий независимости Пирсона\nПо двум дихотомическим переменным можно построить таблицы сопряженности. Сам хи-квадрат тестирует гипотезу о том, что между двумя категориальными переменными нет связи. Он это делает путём сравнения теоретической и эмпирической таблицы частот.\nЭмпирическую таблицу [частот] мы получаем по результатам наблюдений:\n\n\n\n\n\\(X_1\\)\n\\(X_2\\)\n\n\n\n\n\\(Y_1\\)\n\\(p_{X_1,Y_1} = a\\)\n\\(p_{X_2,Y_1} = b\\)\n\n\n\\(Y_2\\)\n\\(p_{X_1,Y_2} = c\\)\n\\(p_{X_2,Y_2} = d\\)\n\n\n\nДалее вычисляются теоретические частоты:\n\n\n\n\n\n\n\n\n\n\\(X_1^*\\)\n\\(X_2^*\\)\n\n\n\n\n\\(Y_1^*\\)\n\\(\\frac{(a+b) \\times (a+c)}{N}\\)\n\\(\\frac{(b+a) \\times (b+d)}{N}\\)\n\n\n\\(Y_2^*\\)\n\\(\\frac{(c+d) \\times (a+c)}{N}\\)\n\\(\\frac{(d+c) \\times (b + d)}{N}\\)\n\n\n\nгде \\(N = a + b + c + d\\).\nЗатем считаются расхождения частот, которые суммируются и получается статистика \\(\\chi^2\\):\n\\[\n\\chi^2 = \\sum_{i,j} \\frac{p_{X_i,Y_j} - p_{X_i^*,Y_j^*}}{p_{X_i^*,Y_j^*}}\n\\]\nСтатистика подчиняется распределению \\(\\chi^2\\), и чем больше значение этой статистики, тем сильнее связаны признаки. Статистические гипотезы для критерии независимости Пирсона формулируются так:\n\\[\n\\begin{split}\nH_0 &: p_{X_{i_1}, Y_{j_1}} = p_{X_{i_2}, Y_{j_2}}, \\, i_1 \\neq i_2, \\, j_1 \\neq j_2 \\\\\nH_1 &: \\exists i_1, i_2, j_1, j_2: p_{X_{i_1}, Y_{j_1}} \\neq p_{X_{i_2}, Y_{j_2}}\n\\end{split}\n\\]\n\nПо значению \\(\\chi^2\\) сложно что-то сказать о силе связи, поэтому его нормируют следующим образом, чтобы получить значения от 0 до 1, которые можно интерпретироват аналогично коэффициенту корреляции:\n\\[\n\\phi = \\sqrt{\\frac{\\chi^2}{N}}\n\\]\n\n\n\n7.4.2 Бисериальный коэффициент корреляции\nЭтот коэффициент используется для вычисления корреляции между количественной (\\(y\\)) и категориальной (\\(x\\)) шкалой и рассчитывается следующим образом:\n\\[\nr = \\frac{\\bar x_1 - \\bar x_2}{s_Y} \\sqrt{\\frac{n_1 n_2}{N(N-1)}},\n\\]\nгде \\(\\bar x_1\\) — среднее по элементам переменной \\(y\\) из группы \\(x_1\\), \\(\\bar x_2\\) — среднее по элементам \\(y\\) из группы \\(x_2\\), \\(s_y\\) — стандартное отклонение по переменной \\(y\\), \\(n_1\\) — число элементов в группе \\(x_1\\), \\(n_2\\) — число элементов в группе \\(x_2\\), \\(N\\) — общее число элементов.\nВажно отметить, что несмотря на то, что значение коэффициента может быть как положительным, так и отрицательным, это не влияет на интерпретацию. Это одно из исключений из общего правила.\n\n\n7.4.3 Рангово-бисериальный коэффициент корреляции\nЕсли у нас не количественная, а ранговая шкала, то применяется рангово-бисериальный коэффициент:\n\\[\nr = \\frac{2(\\bar x_1 - \\bar x_2)}{N},\n\\]\nгде \\(\\bar x_1\\) — средний ранг в группе \\(x_1\\), \\(\\bar x_2\\) — средний ранг в группе \\(x_2\\), \\(N\\) — общее количество наблюдений.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "l7.html#andan-cor-fisher-transform",
    "href": "l7.html#andan-cor-fisher-transform",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "7.5 Преобразование Фишера",
    "text": "7.5 Преобразование Фишера\nУпомянутое ранее преобразование Фишера используется не только для вычисления доверительного интервала для коэффициента корреляции. Его применяют для нахождения так называемой pooled correlation — усредненной корреляции по нескольки отдельным коэффициентам. Для этого первоначально приводят коэффициенты корреляции к \\(z\\)-значениям по формуле:\n\\[\nz_i = \\frac{1}{2} \\ln \\frac{1 + r_i}{1 - r_i} = \\mathop{\\mathrm{artanh}}(r_i)\n\\]\nДалее усредняют каким-либо из способов — зависит от переменных, характеристик выборки и решаемой задачи — а затем возвращаются к исходной «размерности» корреляции через обратное преобразование:\n\\[\nr_P = \\dfrac{e^{2z_P} - 1}{e^{2z_P} + 1} = \\tanh(z_P)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "l7.html#корреляционный-анализ",
    "href": "l7.html#корреляционный-анализ",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "7.2 Корреляционный анализ",
    "text": "7.2 Корреляционный анализ\nДо этого момента мы рассматривали только отдельные переменные и их характерики, однако в практике мы редко работаем только с одной переменной. Как правило, у нас есть многомерное пространство признаков, и нас интересуют взаимосвязи между ними.\n\n7.2.1 Ковариация\nМы хотим описать имеющиеся взаимосвязи как можно проще и опираясь на то, что у нас уже есть. Мы говорили, что дисперсия, или вариация, заключает в себе информацию об изменчивости признака. Если мы хотим исследовать взаимосвязь между признаками, то логично будет посмотреть, как изменяется один из признаков при изменении другого — иначе говоря, рассчитать совместную изменчивость признаков, или ко-вариацию (covariance).\nКак мы её будем считать? Подумаем графически. Расположим две переменные на осях и сопоставим каждому имеющемуся наблюдению точку на плоскости.\n\n\n\n\n\n\n\n\n\nОтметим средние значения по обеим переменным.\n\n\n\n\n\n\n\n\n\nЗаметим, что если наши наблюдения по переменной \\(X_1\\) отклоняются в большую сторону от среднего, то они отклоняются в большую сторону от среднего и по переменной \\(X_2\\). Аналогично, если они будут отклоняться от среднего в меньшую сторону по \\(X_1\\), то в меньшую же сторону от среднего они будут отклоняться и по \\(X_2\\). Обозначим сонаправленные отклонения синим, а разнонаправленные — красным.\n\n\n\n\n\n\n\n\n\nЕсли же у нас обратная ситуация — наблюдения по переменной \\(X_1\\) отклоняются в большую сторону от среднего и вместе с этим они отклоняются в меньшую сторону от среднего и по переменной \\(X_2\\), и наоборот — то мы получим следующую картину:\n\n\n\n\n\n\n\n\n\nЕсли же отклонения от среднего никак не связаны у двух переменны, то это будет выглядеть так:\n\n\n\n\n\n\n\n\n\nПолучается, мы можем на основании согласованности отклонений уже заключить о направлении связи. Произведение отклонений по обеим величинам будет положительно, если отклонения сонаправленны. Запишем это математически.\n\\[\n(\\bar x_1 - x_{i1}) (\\bar x_2 - x_{i2}) &gt; 0 \\Leftarrow \\big( (\\bar x_1 - x_{i1}) &gt; 0 \\wedge (\\bar x_2 - x_{i2}) &gt; 0 \\big) \\vee \\big( (\\bar x_1 - x_{i1}) &lt; 0 \\wedge (\\bar x_2 - x_{i2}) &lt; 0 \\big)\n\\]\nСоответственно, если отклонения будут направлены в разные стороны, из произведение будет отрицательным. Ну, осталось только понять, как совместные отклонения организованы в среднем — это и будет ковариацией двух величин:\n\\[\n\\mathrm{cov}(X_1, X_2) = \\frac{1}{n} \\sum_{i=1}^n (\\bar X_1 - x_{i1}) (\\bar X_2 - x_{i2})\n\\]\nВажно отметить, что ковариация улавливает только линейную составляющую взаимосвязи между признаками, поэтому если \\(\\mathrm{cov}(X_1,X_2) = 0\\), то мы можем сказать, что между переменными нет линейной взаимосвязи, однако это не значит, что между этими переменными нет никакой другой зависимости.\n\n\n\n\nНа рисунке приведены стандартизированные значения ковариации.\n\n\n\nУ ковариации есть два важных недостатка:\n\nэто размерная величина, поэтому её значение зависит от единиц измерения признаков,\nона зависит от дисперсий признаков, поэтому по её значению можно определить только направление связи (прямая или обратная), однако ничего нельзая сказать о силе связи.\n\nПоэтому нам нужно как-то модицифировать эту статистику, чтобы мы могли больше вытащить из её значения.\n\n\n7.2.2 Корреляция\nРаз ковариация зависит от дисперсии, то можно сделать некоторые математические преобразования, чтобы привести эмпирические распределения к какому-то одному виду — сделать так, чтобы они имели одинакое математическое ожидание (среднее) и одинаковую дисперсию. С этой задачей прекрасно справляется стандартизация. Напоминаю формулу:\n\\[\nx_i^* = \\frac{x_i - \\bar x}{s}\n\\]\nПосле такого преобразования математическое ожидание нашего распределения будет равно нуля, а стандартное отклонение — единице. Это избавит нас от влияния дисперсии на значение ковариации. Ковариация двух стандартно нормально распределенных величин называется корреляцией (correlation).\n\\[\n\\mathrm{cov}(X_1^*, X_2^*) = \\frac{1}{n-1} \\sum_{i=1}^n x_{i1}^* x_{i2}^* = \\mathrm{corr}(X_1, X_2),\n\\] где \\(X_1^*\\) и \\(X_2^*\\) — стандартизированные величины \\(X_1\\) и \\(X_2\\) соответственно.\nКорреляцию можно выразить через ковариацию:\n\\[\n\\mathrm{corr}(X_1, X_2) = \\frac{1}{n-1} \\sum_{i=1}^n \\Big( \\frac{\\bar x_1 - x_{i1}}{s_1} \\Big) \\Big( \\frac{\\bar x_2 - x_{i2}}{s_2} \\Big) =\n\\frac{1}{s_1 s_2} \\Big( \\frac{1}{n-1} \\sum_{i=1}^n (\\bar x_1 - x_{i1})(\\bar x_2 - x_{i2}) \\Big) = \\frac{\\mathrm{cov}(X_1, X_2)}{s_1 s_2}\n\\]\nЕсли внимательно всмотреться в формуле, то можно обнаружить, что корреляция это не что иное, как стандартизированное значение ковариации.\nКоэффициент корреляции имеет четкие пределы изменения: \\([-1; \\,1]\\). Крайнее левое значение говорит о том, что присутствует полная обратная линейная взаимосвязь, крайнее правое — что присутствует полная прямая линейная взаимосвязь. Как и ковариация, корреляция ловит только линейную составляющую связи, поэтому нулевое значение корреляци показывает, что между переменными отсутствует линейная взаимосвязь. Это всё еще не значит, что связи нет вовсе.\n\n7.2.2.1 Интерпретация коэффициента корреляции\nПреимущество корреляции над ковариацией в том, что она отражает не только направление, но и силу связи:\n\n\n\nЗначение коэффициента\nИнтерпретация\n\n\n\n\n\\(-1.0\\) – \\(-0.9\\)\nочень сильная обратная связь\n\n\n\\(-0.9\\) – \\(-0.7\\)\nсильная обратная связь\n\n\n\\(-0.7\\) – \\(-0.5\\)\nсредняя обратная связь\n\n\n\\(-0.5\\) – \\(-0.3\\)\nслабая обратная связь\n\n\n\\(-0.3\\) – \\(0.0\\)\nочень слабая обратная связь\n\n\n\\(0.0\\) – \\(0.3\\)\nочень слабая прямая связь\n\n\n\\(0.3\\) – \\(0.5\\)\nслабая прямая связь\n\n\n\\(0.5\\) – \\(0.7\\)\nсредняя прямая связь\n\n\n\\(0.7\\) – \\(0.9\\)\nсильная прямая связь\n\n\n\\(0.9\\) – \\(1.0\\)\nочень сильная прямая связь\n\n\n\n\n\n7.2.2.2 Тестирование статистической значимости коэффициента корреляции\nОценку коэффициента корреляции мы получаем методом моментов, заменяя истинный момент \\(\\rho_{ij}\\) выборочным \\(r_{ij}\\):\n\\[\n\\hat \\rho_{ij} = \\overline{\\big( (X_{ki} - \\bar X_i) (X_{kj} - \\bar X_j) \\big)} = r_{ij}\n\\]\nЕсли в генеральной совокупности связь между признаками отсутствует, то есть \\(\\rho_{ij} = 0\\), будет ли равен нулю \\(r_{ij}\\)? Можно с уверенностью сказать, что не будет, так как выборочный коэффициент корреляции — случайная величина. А мы помним, что вероятность принятия случайной величиной своего конкретного значения равна нулю.\nТогда необходимо протестировать статистическую гипотезу:\n\\[\n\\begin{split}\nH_0&: \\rho_{ij} = 0 \\; \\text{(линейной связи нет)} \\\\\nH_1&: \\rho_{ij} \\neq 0 \\; \\text{(наиболее частый вариант альтернативы)}\n\\end{split}\n\\]\nДля проверки нулевой гипотезы используется следующая статистика:\n\\[\nt = \\frac{r_{ij}}{\\sqrt{\\frac{1 - r^2_{ij}}{n-2}}} \\overset{H_0}{\\thicksim} t(\\nu = n-2)\n\\]\nВывод о статистической значимости коэффициента корреляции делается согласно алгоритму тестировния статистических гипотез.\n\n\n7.2.2.3 Размер эффекта в корреляционном анализе\nЕще одна статистика, которая нам необходима — это размер эффекта. Напомним себе, что размер эффекта — это численное выражение силы взаимосвязи между переменными в генеральной совокупности. Здесь нам необходимо призадуматься, и осознать, что вообще-то корреляция сама по себе выражает силу взаимосвязи между переменными. И, да, это правда — размером эффекта для коэффициента корреляции является сам коэффициент корреляции. Удобненько.\nТак, к сожалению, будет не всегда, но вот с корреляцией это так. Ну, и хорошо.\nРекомендации по интерпретации [абсолютного значения] коэффициента корреляции с точки зрения размера эффекта для социальных наук такие:\n\n\n\nЗначение коэффициента\nРазмер эффекта\n\n\n\n\n\\(0.1\\)\nМалый (small)\n\n\n\\(0.3\\)\nСредний (medium)\n\n\n\\(0.7\\)\nБольшой (large)\n\n\n\n\n\n7.2.2.4 Доверительный интервал для коэффициента корреляции\nС построением интервальной оценки коэффциента корреляции возникают некоторые сложности. Наша задача состоит в том, чтобы определить в каких границах будет лежать значение истинного коэффициента корреляции с заданной вероятностью:\n\\[\n\\mathbb{P}(\\rho_{ij,\\min} &lt; \\rho_{ij} &lt; \\rho_{ij,\\max}) = \\gamma\n\\]\nНам необходимо найти статистику, закон распределения корой известен, однако ранее упомянутся статистика не подходит, так как она имеет распределение Стьюдента, когда верна нулевая гипотеза об отсутствии связи. Если же мы строим интервальную оценку, нас интересует случай наличия связи.\nТакую статистику искали долго, и её удалось найти, когда ввели определённое преобразование выборочного критерия корреляции — z-преобразования Фишера:\n\\[\nz(r_{ij}) = \\frac{1}{2} \\ln \\frac{1 + r_{ij}}{1 - r_{ij}} \\thicksim \\mathcal{N}(\\bar z_{ij}, \\tfrac{1}{n-3}),\n\\] где \\(n\\) — объём выборки, а \\(\\bar z_{ij}\\) получается расчётом по указанной формуле после подставления точечной оценки коэффициента корреляции.\nТогда интервальная оценка для величины \\(z_{ij, \\mathrm{true}}\\) приобретает такой вид:\n\\[\n\\mathbb{P}\\Big( \\bar z_{ij} - t_\\gamma \\sqrt{\\tfrac{1}{n-3}} &lt; z_{ij, \\mathrm{true}} &lt; \\bar z_{ij} + t_\\gamma \\sqrt{\\tfrac{1}{n-3}}  \\Big) = \\gamma\n\\]\nДалее путём обратного преобразования получаются значения границ интервала \\((\\rho_{ij,\\min}, \\; \\rho_{ij,\\max})\\).\n\n\n\n7.2.3 Коэффициенты корреляции для разных шкал\nДла разных шкал разработаны разные коэффициенты корреляции. Оценки коэффициентов будут рассчитываться по-разному, но логика тестирования статистических гипотез остаётся одинаковой.\n\n\n\n\n\n\n\n\nПеременная \\(X\\)\nПеременная \\(Y\\)\nМера связи\n\n\n\n\nИнтервальная или отношений\nИнтервальная или отношений\nКоэффициент Пирсона\n\n\nРанговая, интервальная или отношений\nРанговая, интервальная или отношений\nКоэффициент Спирмена\n\n\nРанговая\nРанговая\nКоэффициент Кенделла",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  }
]